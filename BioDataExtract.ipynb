{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nUhA90ZY4B1",
        "outputId": "26a0bafb-283d-4da4-fbf1-dd5c755cdce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.32.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.2 (from gradio)\n",
            "  Downloading gradio_client-1.10.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.47.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (15.0.1)\n",
            "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.32.0-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.2-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.3-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, xlsxwriter, uvicorn, tomlkit, semantic-version, ruff, python-multipart, pypdfium2, groovy, ffmpy, aiofiles, starlette, safehttpx, pdfminer.six, gradio-client, fastapi, pdfplumber, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.32.0 gradio-client-1.10.2 groovy-0.1.2 pdfminer.six-20250327 pdfplumber-0.11.6 pydub-0.25.1 pypdfium2-4.30.1 python-multipart-0.0.20 ruff-0.11.12 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2 xlsxwriter-3.2.3\n"
          ]
        }
      ],
      "source": [
        "pip install gradio pdfplumber spacy pandas xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB8pUUXzGE_n",
        "outputId": "d0857721-cf4c-4678-d178-23b3b97ef071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the path to your PDF file: /content/sek1_animals.pdf\n",
            "\n",
            "Select entity types to extract (enter y/n):\n",
            "Species Names (y/n): Y\n",
            "Measurements (y/n): \n",
            "Length Measurements (y/n): \n",
            "Person Names (y/n): \n",
            "Organizations (y/n): \n",
            "Dates (y/n): \n",
            "Geopolitical Entities (y/n): \n",
            "Locations (y/n): \n",
            "Extracted text length: 31138\n",
            "## Extracted Entities\n",
            "\n",
            "### SPECIES\n",
            "- n\n",
            "- memory\n",
            "- whiskers\n",
            "- eye\n",
            "- king\n",
            "- Frogs and\n",
            "- salt\n",
            "- cold\n",
            "- turn\n",
            "- crabs\n",
            "- sentences\n",
            "- blood\n",
            "- Look and\n",
            "- watch\n",
            "- meets\n",
            "- Correct\n",
            "- Singular\n",
            "- If\n",
            "- ee\n",
            "- mit\n",
            "- Children´s\n",
            "- Tiger and\n",
            "- save\n",
            "- J\n",
            "- should\n",
            "- oh\n",
            "- x\n",
            "- c.\n",
            "- Hippos are\n",
            "- penguins\n",
            "- terrib\n",
            "- My\n",
            "- The\n",
            "- Find\n",
            "- except\n",
            "- You need\n",
            "- mammal\n",
            "- if\n",
            "- seat\n",
            "- MN\n",
            "- favourite\n",
            "- games\n",
            "- Campbell books\n",
            "- toads\n",
            "- vocabulary\n",
            "- farm\n",
            "- print\n",
            "- salmon\n",
            "- cages\n",
            "- to\n",
            "- ear\n",
            "- record\n",
            "- that\n",
            "- duck\n",
            "- Ten little\n",
            "- harmless\n",
            "- If the\n",
            "- and\n",
            "- feathers\n",
            "- All\n",
            "- Yes\n",
            "- play\n",
            "- acrostic\n",
            "- eggs\n",
            "- Was\n",
            "- dir\n",
            "- Write and\n",
            "- Africa in\n",
            "- GO\n",
            "- with\n",
            "- Lions eat\n",
            "- i\n",
            "- Sing with\n",
            "- You are\n",
            "- LOOK\n",
            "- animal´s\n",
            "- In this\n",
            "- Match the\n",
            "- cards\n",
            "- zebra\n",
            "- Zoo\n",
            "- drawings\n",
            "- Eis\n",
            "- everything\n",
            "- dish\n",
            "- This is\n",
            "- tyle\n",
            "- clothes\n",
            "- Andreae\n",
            "- Mammals\n",
            "- fressen\n",
            "- Snakes eat\n",
            "- red\n",
            "- paw\n",
            "- singular\n",
            "- choose\n",
            "- Let\n",
            "- herbivore\n",
            "- predators\n",
            "- Saltwater\n",
            "- counting\n",
            "- content\n",
            "- Let´s\n",
            "- Here you\n",
            "- Groups of\n",
            "- little\n",
            "- Y\n",
            "- Is their\n",
            "- Mark green\n",
            "- shell\n",
            "- babies\n",
            "- mind\n",
            "- savanna\n",
            "- sharp\n",
            "- tries\n",
            "- fierce\n",
            "- scales\n",
            "- let\n",
            "- elephant\n",
            "- says\n",
            "- how\n",
            "- go\n",
            "- alone\n",
            "- ostrich\n",
            "- cute\n",
            "- als\n",
            "- Extension structures\n",
            "- Tick off\n",
            "- herbivores\n",
            "- correct\n",
            "- David\n",
            "- grouping\n",
            "- gefärbt\n",
            "- blowing\n",
            "- Löwe-\n",
            "- bark\n",
            "- mammals\n",
            "- Correct numbers\n",
            "- eaters\n",
            "- flamingo\n",
            "- insects\n",
            "- numbers\n",
            "- lik\n",
            "- keinen\n",
            "- there\n",
            "- ave\n",
            "- intellige\n",
            "- photos\n",
            "- names\n",
            "- Look\n",
            "- turkey\n",
            "- Oh\n",
            "- S\n",
            "- b.\n",
            "- Core structures\n",
            "- Snakes\n",
            "- C\n",
            "- Are\n",
            "- new\n",
            "- B\n",
            "- all\n",
            "- turtle\n",
            "- GORILLA\n",
            "- animals\n",
            "- A\n",
            "- ELEPHANT\n",
            "- wh\n",
            "- haben\n",
            "- come\n",
            "- bear\n",
            "- The smartest\n",
            "- show\n",
            "- tick\n",
            "- h\n",
            "- What´s\n",
            "- is\n",
            "- So\n",
            "- live\n",
            "- Possible\n",
            "- AWH\n",
            "- Animal chain\n",
            "- GG\n",
            "- s.\n",
            "- eyes\n",
            "- Go\n",
            "- For the\n",
            "- Rhinos are\n",
            "- reading\n",
            "- crawler\n",
            "- Come\n",
            "- chicken\n",
            "- Now it\n",
            "- map\n",
            "- you´d\n",
            "- ypur\n",
            "- hunter\n",
            "- alphabetically\n",
            "- Words in\n",
            "- The elephant\n",
            "- who\n",
            "- TO\n",
            "- must\n",
            "- table\n",
            "- tailfin\n",
            "- The child\n",
            "- eat\n",
            "- Lions are\n",
            "- ice\n",
            "- Words to\n",
            "- off\n",
            "- overlapping\n",
            "- eeth\n",
            "- below\n",
            "- mark\n",
            "- song\n",
            "- carnivore\n",
            "- chameleon\n",
            "- pet\n",
            "- mountains\n",
            "- hump\n",
            "- ipes\n",
            "- Draw your\n",
            "- colours\n",
            "- friends\n",
            "- dog\n",
            "- Tiere im\n",
            "- manta\n",
            "- Rochen\n",
            "- But there\n",
            "- I´d\n",
            "- family\n",
            "- All tigers\n",
            "- All these\n",
            "- EWO\n",
            "- gull\n",
            "- Now\n",
            "- cat\n",
            "- t.\n",
            "- sprouts\n",
            "- Buster fetches\n",
            "- no\n",
            "- text\n",
            "- cross\n",
            "- carry\n",
            "- Truthahn\n",
            "- WO\n",
            "- here\n",
            "- They like\n",
            "- playing\n",
            "- ammals\n",
            "- Rumble\n",
            "- Use the\n",
            "- tuna\n",
            "- plant\n",
            "- cid:219\n",
            "- eats\n",
            "- cid:47\n",
            "- Fisch und\n",
            "- Topic\n",
            "- bird\n",
            "- There are\n",
            "- hunters\n",
            "- swim\n",
            "- put\n",
            "- fins\n",
            "- For\n",
            "- It´s\n",
            "- shuffle\n",
            "- legen\n",
            "- herbivor\n",
            "- lovely\n",
            "- wollen\n",
            "- elements\n",
            "- iceberg\n",
            "- What is\n",
            "- Are you\n",
            "- mindmap\n",
            "- Now you\n",
            "- run\n",
            "- Can\n",
            "- Your partner\n",
            "- Wild\n",
            "- brown\n",
            "- different\n",
            "- Order alphabetically\n",
            "- Saltwater fish\n",
            "- You can\n",
            "- sit\n",
            "- beautiful\n",
            "- he\n",
            "- TIGER\n",
            "- desert\n",
            "- ESNE\n",
            "- patterns\n",
            "- Words\n",
            "- MATCH\n",
            "- Fish have\n",
            "- Number\n",
            "- wild\n",
            "- Name\n",
            "- If there\n",
            "- beak\n",
            "- beschreiben\n",
            "- Animals on\n",
            "- Different\n",
            "- READ\n",
            "- cid:159\n",
            "- nose\n",
            "- V\n",
            "- teacher\n",
            "- skin\n",
            "- hop\n",
            "- LION\n",
            "- list\n",
            "- love\n",
            "- b\n",
            "- chimpanzee\n",
            "- WOLF\n",
            "- down\n",
            "- now\n",
            "- g\n",
            "- I´ve\n",
            "- It has\n",
            "- book\n",
            "- Describe your\n",
            "- F\n",
            "- bananas\n",
            "- neck\n",
            "- Ebra\n",
            "- Did you\n",
            "- Study the\n",
            "- these\n",
            "- zeichnen\n",
            "- phrases\n",
            "- Find a\n",
            "- WRITE\n",
            "- white\n",
            "- Unscramble the\n",
            "- How to\n",
            "- crawl\n",
            "- einer\n",
            "- feather\n",
            "- gehen\n",
            "- http://tiere.m-y-d-s.com\n",
            "- d\n",
            "- poem\n",
            "- likes\n",
            "- THE\n",
            "- Extension vocabulary\n",
            "- Kaltblüter\n",
            "- Predators are\n",
            "- M\n",
            "- Is it\n",
            "- LITTLE\n",
            "- Extension\n",
            "- elephants\n",
            "- Come to\n",
            "- spots\n",
            "- Try\n",
            "- ossicones\n",
            "- dislikes\n",
            "- story\n",
            "- dry\n",
            "- Number the\n",
            "- amphibians\n",
            "- den\n",
            "- Language content\n",
            "- say\n",
            "- shark\n",
            "- My neck\n",
            "- The gruffalo\n",
            "- asks\n",
            "- toes\n",
            "- Predators\n",
            "- Characteristic shapes\n",
            "- Mime\n",
            "- out\n",
            "- elegant\n",
            "- Say\n",
            "- Mamm als\n",
            "- yellow\n",
            "- Oh dear\n",
            "- Do\n",
            "- U\n",
            "- terrifiying\n",
            "- head\n",
            "- This\n",
            "- bees\n",
            "- Tiger\n",
            "- gives\n",
            "- commands\n",
            "- owl\n",
            "- blue\n",
            "- texts\n",
            "- wings\n",
            "- Giles\n",
            "- zoos\n",
            "- himpanzees\n",
            "- klein\n",
            "- child\n",
            "- children\n",
            "- dove\n",
            "- fly\n",
            "- SPEAK\n",
            "- Draw some\n",
            "- milk\n",
            "- this\n",
            "- MR\n",
            "- what\n",
            "- My skin\n",
            "- hippo\n",
            "- plants\n",
            "- In\n",
            "- Here\n",
            "- smartest\n",
            "- forest\n",
            "- sind\n",
            "- teeth\n",
            "- leopard\n",
            "- name\n",
            "- Solution\n",
            "- books\n",
            "- OG\n",
            "- present\n",
            "- N\n",
            "- Teaching\n",
            "- Watch\n",
            "- fetches\n",
            "- Wissenswertes\n",
            "- dangerous\n",
            "- groups\n",
            "- Wind bl\n",
            "- bite\n",
            "- rhinoceros\n",
            "- Read the\n",
            "- claws\n",
            "- Different animals\n",
            "- Tier\n",
            "- r\n",
            "- insect\n",
            "- structures\n",
            "- Fish\n",
            "- listen\n",
            "- reptile\n",
            "- Order\n",
            "- flippers\n",
            "- racoon\n",
            "- Go to\n",
            "- Teaching material\n",
            "- giant\n",
            "- The children\n",
            "- Hai\n",
            "- Animal´s\n",
            "- context\n",
            "- Penguin\n",
            "- pack\n",
            "- lives\n",
            "- part\n",
            "- Please\n",
            "- giraffe\n",
            "- You\n",
            "- E\n",
            "- Tigers live\n",
            "- Try again\n",
            "- My teeth\n",
            "- shapes\n",
            "- in\n",
            "- we\n",
            "- according\n",
            "- They live\n",
            "- y\n",
            "- rainforest\n",
            "- Farm\n",
            "- Name the\n",
            "- Make a\n",
            "- toad\n",
            "- monkey\n",
            "- worksheet\n",
            "- Plural\n",
            "- chain\n",
            "- give\n",
            "- Schuppen\n",
            "- It is\n",
            "- Snakes are\n",
            "- don´t\n",
            "- your\n",
            "- hay\n",
            "- green\n",
            "- Grouping\n",
            "- grass\n",
            "- helpful\n",
            "- lot\n",
            "- poster\n",
            "- Core vocabulary\n",
            "- Did\n",
            "- Eier\n",
            "- will\n",
            "- most\n",
            "- The brown\n",
            "- Are playing\n",
            "- lift\n",
            "- interesting\n",
            "- many\n",
            "- sing\n",
            "- hedgehog\n",
            "- Do you\n",
            "- How many\n",
            "- wirklich\n",
            "- Write\n",
            "- NW\n",
            "- Th\n",
            "- Fleisch\n",
            "- order\n",
            "- D\n",
            "- OO\n",
            "- stripes\n",
            "- stickers\n",
            "- Tiere\n",
            "- guess\n",
            "- Find the\n",
            "- P\n",
            "- right\n",
            "- über\n",
            "- presentation\n",
            "- Penguins\n",
            "- Play\n",
            "- tiger\n",
            "- change\n",
            "- awl\n",
            "- birds\n",
            "- Birds h\n",
            "- Fell\n",
            "- J.\n",
            "- or\n",
            "- Hippos eat\n",
            "- Wild animals\n",
            "- am\n",
            "- gruffalo\n",
            "- Use\n",
            "- ears\n",
            "- Lazar trunk\n",
            "- R\n",
            "- very\n",
            "- need\n",
            "- snake\n",
            "- rhinozeros\n",
            "- tooth\n",
            "- gave\n",
            "- meat\n",
            "- Train\n",
            "- ask\n",
            "- for\n",
            "- But\n",
            "- want\n",
            "- oo\n",
            "- st\n",
            "- wordsearch\n",
            "- crow\n",
            "- K\n",
            "- Core\n",
            "- think\n",
            "- für\n",
            "- partner\n",
            "- were\n",
            "- In the\n",
            "- see\n",
            "- l\n",
            "- near\n",
            "- super\n",
            "- sleep\n",
            "- dromedary\n",
            "- bamboo\n",
            "- Reptiles\n",
            "- land\n",
            "- Activity\n",
            "- Find and\n",
            "- Fish and\n",
            "- Structures for\n",
            "- use\n",
            "- Mime an\n",
            "- Make\n",
            "- bläst\n",
            "- plural\n",
            "- cid:156\n",
            "- They don\n",
            "- Eat a\n",
            "- verschiedener\n",
            "- Language\n",
            "- wrong\n",
            "- Sing\n",
            "- pupils\n",
            "- Penguin song\n",
            "- Q\n",
            "- My fur\n",
            "- Rhinos have\n",
            "- Insekten\n",
            "- G\n",
            "- horn\n",
            "- cid:134\n",
            "- legs\n",
            "- they\n",
            "- snakes\n",
            "- way\n",
            "- L\n",
            "- cut\n",
            "- donkey\n",
            "- on\n",
            "- snow\n",
            "- missing\n",
            "- words\n",
            "- O\n",
            "- CROCODILE\n",
            "- frightening\n",
            "- it´s\n",
            "- leben\n",
            "- Draw an\n",
            "- Guess the\n",
            "- Amphibien\n",
            "- Vocabulary\n",
            "- SHARK\n",
            "- are\n",
            "- horns\n",
            "- have\n",
            "- MG\n",
            "- got\n",
            "- See their\n",
            "- Animal\n",
            "- be\n",
            "- It\n",
            "- lesen\n",
            "- Rumble in\n",
            "- My favourite\n",
            "- class\n",
            "- Zebras eat\n",
            "- flap\n",
            "- Write down\n",
            "- interests\n",
            "- material\n",
            "- Hund\n",
            "- into\n",
            "- fish\n",
            "- stomach\n",
            "- best\n",
            "- u\n",
            "- Let one\n",
            "- animal\n",
            "- cid:173\n",
            "- coloured\n",
            "- clasmates\n",
            "- phone\n",
            "- Fill in\n",
            "- fresh\n",
            "- They\n",
            "- where\n",
            "- cardboard\n",
            "- hair\n",
            "- mouse\n",
            "- catch\n",
            "- Grandma where\n",
            "- you\n",
            "- draw\n",
            "- jump\n",
            "- pig\n",
            "- f\n",
            "- Write the\n",
            "- Nilpferd\n",
            "- There is\n",
            "- parrot\n",
            "- grey\n",
            "- Animals in\n",
            "- reptiles\n",
            "- Merkmale\n",
            "- me\n",
            "- Unscramble\n",
            "- Schnabel\n",
            "- orange\n",
            "- really\n",
            "- again\n",
            "- wood\n",
            "- jaguar\n",
            "- can\n",
            "- Wojtowycz\n",
            "- nice\n",
            "- lay\n",
            "- polar\n",
            "- unscramble\n",
            "- And\n",
            "- Penguins live\n",
            "- essen\n",
            "- lake\n",
            "- read\n",
            "- saltwater\n",
            "- black\n",
            "- mobile\n",
            "- small\n",
            "- reindeer\n",
            "- dorsal\n",
            "- It lives\n",
            "- also\n",
            "- talk\n",
            "- drinks\n",
            "- And if\n",
            "- How\n",
            "- Animals and\n",
            "- Material\n",
            "- penguin\n",
            "- horse\n",
            "- PENGUINS\n",
            "- And you\n",
            "- sheep\n",
            "- tail\n",
            "- look\n",
            "- This animal\n",
            "- racoons\n",
            "- frog\n",
            "- predator\n",
            "- Please don\n",
            "- Penguins are\n",
            "- find\n",
            "- c\n",
            "- Study\n",
            "- not\n",
            "- big\n",
            "- sleepy\n",
            "- kommen\n",
            "- Train reading\n",
            "- polarbear\n",
            "- fast\n",
            "- sehr\n",
            "- T\n",
            "- Play animal\n",
            "- cheetah\n",
            "- They are\n",
            "- wing\n",
            "- Print\n",
            "- ON\n",
            "- important\n",
            "- ein\n",
            "- crocodile\n",
            "- water\n",
            "- may\n",
            "- X\n",
            "- tigers\n",
            "- eisig\n",
            "- Your\n",
            "- fur\n",
            "- my\n",
            "- lion\n",
            "- group\n",
            "- e\n",
            "- hungry\n",
            "- Read and\n",
            "- Watch the\n",
            "- This my\n",
            "- lions\n",
            "- E.g.\n",
            "- No\n",
            "- lang\n",
            "- k\n",
            "- an\n",
            "- ZOO\n",
            "- scary\n",
            "- pouch\n",
            "- climb\n",
            "- hunt\n",
            "- like\n",
            "- Z\n",
            "- icily\n",
            "- Draw\n",
            "- Hippos live\n",
            "- vocubulary\n",
            "- box\n",
            "- Grouping animals\n",
            "- auch\n",
            "- Read\n",
            "- poems\n",
            "- at\n",
            "- wolves\n",
            "- the\n",
            "- cage\n",
            "- command\n",
            "- too\n",
            "- facts\n",
            "- Birds\n",
            "- frogs\n",
            "- NWG\n",
            "- Sit in\n",
            "- PRESENTATION\n",
            "- Giraffes live\n",
            "- Mark the\n",
            "- m\n",
            "- flipper\n",
            "- How nice\n",
            "- re\n",
            "- sea\n",
            "- paws\n",
            "- See\n",
            "- a\n",
            "- ihre\n",
            "- back\n",
            "- t\n",
            "- rhyme\n",
            "- Lions\n",
            "- them\n",
            "- Match\n",
            "- panda\n",
            "- grizzly\n",
            "- Exceptions\n",
            "- cid:55\n",
            "- jungle\n",
            "- pictures\n",
            "- Animals\n",
            "- Setzleiste\n",
            "- It eats\n",
            "- All coloured\n",
            "- rocodiles\n",
            "- Write your\n",
            "- town\n",
            "- leg\n",
            "- cid:161\n",
            "- eine\n",
            "- They eat\n",
            "- trunk\n",
            "- ei\n",
            "- parts\n",
            "- Reptiles have\n",
            "- zoo\n",
            "- H\n",
            "- exercise\n",
            "- flies\n",
            "- circle\n",
            "- wind\n",
            "- eth\n",
            "- help\n",
            "- cid:155\n",
            "- Buster\n",
            "- baby\n",
            "- s\n",
            "- Setzleiste match\n",
            "- beaks\n",
            "- Can you\n",
            "- Drache\n",
            "- rhino\n",
            "- house\n",
            "- write\n",
            "- of\n",
            "- some\n",
            "- eautiful\n",
            "- mane\n",
            "- sehen\n",
            "- has\n",
            "- Describe\n",
            "- kangaroo\n",
            "- ist\n",
            "- Choose\n",
            "- Chimpanzees\n",
            "- Hippos\n",
            "- Write an\n",
            "- spend\n",
            "- I´m\n",
            "- eagle\n",
            "- Lions live\n",
            "- warm\n",
            "- classmates\n",
            "- their\n",
            "- GRIZZLY\n",
            "- Choose an\n",
            "- match\n",
            "- dear\n",
            "- ones\n",
            "- What\n",
            "- Groups\n",
            "- describe\n",
            "- But they\n",
            "- long\n",
            "- Merkmale verschiedener\n",
            "- comprehension\n",
            "- gorilla\n",
            "- about\n",
            "- add\n",
            "- places\n",
            "- There\n",
            "- mini\n",
            "- und\n",
            "- Print out\n",
            "- other\n",
            "- cid:198\n",
            "- The wind\n",
            "- stick\n",
            "- body\n",
            "- alphabetical\n",
            "- Eat\n",
            "- know\n",
            "- Guess\n",
            "- Is\n",
            "- Was ist\n",
            "- it\n",
            "- kite\n",
            "- do\n",
            "- isn´t\n",
            "- I\n",
            "- beobachten\n",
            "- extension\n",
            "- his\n",
            "- speak\n",
            "- o\n",
            "- gemustert\n",
            "- wolf\n",
            "- arms\n",
            "- fin\n",
            "- could\n",
            "- What about\n",
            "- S.11\n",
            "- Milch f\n",
            "- Snakes live\n",
            "- dromedaries\n",
            "- Mime and\n",
            "- W\n",
            "- butterfly\n",
            "- te\n",
            "- always\n",
            "- tusks\n",
            "- Schlange\n",
            "- GIRAFFE\n",
            "- Penguins eat\n",
            "- cid:140\n",
            "\n",
            "\n",
            "Results saved to extracted_entities.xlsx\n",
            "\n",
            "Result:\n",
            "## Extracted Entities\n",
            "\n",
            "### SPECIES\n",
            "- n\n",
            "- memory\n",
            "- whiskers\n",
            "- eye\n",
            "- king\n",
            "- Frogs and\n",
            "- salt\n",
            "- cold\n",
            "- turn\n",
            "- crabs\n",
            "- sentences\n",
            "- blood\n",
            "- Look and\n",
            "- watch\n",
            "- meets\n",
            "- Correct\n",
            "- Singular\n",
            "- If\n",
            "- ee\n",
            "- mit\n",
            "- Children´s\n",
            "- Tiger and\n",
            "- save\n",
            "- J\n",
            "- should\n",
            "- oh\n",
            "- x\n",
            "- c.\n",
            "- Hippos are\n",
            "- penguins\n",
            "- terrib\n",
            "- My\n",
            "- The\n",
            "- Find\n",
            "- except\n",
            "- You need\n",
            "- mammal\n",
            "- if\n",
            "- seat\n",
            "- MN\n",
            "- favourite\n",
            "- games\n",
            "- Campbell books\n",
            "- toads\n",
            "- vocabulary\n",
            "- farm\n",
            "- print\n",
            "- salmon\n",
            "- cages\n",
            "- to\n",
            "- ear\n",
            "- record\n",
            "- that\n",
            "- duck\n",
            "- Ten little\n",
            "- harmless\n",
            "- If the\n",
            "- and\n",
            "- feathers\n",
            "- All\n",
            "- Yes\n",
            "- play\n",
            "- acrostic\n",
            "- eggs\n",
            "- Was\n",
            "- dir\n",
            "- Write and\n",
            "- Africa in\n",
            "- GO\n",
            "- with\n",
            "- Lions eat\n",
            "- i\n",
            "- Sing with\n",
            "- You are\n",
            "- LOOK\n",
            "- animal´s\n",
            "- In this\n",
            "- Match the\n",
            "- cards\n",
            "- zebra\n",
            "- Zoo\n",
            "- drawings\n",
            "- Eis\n",
            "- everything\n",
            "- dish\n",
            "- This is\n",
            "- tyle\n",
            "- clothes\n",
            "- Andreae\n",
            "- Mammals\n",
            "- fressen\n",
            "- Snakes eat\n",
            "- red\n",
            "- paw\n",
            "- singular\n",
            "- choose\n",
            "- Let\n",
            "- herbivore\n",
            "- predators\n",
            "- Saltwater\n",
            "- counting\n",
            "- content\n",
            "- Let´s\n",
            "- Here you\n",
            "- Groups of\n",
            "- little\n",
            "- Y\n",
            "- Is their\n",
            "- Mark green\n",
            "- shell\n",
            "- babies\n",
            "- mind\n",
            "- savanna\n",
            "- sharp\n",
            "- tries\n",
            "- fierce\n",
            "- scales\n",
            "- let\n",
            "- elephant\n",
            "- says\n",
            "- how\n",
            "- go\n",
            "- alone\n",
            "- ostrich\n",
            "- cute\n",
            "- als\n",
            "- Extension structures\n",
            "- Tick off\n",
            "- herbivores\n",
            "- correct\n",
            "- David\n",
            "- grouping\n",
            "- gefärbt\n",
            "- blowing\n",
            "- Löwe-\n",
            "- bark\n",
            "- mammals\n",
            "- Correct numbers\n",
            "- eaters\n",
            "- flamingo\n",
            "- insects\n",
            "- numbers\n",
            "- lik\n",
            "- keinen\n",
            "- there\n",
            "- ave\n",
            "- intellige\n",
            "- photos\n",
            "- names\n",
            "- Look\n",
            "- turkey\n",
            "- Oh\n",
            "- S\n",
            "- b.\n",
            "- Core structures\n",
            "- Snakes\n",
            "- C\n",
            "- Are\n",
            "- new\n",
            "- B\n",
            "- all\n",
            "- turtle\n",
            "- GORILLA\n",
            "- animals\n",
            "- A\n",
            "- ELEPHANT\n",
            "- wh\n",
            "- haben\n",
            "- come\n",
            "- bear\n",
            "- The smartest\n",
            "- show\n",
            "- tick\n",
            "- h\n",
            "- What´s\n",
            "- is\n",
            "- So\n",
            "- live\n",
            "- Possible\n",
            "- AWH\n",
            "- Animal chain\n",
            "- GG\n",
            "- s.\n",
            "- eyes\n",
            "- Go\n",
            "- For the\n",
            "- Rhinos are\n",
            "- reading\n",
            "- crawler\n",
            "- Come\n",
            "- chicken\n",
            "- Now it\n",
            "- map\n",
            "- you´d\n",
            "- ypur\n",
            "- hunter\n",
            "- alphabetically\n",
            "- Words in\n",
            "- The elephant\n",
            "- who\n",
            "- TO\n",
            "- must\n",
            "- table\n",
            "- tailfin\n",
            "- The child\n",
            "- eat\n",
            "- Lions are\n",
            "- ice\n",
            "- Words to\n",
            "- off\n",
            "- overlapping\n",
            "- eeth\n",
            "- below\n",
            "- mark\n",
            "- song\n",
            "- carnivore\n",
            "- chameleon\n",
            "- pet\n",
            "- mountains\n",
            "- hump\n",
            "- ipes\n",
            "- Draw your\n",
            "- colours\n",
            "- friends\n",
            "- dog\n",
            "- Tiere im\n",
            "- manta\n",
            "- Rochen\n",
            "- But there\n",
            "- I´d\n",
            "- family\n",
            "- All tigers\n",
            "- All these\n",
            "- EWO\n",
            "- gull\n",
            "- Now\n",
            "- cat\n",
            "- t.\n",
            "- sprouts\n",
            "- Buster fetches\n",
            "- no\n",
            "- text\n",
            "- cross\n",
            "- carry\n",
            "- Truthahn\n",
            "- WO\n",
            "- here\n",
            "- They like\n",
            "- playing\n",
            "- ammals\n",
            "- Rumble\n",
            "- Use the\n",
            "- tuna\n",
            "- plant\n",
            "- cid:219\n",
            "- eats\n",
            "- cid:47\n",
            "- Fisch und\n",
            "- Topic\n",
            "- bird\n",
            "- There are\n",
            "- hunters\n",
            "- swim\n",
            "- put\n",
            "- fins\n",
            "- For\n",
            "- It´s\n",
            "- shuffle\n",
            "- legen\n",
            "- herbivor\n",
            "- lovely\n",
            "- wollen\n",
            "- elements\n",
            "- iceberg\n",
            "- What is\n",
            "- Are you\n",
            "- mindmap\n",
            "- Now you\n",
            "- run\n",
            "- Can\n",
            "- Your partner\n",
            "- Wild\n",
            "- brown\n",
            "- different\n",
            "- Order alphabetically\n",
            "- Saltwater fish\n",
            "- You can\n",
            "- sit\n",
            "- beautiful\n",
            "- he\n",
            "- TIGER\n",
            "- desert\n",
            "- ESNE\n",
            "- patterns\n",
            "- Words\n",
            "- MATCH\n",
            "- Fish have\n",
            "- Number\n",
            "- wild\n",
            "- Name\n",
            "- If there\n",
            "- beak\n",
            "- beschreiben\n",
            "- Animals on\n",
            "- Different\n",
            "- READ\n",
            "- cid:159\n",
            "- nose\n",
            "- V\n",
            "- teacher\n",
            "- skin\n",
            "- hop\n",
            "- LION\n",
            "- list\n",
            "- love\n",
            "- b\n",
            "- chimpanzee\n",
            "- WOLF\n",
            "- down\n",
            "- now\n",
            "- g\n",
            "- I´ve\n",
            "- It has\n",
            "- book\n",
            "- Describe your\n",
            "- F\n",
            "- bananas\n",
            "- neck\n",
            "- Ebra\n",
            "- Did you\n",
            "- Study the\n",
            "- these\n",
            "- zeichnen\n",
            "- phrases\n",
            "- Find a\n",
            "- WRITE\n",
            "- white\n",
            "- Unscramble the\n",
            "- How to\n",
            "- crawl\n",
            "- einer\n",
            "- feather\n",
            "- gehen\n",
            "- http://tiere.m-y-d-s.com\n",
            "- d\n",
            "- poem\n",
            "- likes\n",
            "- THE\n",
            "- Extension vocabulary\n",
            "- Kaltblüter\n",
            "- Predators are\n",
            "- M\n",
            "- Is it\n",
            "- LITTLE\n",
            "- Extension\n",
            "- elephants\n",
            "- Come to\n",
            "- spots\n",
            "- Try\n",
            "- ossicones\n",
            "- dislikes\n",
            "- story\n",
            "- dry\n",
            "- Number the\n",
            "- amphibians\n",
            "- den\n",
            "- Language content\n",
            "- say\n",
            "- shark\n",
            "- My neck\n",
            "- The gruffalo\n",
            "- asks\n",
            "- toes\n",
            "- Predators\n",
            "- Characteristic shapes\n",
            "- Mime\n",
            "- out\n",
            "- elegant\n",
            "- Say\n",
            "- Mamm als\n",
            "- yellow\n",
            "- Oh dear\n",
            "- Do\n",
            "- U\n",
            "- terrifiying\n",
            "- head\n",
            "- This\n",
            "- bees\n",
            "- Tiger\n",
            "- gives\n",
            "- commands\n",
            "- owl\n",
            "- blue\n",
            "- texts\n",
            "- wings\n",
            "- Giles\n",
            "- zoos\n",
            "- himpanzees\n",
            "- klein\n",
            "- child\n",
            "- children\n",
            "- dove\n",
            "- fly\n",
            "- SPEAK\n",
            "- Draw some\n",
            "- milk\n",
            "- this\n",
            "- MR\n",
            "- what\n",
            "- My skin\n",
            "- hippo\n",
            "- plants\n",
            "- In\n",
            "- Here\n",
            "- smartest\n",
            "- forest\n",
            "- sind\n",
            "- teeth\n",
            "- leopard\n",
            "- name\n",
            "- Solution\n",
            "- books\n",
            "- OG\n",
            "- present\n",
            "- N\n",
            "- Teaching\n",
            "- Watch\n",
            "- fetches\n",
            "- Wissenswertes\n",
            "- dangerous\n",
            "- groups\n",
            "- Wind bl\n",
            "- bite\n",
            "- rhinoceros\n",
            "- Read the\n",
            "- claws\n",
            "- Different animals\n",
            "- Tier\n",
            "- r\n",
            "- insect\n",
            "- structures\n",
            "- Fish\n",
            "- listen\n",
            "- reptile\n",
            "- Order\n",
            "- flippers\n",
            "- racoon\n",
            "- Go to\n",
            "- Teaching material\n",
            "- giant\n",
            "- The children\n",
            "- Hai\n",
            "- Animal´s\n",
            "- context\n",
            "- Penguin\n",
            "- pack\n",
            "- lives\n",
            "- part\n",
            "- Please\n",
            "- giraffe\n",
            "- You\n",
            "- E\n",
            "- Tigers live\n",
            "- Try again\n",
            "- My teeth\n",
            "- shapes\n",
            "- in\n",
            "- we\n",
            "- according\n",
            "- They live\n",
            "- y\n",
            "- rainforest\n",
            "- Farm\n",
            "- Name the\n",
            "- Make a\n",
            "- toad\n",
            "- monkey\n",
            "- worksheet\n",
            "- Plural\n",
            "- chain\n",
            "- give\n",
            "- Schuppen\n",
            "- It is\n",
            "- Snakes are\n",
            "- don´t\n",
            "- your\n",
            "- hay\n",
            "- green\n",
            "- Grouping\n",
            "- grass\n",
            "- helpful\n",
            "- lot\n",
            "- poster\n",
            "- Core vocabulary\n",
            "- Did\n",
            "- Eier\n",
            "- will\n",
            "- most\n",
            "- The brown\n",
            "- Are playing\n",
            "- lift\n",
            "- interesting\n",
            "- many\n",
            "- sing\n",
            "- hedgehog\n",
            "- Do you\n",
            "- How many\n",
            "- wirklich\n",
            "- Write\n",
            "- NW\n",
            "- Th\n",
            "- Fleisch\n",
            "- order\n",
            "- D\n",
            "- OO\n",
            "- stripes\n",
            "- stickers\n",
            "- Tiere\n",
            "- guess\n",
            "- Find the\n",
            "- P\n",
            "- right\n",
            "- über\n",
            "- presentation\n",
            "- Penguins\n",
            "- Play\n",
            "- tiger\n",
            "- change\n",
            "- awl\n",
            "- birds\n",
            "- Birds h\n",
            "- Fell\n",
            "- J.\n",
            "- or\n",
            "- Hippos eat\n",
            "- Wild animals\n",
            "- am\n",
            "- gruffalo\n",
            "- Use\n",
            "- ears\n",
            "- Lazar trunk\n",
            "- R\n",
            "- very\n",
            "- need\n",
            "- snake\n",
            "- rhinozeros\n",
            "- tooth\n",
            "- gave\n",
            "- meat\n",
            "- Train\n",
            "- ask\n",
            "- for\n",
            "- But\n",
            "- want\n",
            "- oo\n",
            "- st\n",
            "- wordsearch\n",
            "- crow\n",
            "- K\n",
            "- Core\n",
            "- think\n",
            "- für\n",
            "- partner\n",
            "- were\n",
            "- In the\n",
            "- see\n",
            "- l\n",
            "- near\n",
            "- super\n",
            "- sleep\n",
            "- dromedary\n",
            "- bamboo\n",
            "- Reptiles\n",
            "- land\n",
            "- Activity\n",
            "- Find and\n",
            "- Fish and\n",
            "- Structures for\n",
            "- use\n",
            "- Mime an\n",
            "- Make\n",
            "- bläst\n",
            "- plural\n",
            "- cid:156\n",
            "- They don\n",
            "- Eat a\n",
            "- verschiedener\n",
            "- Language\n",
            "- wrong\n",
            "- Sing\n",
            "- pupils\n",
            "- Penguin song\n",
            "- Q\n",
            "- My fur\n",
            "- Rhinos have\n",
            "- Insekten\n",
            "- G\n",
            "- horn\n",
            "- cid:134\n",
            "- legs\n",
            "- they\n",
            "- snakes\n",
            "- way\n",
            "- L\n",
            "- cut\n",
            "- donkey\n",
            "- on\n",
            "- snow\n",
            "- missing\n",
            "- words\n",
            "- O\n",
            "- CROCODILE\n",
            "- frightening\n",
            "- it´s\n",
            "- leben\n",
            "- Draw an\n",
            "- Guess the\n",
            "- Amphibien\n",
            "- Vocabulary\n",
            "- SHARK\n",
            "- are\n",
            "- horns\n",
            "- have\n",
            "- MG\n",
            "- got\n",
            "- See their\n",
            "- Animal\n",
            "- be\n",
            "- It\n",
            "- lesen\n",
            "- Rumble in\n",
            "- My favourite\n",
            "- class\n",
            "- Zebras eat\n",
            "- flap\n",
            "- Write down\n",
            "- interests\n",
            "- material\n",
            "- Hund\n",
            "- into\n",
            "- fish\n",
            "- stomach\n",
            "- best\n",
            "- u\n",
            "- Let one\n",
            "- animal\n",
            "- cid:173\n",
            "- coloured\n",
            "- clasmates\n",
            "- phone\n",
            "- Fill in\n",
            "- fresh\n",
            "- They\n",
            "- where\n",
            "- cardboard\n",
            "- hair\n",
            "- mouse\n",
            "- catch\n",
            "- Grandma where\n",
            "- you\n",
            "- draw\n",
            "- jump\n",
            "- pig\n",
            "- f\n",
            "- Write the\n",
            "- Nilpferd\n",
            "- There is\n",
            "- parrot\n",
            "- grey\n",
            "- Animals in\n",
            "- reptiles\n",
            "- Merkmale\n",
            "- me\n",
            "- Unscramble\n",
            "- Schnabel\n",
            "- orange\n",
            "- really\n",
            "- again\n",
            "- wood\n",
            "- jaguar\n",
            "- can\n",
            "- Wojtowycz\n",
            "- nice\n",
            "- lay\n",
            "- polar\n",
            "- unscramble\n",
            "- And\n",
            "- Penguins live\n",
            "- essen\n",
            "- lake\n",
            "- read\n",
            "- saltwater\n",
            "- black\n",
            "- mobile\n",
            "- small\n",
            "- reindeer\n",
            "- dorsal\n",
            "- It lives\n",
            "- also\n",
            "- talk\n",
            "- drinks\n",
            "- And if\n",
            "- How\n",
            "- Animals and\n",
            "- Material\n",
            "- penguin\n",
            "- horse\n",
            "- PENGUINS\n",
            "- And you\n",
            "- sheep\n",
            "- tail\n",
            "- look\n",
            "- This animal\n",
            "- racoons\n",
            "- frog\n",
            "- predator\n",
            "- Please don\n",
            "- Penguins are\n",
            "- find\n",
            "- c\n",
            "- Study\n",
            "- not\n",
            "- big\n",
            "- sleepy\n",
            "- kommen\n",
            "- Train reading\n",
            "- polarbear\n",
            "- fast\n",
            "- sehr\n",
            "- T\n",
            "- Play animal\n",
            "- cheetah\n",
            "- They are\n",
            "- wing\n",
            "- Print\n",
            "- ON\n",
            "- important\n",
            "- ein\n",
            "- crocodile\n",
            "- water\n",
            "- may\n",
            "- X\n",
            "- tigers\n",
            "- eisig\n",
            "- Your\n",
            "- fur\n",
            "- my\n",
            "- lion\n",
            "- group\n",
            "- e\n",
            "- hungry\n",
            "- Read and\n",
            "- Watch the\n",
            "- This my\n",
            "- lions\n",
            "- E.g.\n",
            "- No\n",
            "- lang\n",
            "- k\n",
            "- an\n",
            "- ZOO\n",
            "- scary\n",
            "- pouch\n",
            "- climb\n",
            "- hunt\n",
            "- like\n",
            "- Z\n",
            "- icily\n",
            "- Draw\n",
            "- Hippos live\n",
            "- vocubulary\n",
            "- box\n",
            "- Grouping animals\n",
            "- auch\n",
            "- Read\n",
            "- poems\n",
            "- at\n",
            "- wolves\n",
            "- the\n",
            "- cage\n",
            "- command\n",
            "- too\n",
            "- facts\n",
            "- Birds\n",
            "- frogs\n",
            "- NWG\n",
            "- Sit in\n",
            "- PRESENTATION\n",
            "- Giraffes live\n",
            "- Mark the\n",
            "- m\n",
            "- flipper\n",
            "- How nice\n",
            "- re\n",
            "- sea\n",
            "- paws\n",
            "- See\n",
            "- a\n",
            "- ihre\n",
            "- back\n",
            "- t\n",
            "- rhyme\n",
            "- Lions\n",
            "- them\n",
            "- Match\n",
            "- panda\n",
            "- grizzly\n",
            "- Exceptions\n",
            "- cid:55\n",
            "- jungle\n",
            "- pictures\n",
            "- Animals\n",
            "- Setzleiste\n",
            "- It eats\n",
            "- All coloured\n",
            "- rocodiles\n",
            "- Write your\n",
            "- town\n",
            "- leg\n",
            "- cid:161\n",
            "- eine\n",
            "- They eat\n",
            "- trunk\n",
            "- ei\n",
            "- parts\n",
            "- Reptiles have\n",
            "- zoo\n",
            "- H\n",
            "- exercise\n",
            "- flies\n",
            "- circle\n",
            "- wind\n",
            "- eth\n",
            "- help\n",
            "- cid:155\n",
            "- Buster\n",
            "- baby\n",
            "- s\n",
            "- Setzleiste match\n",
            "- beaks\n",
            "- Can you\n",
            "- Drache\n",
            "- rhino\n",
            "- house\n",
            "- write\n",
            "- of\n",
            "- some\n",
            "- eautiful\n",
            "- mane\n",
            "- sehen\n",
            "- has\n",
            "- Describe\n",
            "- kangaroo\n",
            "- ist\n",
            "- Choose\n",
            "- Chimpanzees\n",
            "- Hippos\n",
            "- Write an\n",
            "- spend\n",
            "- I´m\n",
            "- eagle\n",
            "- Lions live\n",
            "- warm\n",
            "- classmates\n",
            "- their\n",
            "- GRIZZLY\n",
            "- Choose an\n",
            "- match\n",
            "- dear\n",
            "- ones\n",
            "- What\n",
            "- Groups\n",
            "- describe\n",
            "- But they\n",
            "- long\n",
            "- Merkmale verschiedener\n",
            "- comprehension\n",
            "- gorilla\n",
            "- about\n",
            "- add\n",
            "- places\n",
            "- There\n",
            "- mini\n",
            "- und\n",
            "- Print out\n",
            "- other\n",
            "- cid:198\n",
            "- The wind\n",
            "- stick\n",
            "- body\n",
            "- alphabetical\n",
            "- Eat\n",
            "- know\n",
            "- Guess\n",
            "- Is\n",
            "- Was ist\n",
            "- it\n",
            "- kite\n",
            "- do\n",
            "- isn´t\n",
            "- I\n",
            "- beobachten\n",
            "- extension\n",
            "- his\n",
            "- speak\n",
            "- o\n",
            "- gemustert\n",
            "- wolf\n",
            "- arms\n",
            "- fin\n",
            "- could\n",
            "- What about\n",
            "- S.11\n",
            "- Milch f\n",
            "- Snakes live\n",
            "- dromedaries\n",
            "- Mime and\n",
            "- W\n",
            "- butterfly\n",
            "- te\n",
            "- always\n",
            "- tusks\n",
            "- Schlange\n",
            "- GIRAFFE\n",
            "- Penguins eat\n",
            "- cid:140\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pdfplumber\n",
        "import spacy\n",
        "import re\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "import os\n",
        "from io import BytesIO\n",
        "\n",
        "# Load the SpaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"SpaCy model not found. Please download it using: python -m spacy download en_core_web_sm\")\n",
        "    nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Add scientific entity patterns\n",
        "try:\n",
        "    ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "    patterns = [\n",
        "        {\"label\": \"SPECIES\", \"pattern\": [{\"LOWER\": {\"REGEX\": \"[a-z]+\"}}], \"id\": \"scientific_species\"},\n",
        "        {\"label\": \"MEASUREMENT\", \"pattern\": [{\"SHAPE\": \"d+.d+\"}, {\"LOWER\": {\"IN\": [\"mm\", \"cm\", \"m\", \"kg\", \"g\"]}}]},\n",
        "        {\"label\": \"LENGTH\", \"pattern\": [{\"LOWER\": \"fork\"}, {\"LOWER\": \"length\"}]},\n",
        "    ]\n",
        "    ruler.add_patterns(patterns)\n",
        "except Exception as e:\n",
        "    print(f\"Error setting up entity ruler: {e}\")\n",
        "    if \"entity_ruler\" not in nlp.pipe_names:\n",
        "        ruler = nlp.add_pipe(\"entity_ruler\", name=\"entity_ruler\")\n",
        "\n",
        "# Function to extract text from PDF with better error handling\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    if not pdf_path:\n",
        "        return \"No file provided.\"\n",
        "\n",
        "    try:\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                extracted_text = page.extract_text()\n",
        "                if extracted_text:\n",
        "                    text += extracted_text + \"\\n\"\n",
        "\n",
        "        if not text.strip():\n",
        "            return \"No text could be extracted from the PDF.\"\n",
        "\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error processing PDF: {str(e)}\"\n",
        "\n",
        "# Function to extract entities with better error handling\n",
        "def extract_entities(text, entity_types):\n",
        "    if isinstance(text, str) and not text.startswith(\"Error\"):\n",
        "        try:\n",
        "            # Limit text size to avoid memory issues\n",
        "            text = text[:100000]  # Limit to first 100k characters\n",
        "            doc = nlp(text)\n",
        "\n",
        "            # Create a dictionary to store extracted entities\n",
        "            extracted = {entity_type: [] for entity_type in entity_types}\n",
        "\n",
        "            # Extract required entity types\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_ in entity_types:\n",
        "                    extracted[ent.label_].append(ent.text)\n",
        "\n",
        "            # Special case for scientific species names\n",
        "            if \"SPECIES\" in entity_types:\n",
        "                scientific_names = re.findall(r'[A-Z][a-z]+ [a-z]+', text)\n",
        "                extracted[\"SPECIES\"].extend(scientific_names)\n",
        "\n",
        "            # Special case for measurements\n",
        "            if \"MEASUREMENT\" in entity_types:\n",
        "                measurements = re.findall(r'\\d+\\.?\\d*\\s*(?:mm|cm|m|kg|g)', text)\n",
        "                extracted[\"MEASUREMENT\"].extend(measurements)\n",
        "\n",
        "            # Special case for fork length\n",
        "            if \"LENGTH\" in entity_types:\n",
        "                fork_lengths = re.findall(r'(?:fork|total)?\\s*length\\s*(?:of)?\\s*\\d+\\.?\\d*\\s*(?:mm|cm|m)', text, re.IGNORECASE)\n",
        "                extracted[\"LENGTH\"].extend(fork_lengths)\n",
        "\n",
        "            # Remove duplicates\n",
        "            for entity_type in entity_types:\n",
        "                extracted[entity_type] = list(set(extracted[entity_type]))\n",
        "\n",
        "            return extracted\n",
        "        except Exception as e:\n",
        "            return f\"Error extracting entities: {str(e)}\"\n",
        "    else:\n",
        "        return f\"Cannot extract entities: {text}\"\n",
        "\n",
        "# Main function for processing PDFs\n",
        "def process_pdf(pdf_path, entity_types):\n",
        "    if not pdf_path:\n",
        "        return \"Please provide a PDF file path.\"\n",
        "\n",
        "    if not entity_types:\n",
        "        return \"Please specify at least one entity type to extract.\"\n",
        "\n",
        "    try:\n",
        "        # Extract text from PDF\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        print(f\"Extracted text length: {len(text) if isinstance(text, str) else 'Not a string'}\")\n",
        "\n",
        "        if isinstance(text, str) and text.startswith(\"Error\"):\n",
        "            return text, None\n",
        "\n",
        "        # Extract entities\n",
        "        extracted = extract_entities(text, entity_types)\n",
        "\n",
        "        if isinstance(extracted, str) and extracted.startswith(\"Error\"):\n",
        "            return extracted, None\n",
        "\n",
        "        # Prepare results\n",
        "        result_text = \"## Extracted Entities\\n\\n\"\n",
        "\n",
        "        try:\n",
        "            # Create Excel file with multiple sheets\n",
        "            output_file = \"extracted_entities.xlsx\"\n",
        "            with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
        "                for entity_type in entity_types:\n",
        "                    if extracted[entity_type]:\n",
        "                        result_text += f\"### {entity_type}\\n\"\n",
        "                        for item in extracted[entity_type]:\n",
        "                            result_text += f\"- {item}\\n\"\n",
        "                        result_text += \"\\n\"\n",
        "\n",
        "                        # Add sheet to Excel file\n",
        "                        df = pd.DataFrame({entity_type: extracted[entity_type]})\n",
        "                        df.to_excel(writer, sheet_name=entity_type[:31], index=False)\n",
        "                    else:\n",
        "                        result_text += f\"### {entity_type}\\n\"\n",
        "                        result_text += \"No entities of this type found.\\n\\n\"\n",
        "\n",
        "            print(result_text)\n",
        "            print(f\"Results saved to {output_file}\")\n",
        "            return result_text, output_file\n",
        "        except Exception as e:\n",
        "            return f\"Error creating Excel file: {str(e)}\", None\n",
        "    except Exception as e:\n",
        "        return f\"Unexpected error: {str(e)}\", None\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the path to your PDF file\n",
        "    pdf_path = input(\"Enter the path to your PDF file: \")\n",
        "\n",
        "    # Define entity types to extract\n",
        "    print(\"\\nSelect entity types to extract (enter y/n):\")\n",
        "    entity_types = []\n",
        "\n",
        "    if input(\"Species Names (y/n): \").lower() == 'y':\n",
        "        entity_types.append(\"SPECIES\")\n",
        "    if input(\"Measurements (y/n): \").lower() == 'y':\n",
        "        entity_types.append(\"MEASUREMENT\")\n",
        "    if input(\"Length Measurements (y/n): \").lower() == 'y':\n",
        "        entity_types.append(\"LENGTH\")\n",
        "    if input(\"Person Names (y/n): \").lower() == 'y':\n",
        "        entity_types.append(\"PERSON\")\n",
        "    if input(\"Organizations (y/n): \").lower() == 'y':\n",
        "        entity_types.append(\"ORG\")\n",
        "    if input(\"Dates (y/n): \").lower() == 'y':\n",
        "        entity_types.append(\"DATE\")\n",
        "    if input(\"Geopolitical Entities (y/n): \").lower() == 'y':\n",
        "        entity_types.append(\"GPE\")\n",
        "    if input(\"Locations (y/n): \").lower() == 'y':\n",
        "        entity_types.append(\"LOC\")\n",
        "\n",
        "    # Process the PDF\n",
        "    result, output_file = process_pdf(pdf_path, entity_types)\n",
        "\n",
        "    # Display the result\n",
        "    print(\"\\nResult:\")\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_md\n",
        "!python -m spacy download en_core_web_lg\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "mtJuxY0Izton"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5Whro5QiPH5",
        "outputId": "cf4414a1-c67b-470a-e427-32d11bb5831d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available NLP models: SpaCy Small, SpaCy Large\n",
            "Enter the path to your PDF file: /content/Weight_Measurement_and_Identification_of_Cow_Type_.pdf\n",
            "\n",
            "Available models:\n",
            "1. SpaCy Small\n",
            "2. SpaCy Large\n",
            "\n",
            "Select models (comma-separated numbers, e.g., 1,2): 2\n",
            "\n",
            "Enter output directory (leave blank for current directory): \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing /content/Weight_Measurement_and_Identification_of_Cow_Type_.pdf with models: SpaCy Large\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## Animal Entity Extraction Comparison\n",
            "\n",
            "### Summary\n",
            "| Entity Type | SpaCy Large |\n",
            "|---|---|\n",
            "| SPECIES | 81 |\n",
            "| ANIMAL_GROUP | 0 |\n",
            "| ANIMAL_BODY_PART | 2 |\n",
            "| MEASUREMENT | 3 |\n",
            "| LENGTH | 2 |\n",
            "| HABITAT | 1 |\n",
            "\n",
            "### Processing Times\n",
            "| Model | Time (seconds) |\n",
            "|---|---|\n",
            "| SpaCy Large | 4.90 |\n",
            "\n",
            "## Results from SpaCy Large\n",
            "\n",
            "### SPECIES (81 found)\n",
            "- Agriculture and\n",
            "- Agriculture has\n",
            "- All content\n",
            "- Angus cattle\n",
            "- Angus cows\n",
            "- Angus type\n",
            "- Another method\n",
            "- Article in\n",
            "- At this\n",
            "- Body weight\n",
            "- Brahma cows\n",
            "- Brahman cow\n",
            "- Calculation of\n",
            "- Calculation results\n",
            "- Carcass characteristics\n",
            "- Characteristic features\n",
            "- Computer vision\n",
            "- Corresponding author\n",
            "- Cow\n",
            "- Cows are\n",
            "- ... and 61 more (see Excel file for complete list)\n",
            "\n",
            "### ANIMAL_GROUP\n",
            "No entities of this type found.\n",
            "\n",
            "### ANIMAL_BODY_PART (2 found)\n",
            "- horn\n",
            "- tail\n",
            "\n",
            "### MEASUREMENT (3 found)\n",
            "- 260 kg\n",
            "- 363 kg\n",
            "- 416 kg\n",
            "\n",
            "### LENGTH (2 found)\n",
            "- Body Length\n",
            "- body length\n",
            "\n",
            "### HABITAT (1 found)\n",
            "- pen\n",
            "\n",
            "\n",
            "Processing completed in 6.05 seconds.\n",
            "\n",
            "Results saved to Excel file: ./animal_entity_extraction_comparison.xlsx\n",
            "Comparison chart saved to: ./animal_entity_comparison_chart.png\n"
          ]
        }
      ],
      "source": [
        "import pdfplumber\n",
        "import spacy\n",
        "import re\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "import os\n",
        "from io import BytesIO\n",
        "import logging\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.StreamHandler()]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Dictionary to store loaded models\n",
        "nlp_models = {}\n",
        "\n",
        "# Available NLP models\n",
        "AVAILABLE_MODELS = {\n",
        "    \"SpaCy Small\": \"en_core_web_sm\",\n",
        "    \"SpaCy Large\": \"en_core_web_lg\",\n",
        "}\n",
        "\n",
        "# Function to load NLP model\n",
        "def load_nlp_model(model_name):\n",
        "    if model_name in nlp_models:\n",
        "        return nlp_models[model_name]\n",
        "\n",
        "    model_path = AVAILABLE_MODELS.get(model_name)\n",
        "    if not model_path:\n",
        "        logger.error(f\"Unknown model: {model_name}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"Loading model: {model_name} ({model_path})\")\n",
        "        nlp = spacy.load(model_path)\n",
        "\n",
        "        # Add animal entity patterns\n",
        "        if \"entity_ruler\" not in nlp.pipe_names:\n",
        "            ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "            patterns = [\n",
        "                # Improved SPECIES pattern for scientific names (Genus species)\n",
        "                {\"label\": \"SPECIES\", \"pattern\": [\n",
        "                    {\"TEXT\": {\"REGEX\": \"^[A-Z][a-z]+$\"}},  # Genus - capitalized word\n",
        "                    {\"TEXT\": {\"REGEX\": \"^[a-z]+$\"}}        # species - lowercase word\n",
        "                ]},\n",
        "                # Common animal names pattern\n",
        "                {\"label\": \"SPECIES\", \"pattern\": [\n",
        "                    {\"LOWER\": {\"IN\": [\"dog\", \"cat\", \"horse\", \"cow\", \"pig\", \"sheep\", \"goat\", \"chicken\",\n",
        "                                    \"fish\", \"bird\", \"elephant\", \"lion\", \"tiger\", \"bear\", \"wolf\",\n",
        "                                    \"fox\", \"deer\", \"rabbit\", \"rat\", \"mouse\", \"cattle\", \"poultry\"]}}\n",
        "                ]},\n",
        "                # Animal names with adjectives\n",
        "                {\"label\": \"SPECIES\", \"pattern\": [\n",
        "                    {\"LOWER\": {\"IN\": [\"domestic\", \"farm\", \"wild\", \"laboratory\", \"companion\", \"pet\"]}},\n",
        "                    {\"LOWER\": {\"IN\": [\"animal\", \"animals\", \"species\"]}}\n",
        "                ]},\n",
        "                {\"label\": \"ANIMAL_GROUP\", \"pattern\": [\n",
        "                    {\"LOWER\": {\"IN\": [\"herd\", \"flock\", \"pack\", \"pod\", \"school\", \"colony\", \"pride\", \"swarm\", \"murder\"]}}\n",
        "                ]},\n",
        "                {\"label\": \"ANIMAL_BODY_PART\", \"pattern\": [\n",
        "                    {\"LOWER\": {\"IN\": [\"paw\", \"claw\", \"hoof\", \"beak\", \"wing\", \"fin\", \"tail\", \"horn\", \"tusk\", \"scale\",\n",
        "                                    \"feather\", \"fur\", \"mane\", \"whisker\", \"snout\", \"bill\"]}}\n",
        "                ]},\n",
        "                {\"label\": \"MEASUREMENT\", \"pattern\": [\n",
        "                    {\"SHAPE\": {\"IN\": [\"d+\", \"d+.d+\"]}},\n",
        "                    {\"LOWER\": {\"IN\": [\"mm\", \"cm\", \"m\", \"kg\", \"g\", \"ml\", \"l\"]}}\n",
        "                ]},\n",
        "                {\"label\": \"LENGTH\", \"pattern\": [\n",
        "                    {\"LOWER\": {\"IN\": [\"fork\", \"total\", \"standard\", \"body\", \"tail\"]}},\n",
        "                    {\"LOWER\": \"length\"}\n",
        "                ]},\n",
        "                {\"label\": \"HABITAT\", \"pattern\": [\n",
        "                    {\"LOWER\": {\"IN\": [\"forest\", \"jungle\", \"savanna\", \"ocean\", \"lake\", \"river\", \"mountain\",\n",
        "                                    \"desert\", \"tundra\", \"reef\", \"farm\", \"pasture\", \"pen\", \"enclosure\", \"cage\"]}}\n",
        "                ]},\n",
        "            ]\n",
        "            ruler.add_patterns(patterns)\n",
        "\n",
        "        nlp_models[model_name] = nlp\n",
        "        logger.info(f\"Model {model_name} loaded successfully\")\n",
        "        return nlp\n",
        "    except OSError as e:\n",
        "        logger.warning(f\"Error loading SpaCy model {model_path}: {str(e)}\")\n",
        "        logger.warning(f\"Please download it using: python -m spacy download {model_path}\")\n",
        "\n",
        "        # Create a blank model as fallback\n",
        "        nlp = spacy.blank(\"en\")\n",
        "        nlp_models[model_name] = nlp\n",
        "        return nlp\n",
        "\n",
        "# Check which models are installed\n",
        "def get_available_models():\n",
        "    installed_models = []\n",
        "    for name, path in AVAILABLE_MODELS.items():\n",
        "        try:\n",
        "            spacy.load(path)\n",
        "            installed_models.append(name)\n",
        "        except OSError:\n",
        "            pass\n",
        "\n",
        "    # Ensure at least one model is available\n",
        "    if not installed_models:\n",
        "        try:\n",
        "            spacy.load(\"en_core_web_sm\")\n",
        "            installed_models.append(\"SpaCy Small\")\n",
        "        except OSError:\n",
        "            installed_models.append(\"Blank Model\")\n",
        "            logger.warning(\"No SpaCy models installed. Please run: python -m spacy download en_core_web_sm\")\n",
        "\n",
        "    return installed_models\n",
        "\n",
        "# Function to extract text from PDF with better error handling\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    if not pdf_file or not os.path.exists(pdf_file):\n",
        "        return \"No file provided or file does not exist.\"\n",
        "\n",
        "    try:\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(pdf_file) as pdf:\n",
        "            total_pages = len(pdf.pages)\n",
        "            logger.info(f\"Processing PDF with {total_pages} pages\")\n",
        "\n",
        "            for i, page in enumerate(pdf.pages):\n",
        "                logger.info(f\"Extracting text from page {i+1}/{total_pages}\")\n",
        "                extracted_text = page.extract_text()\n",
        "                if extracted_text:\n",
        "                    text += extracted_text + \"\\n\"\n",
        "\n",
        "        if not text.strip():\n",
        "            return \"No text could be extracted from the PDF.\"\n",
        "\n",
        "        logger.info(f\"Successfully extracted {len(text)} characters of text\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing PDF: {str(e)}\", exc_info=True)\n",
        "        return f\"Error processing PDF: {str(e)}\"\n",
        "\n",
        "# Function to extract entities with better error handling and performance\n",
        "def extract_entities(text, entity_types, model_name):\n",
        "    if isinstance(text, str) and not text.startswith(\"Error\"):\n",
        "        try:\n",
        "            # Get the specified model\n",
        "            nlp = nlp_models.get(model_name)\n",
        "            if nlp is None:\n",
        "                nlp = load_nlp_model(model_name)\n",
        "                if nlp is None:\n",
        "                    return f\"Could not load NLP model: {model_name}\"\n",
        "\n",
        "            # Limit text size to avoid memory issues\n",
        "            if len(text) > 100000:\n",
        "                logger.warning(f\"Text is very large ({len(text)} chars). Limiting to first 100k characters.\")\n",
        "                text = text[:100000]  # Limit to first 100k characters\n",
        "\n",
        "            logger.info(f\"Processing text with {model_name} ({len(text)} chars)\")\n",
        "            doc = nlp(text)\n",
        "\n",
        "            # Create a dictionary to store extracted entities\n",
        "            extracted = {entity_type: [] for entity_type in entity_types}\n",
        "\n",
        "            # Extract required entity types\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_ in entity_types:\n",
        "                    extracted[ent.label_].append(ent.text)\n",
        "\n",
        "            # Special case for animal scientific species names\n",
        "            if \"SPECIES\" in entity_types:\n",
        "                logger.info(\"Extracting scientific species names with regex\")\n",
        "\n",
        "                # Additional check for common animal names (with capitalized nouns)\n",
        "                common_animals = re.findall(r'\\b(?:African|Asian|American|European|Australian|Western|Eastern|Northern|Southern)?\\s*(?:Lion|Tiger|Bear|Elephant|Giraffe|Rhinoceros|Zebra|Cheetah|Leopard|Wolf|Fox|Deer|Moose|Eagle|Hawk|Owl|Parrot|Dolphin|Whale|Shark|Turtle|Snake|Crocodile|Alligator|Frog|Toad|Fish|Salmon|Trout|Bass|Butterfly|Bee|Ant|Spider)\\b', text)\n",
        "                extracted[\"SPECIES\"].extend(common_animals)\n",
        "\n",
        "            # Special case for animal groups\n",
        "            if \"ANIMAL_GROUP\" in entity_types:\n",
        "                logger.info(\"Extracting animal group terms with regex\")\n",
        "                groups = re.findall(r'\\b(?:herd|flock|pack|pod|school|colony|pride|swarm|murder)\\s+of\\s+\\w+\\b', text, re.IGNORECASE)\n",
        "                extracted[\"ANIMAL_GROUP\"].extend(groups)\n",
        "\n",
        "            # Special case for body parts\n",
        "            if \"ANIMAL_BODY_PART\" in entity_types:\n",
        "                logger.info(\"Extracting animal body parts with regex\")\n",
        "                body_parts = re.findall(r'\\b(?:paw|claw|hoof|beak|wing|fin|tail|horn|tusk|scale|feather|fur|mane|whisker|snout|beak|bill)\\b', text, re.IGNORECASE)\n",
        "                extracted[\"ANIMAL_BODY_PART\"].extend(body_parts)\n",
        "\n",
        "            # Special case for habitats\n",
        "            if \"HABITAT\" in entity_types:\n",
        "                logger.info(\"Extracting habitat terms with regex\")\n",
        "                habitats = re.findall(r'\\b(?:forest|jungle|savanna|ocean|lake|river|mountain|desert|tundra|reef|wetland|marsh|grassland|meadow)\\b', text, re.IGNORECASE)\n",
        "                extracted[\"HABITAT\"].extend(habitats)\n",
        "\n",
        "            # Special case for measurements - improved regex\n",
        "            if \"MEASUREMENT\" in entity_types:\n",
        "                logger.info(\"Extracting measurements with regex\")\n",
        "                measurements = re.findall(r'\\b\\d+\\.?\\d*\\s*(?:mm|cm|m|kg|g|ml|l)\\b', text)\n",
        "                extracted[\"MEASUREMENT\"].extend(measurements)\n",
        "\n",
        "            # Special case for length measurements\n",
        "            if \"LENGTH\" in entity_types:\n",
        "                logger.info(\"Extracting length measurements with regex\")\n",
        "                length_patterns = re.findall(r'(?:fork|total|standard|body|tail)?\\s*length\\s*(?:of)?\\s*\\d+\\.?\\d*\\s*(?:mm|cm|m)', text, re.IGNORECASE)\n",
        "                extracted[\"LENGTH\"].extend(length_patterns)\n",
        "\n",
        "            # Remove duplicates and sort\n",
        "            for entity_type in entity_types:\n",
        "                extracted[entity_type] = sorted(list(set(extracted[entity_type])))\n",
        "                logger.info(f\"Found {len(extracted[entity_type])} unique {entity_type} entities with {model_name}\")\n",
        "\n",
        "            return extracted\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting entities with {model_name}: {str(e)}\", exc_info=True)\n",
        "            return f\"Error extracting entities with {model_name}: {str(e)}\"\n",
        "    else:\n",
        "        return f\"Cannot extract entities: {text}\"\n",
        "\n",
        "# Function to create Excel file with comparison\n",
        "def create_comparison_excel(extracted_by_model, entity_types, models, output_path=None):\n",
        "    try:\n",
        "        # Create a temporary file if no output path is provided\n",
        "        if output_path is None:\n",
        "            temp_dir = tempfile.mkdtemp()\n",
        "            output_file = os.path.join(temp_dir, \"animal_entity_extraction_comparison.xlsx\")\n",
        "        else:\n",
        "            output_file = output_path\n",
        "\n",
        "        logger.info(f\"Creating comparison Excel file at {output_file}\")\n",
        "        with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
        "            # Add summary sheet\n",
        "            summary_data = []\n",
        "            for entity_type in entity_types:\n",
        "                for model in models:\n",
        "                    if model in extracted_by_model:\n",
        "                        count = len(extracted_by_model[model].get(entity_type, []))\n",
        "                        summary_data.append({\n",
        "                            'Model': model,\n",
        "                            'Entity Type': entity_type,\n",
        "                            'Count': count\n",
        "                        })\n",
        "\n",
        "            summary_df = pd.DataFrame(summary_data)\n",
        "            summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
        "\n",
        "            # Create comparison workbook\n",
        "            workbook = writer.book\n",
        "\n",
        "            # Add chart sheet\n",
        "            chart_sheet = workbook.add_worksheet('Comparison Chart')\n",
        "\n",
        "            # Create a chart for each entity type\n",
        "            for i, entity_type in enumerate(entity_types):\n",
        "                chart_data = []\n",
        "                for model in models:\n",
        "                    if model in extracted_by_model:\n",
        "                        count = len(extracted_by_model[model].get(entity_type, []))\n",
        "                        chart_data.append({\n",
        "                            'Model': model,\n",
        "                            'Count': count\n",
        "                        })\n",
        "\n",
        "                # Create chart data\n",
        "                chart_df = pd.DataFrame(chart_data)\n",
        "                chart_df.to_excel(writer, sheet_name=f'Chart_{entity_type[:20]}', index=False)\n",
        "\n",
        "                # Add chart\n",
        "                chart = workbook.add_chart({'type': 'column'})\n",
        "                chart.add_series({\n",
        "                    'name': entity_type,\n",
        "                    'categories': f'=Chart_{entity_type[:20]}!$A$2:$A${len(models)+1}',\n",
        "                    'values': f'=Chart_{entity_type[:20]}!$B$2:$B${len(models)+1}',\n",
        "                })\n",
        "\n",
        "                chart.set_title({'name': f'{entity_type} Count by Model'})\n",
        "                chart.set_x_axis({'name': 'Model'})\n",
        "                chart.set_y_axis({'name': 'Count'})\n",
        "\n",
        "                # Position charts in a grid\n",
        "                row = (i // 2) * 15\n",
        "                col = (i % 2) * 8\n",
        "                chart_sheet.insert_chart(row, col, chart)\n",
        "\n",
        "            # Add individual sheets for each entity type and model\n",
        "            for model in models:\n",
        "                if model in extracted_by_model:\n",
        "                    for entity_type in entity_types:\n",
        "                        if extracted_by_model[model].get(entity_type, []):\n",
        "                            # Sanitize sheet name (Excel has a 31 character limit)\n",
        "                            sheet_name = f\"{model[:15]}_{entity_type[:15]}\"\n",
        "                            df = pd.DataFrame({entity_type: extracted_by_model[model].get(entity_type, [])})\n",
        "                            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "            # Add unique/shared entities analysis\n",
        "            if len(models) > 1:\n",
        "                for entity_type in entity_types:\n",
        "                    all_entities = set()\n",
        "                    for model in models:\n",
        "                        if model in extracted_by_model:\n",
        "                            all_entities.update(extracted_by_model[model].get(entity_type, []))\n",
        "\n",
        "                    comparison_data = []\n",
        "                    for entity in all_entities:\n",
        "                        row = {'Entity': entity}\n",
        "                        for model in models:\n",
        "                            if model in extracted_by_model:\n",
        "                                row[model] = entity in extracted_by_model[model].get(entity_type, [])\n",
        "                        comparison_data.append(row)\n",
        "\n",
        "                    if comparison_data:\n",
        "                        sheet_name = f\"Comp_{entity_type[:20]}\"\n",
        "                        comp_df = pd.DataFrame(comparison_data)\n",
        "                        comp_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "        return output_file\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating Excel file: {str(e)}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "# Function to generate comparison visualization\n",
        "def generate_comparison_chart(extracted_by_model, entity_types, models, output_path=None):\n",
        "    try:\n",
        "        if output_path is None:\n",
        "            temp_dir = tempfile.mkdtemp()\n",
        "            chart_file = os.path.join(temp_dir, \"animal_comparison_chart.png\")\n",
        "        else:\n",
        "            chart_file = output_path\n",
        "\n",
        "        # Set up the plot\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # Prepare data\n",
        "        x = np.arange(len(entity_types))\n",
        "        width = 0.8 / len(models)\n",
        "\n",
        "        # Plot bars for each model\n",
        "        for i, model in enumerate(models):\n",
        "            if model in extracted_by_model:\n",
        "                counts = [len(extracted_by_model[model].get(entity_type, [])) for entity_type in entity_types]\n",
        "                offset = width * i - width * (len(models) - 1) / 2\n",
        "                ax.bar(x + offset, counts, width, label=model)\n",
        "\n",
        "        # Add labels and title\n",
        "        ax.set_ylabel('Number of Entities')\n",
        "        ax.set_title('Animal Entity Extraction Comparison by Model')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(entity_types)\n",
        "        ax.legend()\n",
        "\n",
        "        # Save the figure\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(chart_file)\n",
        "        plt.close()\n",
        "\n",
        "        return chart_file\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating comparison chart: {str(e)}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "# Main function for processing PDFs with multiple models\n",
        "def process_pdf_multi_model(pdf_path, entity_types, selected_models, output_dir=None):\n",
        "    if not pdf_path or not os.path.exists(pdf_path):\n",
        "        return \"Please provide a valid PDF file path.\", None, None, None\n",
        "\n",
        "    if not entity_types:\n",
        "        return \"Please specify at least one entity type to extract.\", None, None, None\n",
        "\n",
        "    if not selected_models:\n",
        "        return \"Please specify at least one NLP model.\", None, None, None\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        if output_dir and not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        # Extract text from PDF\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        text_extraction_time = time.time() - start_time\n",
        "        logger.info(f\"Text extraction completed in {text_extraction_time:.2f} seconds\")\n",
        "\n",
        "        if isinstance(text, str) and text.startswith(\"Error\"):\n",
        "            return text, None, None, None\n",
        "\n",
        "        # Extract entities using each selected model\n",
        "        extracted_by_model = {}\n",
        "        entity_extraction_times = {}\n",
        "\n",
        "        for model_name in selected_models:\n",
        "            model_start_time = time.time()\n",
        "            extracted = extract_entities(text, entity_types, model_name)\n",
        "            extraction_time = time.time() - model_start_time\n",
        "\n",
        "            if isinstance(extracted, str) and extracted.startswith(\"Error\"):\n",
        "                extracted_by_model[model_name] = {\"ERROR\": [extracted]}\n",
        "            else:\n",
        "                extracted_by_model[model_name] = extracted\n",
        "\n",
        "            entity_extraction_times[model_name] = extraction_time\n",
        "            logger.info(f\"Entity extraction with {model_name} completed in {extraction_time:.2f} seconds\")\n",
        "\n",
        "        # Prepare comparison results\n",
        "        result_text = \"## Animal Entity Extraction Comparison\\n\\n\"\n",
        "        result_text += \"### Summary\\n\"\n",
        "\n",
        "        # Add summary table to the result text\n",
        "        result_text += \"| Entity Type | \" + \" | \".join(selected_models) + \" |\\n\"\n",
        "        result_text += \"|\" + \"---|\" * (len(selected_models) + 1) + \"\\n\"\n",
        "\n",
        "        for entity_type in entity_types:\n",
        "            result_text += f\"| {entity_type} |\"\n",
        "            for model_name in selected_models:\n",
        "                if model_name in extracted_by_model and entity_type in extracted_by_model[model_name]:\n",
        "                    count = len(extracted_by_model[model_name][entity_type])\n",
        "                    result_text += f\" {count} |\"\n",
        "                else:\n",
        "                    result_text += \" 0 |\"\n",
        "            result_text += \"\\n\"\n",
        "\n",
        "        result_text += \"\\n### Processing Times\\n\"\n",
        "        result_text += \"| Model | Time (seconds) |\\n\"\n",
        "        result_text += \"|---|---|\\n\"\n",
        "        for model_name, time_taken in entity_extraction_times.items():\n",
        "            result_text += f\"| {model_name} | {time_taken:.2f} |\\n\"\n",
        "\n",
        "        # Add detailed results for each model\n",
        "        for model_name in selected_models:\n",
        "            result_text += f\"\\n## Results from {model_name}\\n\\n\"\n",
        "\n",
        "            if model_name in extracted_by_model:\n",
        "                if \"ERROR\" in extracted_by_model[model_name]:\n",
        "                    result_text += f\"Error: {extracted_by_model[model_name]['ERROR'][0]}\\n\\n\"\n",
        "                    continue\n",
        "\n",
        "                for entity_type in entity_types:\n",
        "                    entities = extracted_by_model[model_name].get(entity_type, [])\n",
        "                    if entities:\n",
        "                        result_text += f\"### {entity_type} ({len(entities)} found)\\n\"\n",
        "                        # Show only the first 20 items to keep it manageable\n",
        "                        for item in entities[:20]:\n",
        "                            result_text += f\"- {item}\\n\"\n",
        "                        if len(entities) > 20:\n",
        "                            result_text += f\"- ... and {len(entities) - 20} more (see Excel file for complete list)\\n\"\n",
        "                        result_text += \"\\n\"\n",
        "                    else:\n",
        "                        result_text += f\"### {entity_type}\\n\"\n",
        "                        result_text += \"No entities of this type found.\\n\\n\"\n",
        "            else:\n",
        "                result_text += \"No results available.\\n\\n\"\n",
        "\n",
        "        # Find unique entities\n",
        "        if len(selected_models) > 1:\n",
        "            result_text += \"\\n## Unique and Common Entities\\n\\n\"\n",
        "\n",
        "            for entity_type in entity_types:\n",
        "                result_text += f\"### {entity_type}\\n\"\n",
        "\n",
        "                # Count entities per model\n",
        "                entity_counts = Counter()\n",
        "                all_entities = set()\n",
        "\n",
        "                for model_name in selected_models:\n",
        "                    if model_name in extracted_by_model and entity_type in extracted_by_model[model_name]:\n",
        "                        entities = set(extracted_by_model[model_name][entity_type])\n",
        "                        all_entities.update(entities)\n",
        "                        entity_counts.update(entities)\n",
        "\n",
        "                # Find common entities (present in all models)\n",
        "                common_entities = [entity for entity, count in entity_counts.items()\n",
        "                                  if count == len(selected_models)]\n",
        "\n",
        "                # Find unique entities (present in only one model)\n",
        "                unique_by_model = {}\n",
        "                for model_name in selected_models:\n",
        "                    if model_name in extracted_by_model and entity_type in extracted_by_model[model_name]:\n",
        "                        model_entities = set(extracted_by_model[model_name][entity_type])\n",
        "                        unique_entities = [entity for entity in model_entities\n",
        "                                          if entity_counts[entity] == 1]\n",
        "                        if unique_entities:\n",
        "                            unique_by_model[model_name] = unique_entities\n",
        "\n",
        "                # Report common entities\n",
        "                if common_entities:\n",
        "                    result_text += f\"- **Common across all models**: {len(common_entities)} entities\\n\"\n",
        "                    if len(common_entities) <= 5:\n",
        "                        for entity in common_entities:\n",
        "                            result_text += f\"  - {entity}\\n\"\n",
        "                    else:\n",
        "                        for entity in common_entities[:5]:\n",
        "                            result_text += f\"  - {entity}\\n\"\n",
        "                        result_text += f\"  - ... and {len(common_entities) - 5} more\\n\"\n",
        "                else:\n",
        "                    result_text += \"- No common entities found across all models\\n\"\n",
        "\n",
        "                # Report unique entities by model\n",
        "                for model_name, unique_entities in unique_by_model.items():\n",
        "                    result_text += f\"- **Unique to {model_name}**: {len(unique_entities)} entities\\n\"\n",
        "                    if len(unique_entities) <= 3:\n",
        "                        for entity in unique_entities:\n",
        "                            result_text += f\"  - {entity}\\n\"\n",
        "                    else:\n",
        "                        for entity in list(unique_entities)[:3]:\n",
        "                            result_text += f\"  - {entity}\\n\"\n",
        "                        result_text += f\"  - ... and {len(unique_entities) - 3} more\\n\"\n",
        "\n",
        "                result_text += \"\\n\"\n",
        "\n",
        "        # Create Excel file\n",
        "        excel_output_path = None\n",
        "        if output_dir:\n",
        "            excel_output_path = os.path.join(output_dir, \"animal_entity_extraction_comparison.xlsx\")\n",
        "\n",
        "        excel_start_time = time.time()\n",
        "        output_file = create_comparison_excel(extracted_by_model, entity_types, selected_models, excel_output_path)\n",
        "        excel_creation_time = time.time() - excel_start_time\n",
        "        logger.info(f\"Excel file creation completed in {excel_creation_time:.2f} seconds\")\n",
        "\n",
        "        # Create comparison chart\n",
        "        chart_output_path = None\n",
        "        if output_dir:\n",
        "            chart_output_path = os.path.join(output_dir, \"animal_entity_comparison_chart.png\")\n",
        "\n",
        "        chart_file = generate_comparison_chart(extracted_by_model, entity_types, selected_models, chart_output_path)\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        result_text += f\"\\nProcessing completed in {total_time:.2f} seconds.\"\n",
        "\n",
        "        # Provide a sample of the extracted text for reference\n",
        "        text_sample = text[:500] + \"...\" if len(text) > 500 else text\n",
        "\n",
        "        if output_file:\n",
        "            logger.info(f\"Results saved to {output_file}\")\n",
        "            return result_text, output_file, text_sample, chart_file\n",
        "        else:\n",
        "            return result_text + \"\\nFailed to create Excel file.\", None, text_sample, None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n",
        "        return f\"Unexpected error: {str(e)}\", None, None, None\n",
        "\n",
        "# Install missing models helper function\n",
        "def install_model_message():\n",
        "    print(\"\\nTo install SpaCy models, use the following commands:\")\n",
        "    print(\"python -m spacy download en_core_web_sm  # Small model (fastest)\")\n",
        "    print(\"python -m spacy download en_core_web_md  # Medium model\")\n",
        "    print(\"python -m spacy download en_core_web_lg  # Large model (most accurate)\")\n",
        "    print(\"python -m spacy download en_core_web_trf  # Transformer model (most accurate but slowest)\")\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Print available models\n",
        "    available_models = get_available_models()\n",
        "    print(f\"Available NLP models: {', '.join(available_models)}\")\n",
        "\n",
        "    if len(available_models) < 2:\n",
        "        print(\"Only one model is available. For comparison, install additional models.\")\n",
        "        install_model_message()\n",
        "\n",
        "    # Example usage\n",
        "    pdf_path = input(\"Enter the path to your PDF file: \")\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(\"File not found!\")\n",
        "        return\n",
        "\n",
        "    # Define animal-specific entity types\n",
        "    entity_types = [\"SPECIES\", \"ANIMAL_GROUP\", \"ANIMAL_BODY_PART\", \"MEASUREMENT\", \"LENGTH\", \"HABITAT\"]\n",
        "\n",
        "    # Let user select models\n",
        "    print(\"\\nAvailable models:\")\n",
        "    for i, model in enumerate(available_models):\n",
        "        print(f\"{i+1}. {model}\")\n",
        "\n",
        "    model_indices = input(\"\\nSelect models (comma-separated numbers, e.g., 1,2): \")\n",
        "    selected_models = []\n",
        "    try:\n",
        "        indices = [int(idx.strip()) for idx in model_indices.split(\",\")]\n",
        "        for idx in indices:\n",
        "            if 1 <= idx <= len(available_models):\n",
        "                selected_models.append(available_models[idx-1])\n",
        "    except:\n",
        "        print(\"Invalid selection. Using the first available model.\")\n",
        "        selected_models = [available_models[0]]\n",
        "\n",
        "    # Define output directory\n",
        "    output_dir = input(\"\\nEnter output directory (leave blank for current directory): \")\n",
        "    if not output_dir:\n",
        "        output_dir = \".\"\n",
        "\n",
        "    # Process the PDF\n",
        "    print(f\"\\nProcessing {pdf_path} with models: {', '.join(selected_models)}\")\n",
        "    results, excel_file, text_sample, chart_file = process_pdf_multi_model(\n",
        "        pdf_path, entity_types, selected_models, output_dir\n",
        "    )\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + results)\n",
        "\n",
        "    if excel_file:\n",
        "        print(f\"\\nResults saved to Excel file: {excel_file}\")\n",
        "\n",
        "    if chart_file:\n",
        "        print(f\"Comparison chart saved to: {chart_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt1Grq_DzT1F"
      },
      "source": [
        "\\--------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import spacy\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import logging\n",
        "import time\n",
        "import os\n",
        "import tempfile\n",
        "from io import BytesIO\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.StreamHandler()]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Dictionary to store loaded models\n",
        "nlp_models = {}\n",
        "\n",
        "# Available NLP models\n",
        "AVAILABLE_MODELS = {\n",
        "    \"SpaCy Small\": \"en_core_web_sm\",\n",
        "    \"SpaCy Large\": \"en_core_web_lg\",\n",
        "}\n",
        "\n",
        "# Function to load NLP model\n",
        "def load_nlp_model(model_name):\n",
        "    if model_name in nlp_models:\n",
        "        return nlp_models[model_name]\n",
        "\n",
        "    model_path = AVAILABLE_MODELS.get(model_name)\n",
        "    if not model_path:\n",
        "        logger.error(f\"Unknown model: {model_name}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"Loading model: {model_name} ({model_path})\")\n",
        "        nlp = spacy.load(model_path)\n",
        "\n",
        "        # Add animal entity patterns\n",
        "        if \"entity_ruler\" not in nlp.pipe_names:\n",
        "            ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "            patterns = [\n",
        "                # Improved SPECIES pattern for scientific names (Genus species)\n",
        "                {\"label\": \"SPECIES\", \"pattern\": [\n",
        "                    {\"TEXT\": {\"REGEX\": \"^[A-Z][a-z]+$\"}},  # Genus - capitalized word\n",
        "                    {\"TEXT\": {\"REGEX\": \"^[a-z]+$\"}}        # species - lowercase word\n",
        "                ]},\n",
        "                # Common animal names pattern\n",
        "                {\"label\": \"SPECIES\", \"pattern\": [\n",
        "                    {\"LOWER\": {\"IN\": [\"dog\", \"cat\", \"horse\", \"cow\", \"pig\", \"sheep\", \"goat\", \"chicken\",\n",
        "                                    \"fish\", \"bird\", \"elephant\", \"lion\", \"tiger\", \"bear\", \"wolf\",\n",
        "                                    \"fox\", \"deer\", \"rabbit\", \"rat\", \"mouse\", \"cattle\", \"poultry\"]}}\n",
        "                ]},\n",
        "                # Animal names with adjectives\n",
        "                {\"label\": \"SPECIES\", \"pattern\": [\n",
        "                    {\"LOWER\": {\"IN\": [\"domestic\", \"farm\", \"wild\", \"laboratory\", \"companion\", \"pet\"]}},\n",
        "                    {\"LOWER\": {\"IN\": [\"animal\", \"animals\", \"species\"]}}\n",
        "                ]},\n",
        "                {\"label\": \"ANIMAL_GROUP\", \"pattern\": [\n",
        "                    {\"LOWER\": {\"IN\": [\"herd\", \"flock\", \"pack\", \"pod\", \"school\", \"colony\", \"pride\", \"swarm\", \"murder\"]}}\n",
        "                ]},\n",
        "                {\"label\": \"ANIMAL_BODY_PART\", \"pattern\": [\n",
        "                    {\"LOWER\": {\"IN\": [\"paw\", \"claw\", \"hoof\", \"beak\", \"wing\", \"fin\", \"tail\", \"horn\", \"tusk\", \"scale\",\n",
        "                                    \"feather\", \"fur\", \"mane\", \"whisker\", \"snout\", \"bill\"]}}\n",
        "                ]},\n",
        "                {\"label\": \"MEASUREMENT\", \"pattern\": [\n",
        "                    {\"SHAPE\": {\"IN\": [\"d+\", \"d+.d+\"]}},\n",
        "                    {\"LOWER\": {\"IN\": [\"mm\", \"cm\", \"m\", \"kg\", \"g\", \"ml\", \"l\"]}}\n",
        "                ]},\n",
        "                {\"label\": \"LENGTH\", \"pattern\": [\n",
        "                    {\"LOWER\": {\"IN\": [\"fork\", \"total\", \"standard\", \"body\", \"tail\"]}},\n",
        "                    {\"LOWER\": \"length\"}\n",
        "                ]},\n",
        "                {\"label\": \"HABITAT\", \"pattern\": [\n",
        "                    {\"LOWER\": {\"IN\": [\"forest\", \"jungle\", \"savanna\", \"ocean\", \"lake\", \"river\", \"mountain\",\n",
        "                                    \"desert\", \"tundra\", \"reef\", \"farm\", \"pasture\", \"pen\", \"enclosure\", \"cage\"]}}\n",
        "                ]},\n",
        "            ]\n",
        "            ruler.add_patterns(patterns)\n",
        "\n",
        "        nlp_models[model_name] = nlp\n",
        "        logger.info(f\"Model {model_name} loaded successfully\")\n",
        "        return nlp\n",
        "    except OSError as e:\n",
        "        logger.warning(f\"Error loading SpaCy model {model_path}: {str(e)}\")\n",
        "        logger.warning(f\"Please download it using: python -m spacy download {model_path}\")\n",
        "\n",
        "        # Create a blank model as fallback\n",
        "        nlp = spacy.blank(\"en\")\n",
        "        nlp_models[model_name] = nlp\n",
        "        return nlp\n",
        "\n",
        "# Check which models are installed\n",
        "def get_available_models():\n",
        "    installed_models = []\n",
        "    for name, path in AVAILABLE_MODELS.items():\n",
        "        try:\n",
        "            spacy.load(path)\n",
        "            installed_models.append(name)\n",
        "        except OSError:\n",
        "            pass\n",
        "\n",
        "    # Ensure at least one model is available\n",
        "    if not installed_models:\n",
        "        try:\n",
        "            spacy.load(\"en_core_web_sm\")\n",
        "            installed_models.append(\"SpaCy Small\")\n",
        "        except OSError:\n",
        "            installed_models.append(\"Blank Model\")\n",
        "            logger.warning(\"No SpaCy models installed. Please run: python -m spacy download en_core_web_sm\")\n",
        "\n",
        "    return installed_models\n",
        "\n",
        "# Function to extract text from PDF with better error handling\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    if not pdf_file or not os.path.exists(pdf_file):\n",
        "        return \"No file provided or file does not exist.\"\n",
        "\n",
        "    try:\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(pdf_file) as pdf:\n",
        "            total_pages = len(pdf.pages)\n",
        "            logger.info(f\"Processing PDF with {total_pages} pages\")\n",
        "\n",
        "            for i, page in enumerate(pdf.pages):\n",
        "                logger.info(f\"Extracting text from page {i+1}/{total_pages}\")\n",
        "                extracted_text = page.extract_text()\n",
        "                if extracted_text:\n",
        "                    text += extracted_text + \"\\n\"\n",
        "\n",
        "        if not text.strip():\n",
        "            return \"No text could be extracted from the PDF.\"\n",
        "\n",
        "        logger.info(f\"Successfully extracted {len(text)} characters of text\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing PDF: {str(e)}\", exc_info=True)\n",
        "        return f\"Error processing PDF: {str(e)}\"\n",
        "\n",
        "# Function to extract entities with better error handling and performance\n",
        "def extract_entities(text, entity_types, model_name):\n",
        "    if isinstance(text, str) and not text.startswith(\"Error\"):\n",
        "        try:\n",
        "            # Get the specified model\n",
        "            nlp = nlp_models.get(model_name)\n",
        "            if nlp is None:\n",
        "                nlp = load_nlp_model(model_name)\n",
        "                if nlp is None:\n",
        "                    return f\"Could not load NLP model: {model_name}\"\n",
        "\n",
        "            # Limit text size to avoid memory issues\n",
        "            if len(text) > 100000:\n",
        "                logger.warning(f\"Text is very large ({len(text)} chars). Limiting to first 100k characters.\")\n",
        "                text = text[:100000]  # Limit to first 100k characters\n",
        "\n",
        "            logger.info(f\"Processing text with {model_name} ({len(text)} chars)\")\n",
        "            doc = nlp(text)\n",
        "\n",
        "            # Create a dictionary to store extracted entities\n",
        "            extracted = {entity_type: [] for entity_type in entity_types}\n",
        "\n",
        "            # Extract required entity types\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_ in entity_types:\n",
        "                    extracted[ent.label_].append(ent.text)\n",
        "\n",
        "            # Special case for animal scientific species names\n",
        "            if \"SPECIES\" in entity_types:\n",
        "                logger.info(\"Extracting scientific species names with regex\")\n",
        "\n",
        "                # Additional check for common animal names (with capitalized nouns)\n",
        "                common_animals = re.findall(r'\\b(?:African|Asian|American|European|Australian|Western|Eastern|Northern|Southern)?\\s*(?:Lion|Tiger|Bear|Elephant|Giraffe|Rhinoceros|Zebra|Cheetah|Leopard|Wolf|Fox|Deer|Moose|Eagle|Hawk|Owl|Parrot|Dolphin|Whale|Shark|Turtle|Snake|Crocodile|Alligator|Frog|Toad|Fish|Salmon|Trout|Bass|Butterfly|Bee|Ant|Spider)\\b', text)\n",
        "                extracted[\"SPECIES\"].extend(common_animals)\n",
        "\n",
        "            # Special case for animal groups\n",
        "            if \"ANIMAL_GROUP\" in entity_types:\n",
        "                logger.info(\"Extracting animal group terms with regex\")\n",
        "                groups = re.findall(r'\\b(?:herd|flock|pack|pod|school|colony|pride|swarm|murder)\\s+of\\s+\\w+\\b', text, re.IGNORECASE)\n",
        "                extracted[\"ANIMAL_GROUP\"].extend(groups)\n",
        "\n",
        "            # Special case for body parts\n",
        "            if \"ANIMAL_BODY_PART\" in entity_types:\n",
        "                logger.info(\"Extracting animal body parts with regex\")\n",
        "                body_parts = re.findall(r'\\b(?:paw|claw|hoof|beak|wing|fin|tail|horn|tusk|scale|feather|fur|mane|whisker|snout|beak|bill)\\b', text, re.IGNORECASE)\n",
        "                extracted[\"ANIMAL_BODY_PART\"].extend(body_parts)\n",
        "\n",
        "            # Special case for habitats\n",
        "            if \"HABITAT\" in entity_types:\n",
        "                logger.info(\"Extracting habitat terms with regex\")\n",
        "                habitats = re.findall(r'\\b(?:forest|jungle|savanna|ocean|lake|river|mountain|desert|tundra|reef|wetland|marsh|grassland|meadow)\\b', text, re.IGNORECASE)\n",
        "                extracted[\"HABITAT\"].extend(habitats)\n",
        "\n",
        "            # Special case for measurements - improved regex\n",
        "            if \"MEASUREMENT\" in entity_types:\n",
        "                logger.info(\"Extracting measurements with regex\")\n",
        "                measurements = re.findall(r'\\b\\d+\\.?\\d*\\s*(?:mm|cm|m|kg|g|ml|l)\\b', text)\n",
        "                extracted[\"MEASUREMENT\"].extend(measurements)\n",
        "\n",
        "            # Special case for length measurements\n",
        "            if \"LENGTH\" in entity_types:\n",
        "                logger.info(\"Extracting length measurements with regex\")\n",
        "                length_patterns = re.findall(r'(?:fork|total|standard|body|tail)?\\s*length\\s*(?:of)?\\s*\\d+\\.?\\d*\\s*(?:mm|cm|m)', text, re.IGNORECASE)\n",
        "                extracted[\"LENGTH\"].extend(length_patterns)\n",
        "\n",
        "            # Remove duplicates and sort\n",
        "            for entity_type in entity_types:\n",
        "                extracted[entity_type] = sorted(list(set(extracted[entity_type])))\n",
        "                logger.info(f\"Found {len(extracted[entity_type])} unique {entity_type} entities with {model_name}\")\n",
        "\n",
        "            return extracted\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting entities with {model_name}: {str(e)}\", exc_info=True)\n",
        "            return f\"Error extracting entities with {model_name}: {str(e)}\"\n",
        "    else:\n",
        "        return f\"Cannot extract entities: {text}\"\n",
        "\n",
        "# Function to generate comparison visualization\n",
        "def generate_comparison_chart(extracted_by_model, entity_types, models, output_path=None):\n",
        "    try:\n",
        "        if output_path is None:\n",
        "            temp_dir = tempfile.mkdtemp()\n",
        "            chart_file = os.path.join(temp_dir, \"animal_comparison_chart.png\")\n",
        "        else:\n",
        "            chart_file = output_path\n",
        "\n",
        "        # Set up the plot\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # Prepare data\n",
        "        x = np.arange(len(entity_types))\n",
        "        width = 0.8 / len(models)\n",
        "\n",
        "        # Plot bars for each model\n",
        "        for i, model in enumerate(models):\n",
        "            if model in extracted_by_model:\n",
        "                counts = [len(extracted_by_model[model].get(entity_type, [])) for entity_type in entity_types]\n",
        "                offset = width * i - width * (len(models) - 1) / 2\n",
        "                ax.bar(x + offset, counts, width, label=model)\n",
        "\n",
        "        # Add labels and title\n",
        "        ax.set_ylabel('Number of Entities')\n",
        "        ax.set_title('Animal Entity Extraction Comparison by Model')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(entity_types)\n",
        "        ax.legend()\n",
        "\n",
        "        # Save the figure\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(chart_file)\n",
        "        plt.close()\n",
        "\n",
        "        return chart_file\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating comparison chart: {str(e)}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "# Main function for processing PDFs with multiple models\n",
        "def process_pdf_multi_model(pdf_path, entity_types, selected_models, output_dir=None):\n",
        "    if not pdf_path or not os.path.exists(pdf_path):\n",
        "        return \"Please provide a valid PDF file path.\", None, None\n",
        "\n",
        "    if not entity_types:\n",
        "        return \"Please specify at least one entity type to extract.\", None, None\n",
        "\n",
        "    if not selected_models:\n",
        "        return \"Please specify at least one NLP model.\", None, None\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        if output_dir and not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        # Extract text from PDF\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        text_extraction_time = time.time() - start_time\n",
        "        logger.info(f\"Text extraction completed in {text_extraction_time:.2f} seconds\")\n",
        "\n",
        "        if isinstance(text, str) and text.startswith(\"Error\"):\n",
        "            return text, None, None\n",
        "\n",
        "        # Extract entities using each selected model\n",
        "        extracted_by_model = {}\n",
        "        entity_extraction_times = {}\n",
        "\n",
        "        for model_name in selected_models:\n",
        "            model_start_time = time.time()\n",
        "            extracted = extract_entities(text, entity_types, model_name)\n",
        "            extraction_time = time.time() - model_start_time\n",
        "\n",
        "            if isinstance(extracted, str) and extracted.startswith(\"Error\"):\n",
        "                extracted_by_model[model_name] = {\"ERROR\": [extracted]}\n",
        "            else:\n",
        "                extracted_by_model[model_name] = extracted\n",
        "\n",
        "            entity_extraction_times[model_name] = extraction_time\n",
        "            logger.info(f\"Entity extraction with {model_name} completed in {extraction_time:.2f} seconds\")\n",
        "\n",
        "        # Prepare comparison results\n",
        "        result_text = \"## Animal Entity Extraction Comparison\\n\\n\"\n",
        "        result_text += \"### Summary\\n\"\n",
        "\n",
        "        # Add summary table to the result text\n",
        "        result_text += \"| Entity Type | \" + \" | \".join(selected_models) + \" |\\n\"\n",
        "        result_text += \"|\" + \"---|\" * (len(selected_models) + 1) + \"\\n\"\n",
        "\n",
        "        for entity_type in entity_types:\n",
        "            result_text += f\"| {entity_type} |\"\n",
        "            for model_name in selected_models:\n",
        "                if model_name in extracted_by_model and entity_type in extracted_by_model[model_name]:\n",
        "                    count = len(extracted_by_model[model_name][entity_type])\n",
        "                    result_text += f\" {count} |\"\n",
        "                else:\n",
        "                    result_text += \" 0 |\"\n",
        "            result_text += \"\\n\"\n",
        "\n",
        "        result_text += \"\\n### Processing Times\\n\"\n",
        "        result_text += \"| Model | Time (seconds) |\\n\"\n",
        "        result_text += \"|---|---|\\n\"\n",
        "        for model_name, time_taken in entity_extraction_times.items():\n",
        "            result_text += f\"| {model_name} | {time_taken:.2f} |\\n\"\n",
        "\n",
        "        # Add detailed results for each model\n",
        "        for model_name in selected_models:\n",
        "            result_text += f\"\\n## Results from {model_name}\\n\\n\"\n",
        "\n",
        "            if model_name in extracted_by_model:\n",
        "                if \"ERROR\" in extracted_by_model[model_name]:\n",
        "                    result_text += f\"Error: {extracted_by_model[model_name]['ERROR'][0]}\\n\\n\"\n",
        "                    continue\n",
        "\n",
        "                for entity_type in entity_types:\n",
        "                    entities = extracted_by_model[model_name].get(entity_type, [])\n",
        "                    if entities:\n",
        "                        result_text += f\"### {entity_type} ({len(entities)} found)\\n\"\n",
        "                        # Show all items\n",
        "                        for item in entities:\n",
        "                            result_text += f\"- {item}\\n\"\n",
        "                        result_text += \"\\n\"\n",
        "                    else:\n",
        "                        result_text += f\"### {entity_type}\\n\"\n",
        "                        result_text += \"No entities of this type found.\\n\\n\"\n",
        "            else:\n",
        "                result_text += \"No results available.\\n\\n\"\n",
        "\n",
        "        # Find unique entities\n",
        "        if len(selected_models) > 1:\n",
        "            result_text += \"\\n## Unique and Common Entities\\n\\n\"\n",
        "\n",
        "            for entity_type in entity_types:\n",
        "                result_text += f\"### {entity_type}\\n\"\n",
        "\n",
        "                # Count entities per model\n",
        "                entity_counts = Counter()\n",
        "                all_entities = set()\n",
        "\n",
        "                for model_name in selected_models:\n",
        "                    if model_name in extracted_by_model and entity_type in extracted_by_model[model_name]:\n",
        "                        entities = set(extracted_by_model[model_name][entity_type])\n",
        "                        all_entities.update(entities)\n",
        "                        entity_counts.update(entities)\n",
        "\n",
        "                # Find common entities (present in all models)\n",
        "                common_entities = [entity for entity, count in entity_counts.items()\n",
        "                                  if count == len(selected_models)]\n",
        "\n",
        "                # Find unique entities (present in only one model)\n",
        "                unique_by_model = {}\n",
        "                for model_name in selected_models:\n",
        "                    if model_name in extracted_by_model and entity_type in extracted_by_model[model_name]:\n",
        "                        model_entities = set(extracted_by_model[model_name][entity_type])\n",
        "                        unique_entities = [entity for entity in model_entities\n",
        "                                          if entity_counts[entity] == 1]\n",
        "                        if unique_entities:\n",
        "                            unique_by_model[model_name] = unique_entities\n",
        "\n",
        "                # Report common entities\n",
        "                if common_entities:\n",
        "                    result_text += f\"- **Common across all models**: {len(common_entities)} entities\\n\"\n",
        "                    # Show all common entities\n",
        "                    for entity in common_entities:\n",
        "                        result_text += f\"  - {entity}\\n\"\n",
        "                else:\n",
        "                    result_text += \"- No common entities found across all models\\n\"\n",
        "\n",
        "                # Report unique entities by model\n",
        "                for model_name, unique_entities in unique_by_model.items():\n",
        "                    result_text += f\"- **Unique to {model_name}**: {len(unique_entities)} entities\\n\"\n",
        "                    # Show all unique entities\n",
        "                    for entity in unique_entities:\n",
        "                        result_text += f\"  - {entity}\\n\"\n",
        "\n",
        "                result_text += \"\\n\"\n",
        "\n",
        "        # Create comparison chart\n",
        "        chart_output_path = None\n",
        "        if output_dir:\n",
        "            chart_output_path = os.path.join(output_dir, \"animal_entity_comparison_chart.png\")\n",
        "\n",
        "        chart_file = generate_comparison_chart(extracted_by_model, entity_types, selected_models, chart_output_path)\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        result_text += f\"\\nProcessing completed in {total_time:.2f} seconds.\"\n",
        "\n",
        "        # Provide a sample of the extracted text for reference\n",
        "        text_sample = text[:500] + \"...\" if len(text) > 500 else text\n",
        "\n",
        "        if chart_file:\n",
        "            logger.info(f\"Comparison chart saved to: {chart_file}\")\n",
        "            return result_text, text_sample, chart_file\n",
        "        else:\n",
        "            return result_text, text_sample, None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n",
        "        return f\"Unexpected error: {str(e)}\", None, None\n",
        "\n",
        "# Install missing models helper function\n",
        "def install_model_message():\n",
        "    print(\"\\nTo install SpaCy models, use the following commands:\")\n",
        "    print(\"python -m spacy download en_core_web_sm  # Small model (fastest)\")\n",
        "    print(\"python -m spacy download en_core_web_md  # Medium model\")\n",
        "    print(\"python -m spacy download en_core_web_lg  # Large model (most accurate)\")\n",
        "    print(\"python -m spacy download en_core_web_trf  # Transformer model (most accurate but slowest)\")\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Print available models\n",
        "    available_models = get_available_models()\n",
        "    print(f\"Available NLP models: {', '.join(available_models)}\")\n",
        "\n",
        "    if len(available_models) < 2:\n",
        "        print(\"Only one model is available. For comparison, install additional models.\")\n",
        "        install_model_message()\n",
        "\n",
        "    # Example usage\n",
        "    pdf_path = input(\"Enter the path to your PDF file: \")\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(\"File not found!\")\n",
        "        return\n",
        "\n",
        "    # Define animal-specific entity types\n",
        "    entity_types = [\"SPECIES\", \"ANIMAL_GROUP\", \"ANIMAL_BODY_PART\", \"MEASUREMENT\", \"LENGTH\", \"HABITAT\"]\n",
        "\n",
        "    # Let user select models\n",
        "    print(\"\\nAvailable models:\")\n",
        "    for i, model in enumerate(available_models):\n",
        "        print(f\"{i+1}. {model}\")\n",
        "\n",
        "    model_indices = input(\"\\nSelect models (comma-separated numbers, e.g., 1,2): \")\n",
        "    selected_models = []\n",
        "    try:\n",
        "        indices = [int(idx.strip()) for idx in model_indices.split(\",\")]\n",
        "        for idx in indices:\n",
        "            if 1 <= idx <= len(available_models):\n",
        "                selected_models.append(available_models[idx-1])\n",
        "    except:\n",
        "        print(\"Invalid selection. Using the first available model.\")\n",
        "        selected_models = [available_models[0]]\n",
        "\n",
        "    # Define output directory\n",
        "    output_dir = input(\"\\nEnter output directory (leave blank for current directory): \")\n",
        "    if not output_dir:\n",
        "        output_dir = \".\"\n",
        "\n",
        "    # Process the PDF\n",
        "    print(f\"\\nProcessing {pdf_path} with models: {', '.join(selected_models)}\")\n",
        "    results, text_sample, chart_file = process_pdf_multi_model(\n",
        "        pdf_path, entity_types, selected_models, output_dir\n",
        "    )\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + results)\n",
        "\n",
        "    if chart_file:\n",
        "        print(f\"Comparison chart saved to: {chart_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYk_POAL6rVl",
        "outputId": "77a901d8-82ef-4c3c-fe50-5a721931aa72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available NLP models: SpaCy Small, SpaCy Large\n",
            "Enter the path to your PDF file: /content/story_frogs.pdf\n",
            "\n",
            "Available models:\n",
            "1. SpaCy Small\n",
            "2. SpaCy Large\n",
            "\n",
            "Select models (comma-separated numbers, e.g., 1,2): 2\n",
            "\n",
            "Enter output directory (leave blank for current directory): \n",
            "\n",
            "Processing /content/story_frogs.pdf with models: SpaCy Large\n",
            "\n",
            "## Animal Entity Extraction Comparison\n",
            "\n",
            "### Summary\n",
            "| Entity Type | SpaCy Large |\n",
            "|---|---|\n",
            "| SPECIES | 47 |\n",
            "| ANIMAL_GROUP | 1 |\n",
            "| ANIMAL_BODY_PART | 1 |\n",
            "| MEASUREMENT | 0 |\n",
            "| LENGTH | 0 |\n",
            "| HABITAT | 0 |\n",
            "\n",
            "### Processing Times\n",
            "| Model | Time (seconds) |\n",
            "|---|---|\n",
            "| SpaCy Large | 7.92 |\n",
            "\n",
            "## Results from SpaCy Large\n",
            "\n",
            "### SPECIES (47 found)\n",
            "- Africa in\n",
            "- As this\n",
            "- Both of\n",
            "- Despite all\n",
            "- Frogs are\n",
            "- Frogs have\n",
            "- Fruitful fecundity\n",
            "- He did\n",
            "- He put\n",
            "- He saw\n",
            "- In many\n",
            "- In the\n",
            "- In this\n",
            "- It has\n",
            "- Italian scientist\n",
            "- Long before\n",
            "- Long live\n",
            "- Princely models\n",
            "- Prolific egg\n",
            "- Researchers also\n",
            "- Scientists knew\n",
            "- Scientists watch\n",
            "- Size matters\n",
            "- That discovery\n",
            "- The dark\n",
            "- The egg\n",
            "- The eggs\n",
            "- The frog\n",
            "- The process\n",
            "- Their size\n",
            "- These eggs\n",
            "- These frog\n",
            "- These legs\n",
            "- They also\n",
            "- They were\n",
            "- This kind\n",
            "- Vive les\n",
            "- While it\n",
            "- Xenopus became\n",
            "- Xenopus cells\n",
            "- Xenopus eggs\n",
            "- Xenopus embryos\n",
            "- Xenopus female\n",
            "- Xenopus laevis\n",
            "- Xenopus produce\n",
            "- Xenopus tropicalis\n",
            "- Xenopus will\n",
            "\n",
            "### ANIMAL_GROUP (1 found)\n",
            "- school\n",
            "\n",
            "### ANIMAL_BODY_PART (1 found)\n",
            "- claw\n",
            "\n",
            "### MEASUREMENT\n",
            "No entities of this type found.\n",
            "\n",
            "### LENGTH\n",
            "No entities of this type found.\n",
            "\n",
            "### HABITAT\n",
            "No entities of this type found.\n",
            "\n",
            "\n",
            "Processing completed in 9.64 seconds.\n",
            "Comparison chart saved to: ./animal_entity_comparison_chart.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ll"
      ],
      "metadata": {
        "id": "c7l7YcqkBtxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import spacy\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import logging\n",
        "import time\n",
        "import os\n",
        "import tempfile\n",
        "from io import BytesIO\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.StreamHandler()]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Dictionary to store loaded models\n",
        "nlp_models = {}\n",
        "transformer_models = {}\n",
        "\n",
        "# Available NLP models\n",
        "AVAILABLE_MODELS = {\n",
        "    \"SpaCy Small\": \"en_core_web_sm\",\n",
        "    \"SpaCy Large\": \"en_core_web_lg\",\n",
        "    \"BioBERT\": \"dmis-lab/biobert-base-cased-v1.1-named-entity-recognition\"\n",
        "}\n",
        "\n",
        "# Function to load NLP model\n",
        "def load_nlp_model(model_name):\n",
        "    if model_name in nlp_models:\n",
        "        return nlp_models[model_name]\n",
        "\n",
        "    model_path = AVAILABLE_MODELS.get(model_name)\n",
        "    if not model_path:\n",
        "        logger.error(f\"Unknown model: {model_name}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # BioBERT uses HuggingFace Transformers\n",
        "        if model_name == \"BioBERT\":\n",
        "            logger.info(f\"Loading BioBERT model: {model_path}\")\n",
        "            try:\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "                model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "\n",
        "                # Create NER pipeline\n",
        "                ner_model = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)\n",
        "                transformer_models[model_name] = ner_model\n",
        "                nlp_models[model_name] = \"transformer\"\n",
        "                logger.info(f\"BioBERT model loaded successfully\")\n",
        "                return \"transformer\"\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading BioBERT model: {str(e)}\")\n",
        "                logger.warning(\"Please install transformers: pip install transformers torch\")\n",
        "                return None\n",
        "        else:\n",
        "            # SpaCy models\n",
        "            logger.info(f\"Loading model: {model_name} ({model_path})\")\n",
        "            nlp = spacy.load(model_path)\n",
        "\n",
        "            # Add animal entity patterns\n",
        "            if \"entity_ruler\" not in nlp.pipe_names:\n",
        "                ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "                patterns = [\n",
        "                    # Improved SPECIES pattern for scientific names (Genus species)\n",
        "                    {\"label\": \"SPECIES\", \"pattern\": [\n",
        "                        {\"TEXT\": {\"REGEX\": \"^[A-Z][a-z]+$\"}},  # Genus - capitalized word\n",
        "                        {\"TEXT\": {\"REGEX\": \"^[a-z]+$\"}}        # species - lowercase word\n",
        "                    ]},\n",
        "                    # Common animal names pattern\n",
        "                    {\"label\": \"SPECIES\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"dog\", \"cat\", \"horse\", \"cow\", \"pig\", \"sheep\", \"goat\", \"chicken\",\n",
        "                                        \"fish\", \"salmon\", \"trout\", \"bass\", \"butterfly\", \"bee\", \"ant\", \"spider\",\n",
        "                                        \"lion\", \"tiger\", \"bear\", \"elephant\", \"giraffe\", \"rhinoceros\", \"zebra\",\n",
        "                                        \"cheetah\", \"leopard\", \"wolf\", \"fox\", \"deer\", \"moose\", \"eagle\", \"hawk\",\n",
        "                                        \"owl\", \"parrot\", \"dolphin\", \"whale\", \"shark\", \"turtle\", \"snake\",\n",
        "                                        \"crocodile\", \"alligator\", \"frog\", \"toad\"]}}\n",
        "                    ]},\n",
        "                    # Animal groups with adjectives\n",
        "                    {\"label\": \"SPECIES\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"african\", \"asian\", \"american\", \"european\", \"australian\",\n",
        "                                         \"western\", \"eastern\", \"northern\", \"southern\"]}},\n",
        "                        {\"LOWER\": {\"IN\": [\"lion\", \"tiger\", \"bear\", \"elephant\", \"giraffe\", \"rhinoceros\",\n",
        "                                         \"zebra\", \"cheetah\", \"leopard\", \"wolf\", \"fox\", \"deer\", \"moose\",\n",
        "                                         \"eagle\", \"hawk\", \"owl\", \"parrot\", \"dolphin\", \"whale\", \"shark\",\n",
        "                                         \"turtle\", \"snake\", \"crocodile\", \"alligator\"]}}\n",
        "                    ]},\n",
        "                    # Animal groups\n",
        "                    {\"label\": \"ANIMAL_GROUP\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"herd\", \"flock\", \"pack\", \"pod\", \"school\", \"colony\", \"pride\",\n",
        "                                         \"swarm\", \"murder\", \"gaggle\", \"hive\", \"brood\", \"drift\"]}}\n",
        "                    ]},\n",
        "                    # Animal body parts\n",
        "                    {\"label\": \"ANIMAL_BODY_PART\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"paw\", \"claw\", \"hoof\", \"beak\", \"wing\", \"fin\", \"tail\", \"horn\",\n",
        "                                         \"tusk\", \"scale\", \"feather\", \"fur\", \"mane\", \"whisker\", \"snout\",\n",
        "                                         \"bill\", \"gills\", \"tentacle\", \"antenna\"]}}\n",
        "                    ]},\n",
        "                    # Measurements\n",
        "                    {\"label\": \"MEASUREMENT\", \"pattern\": [\n",
        "                        {\"SHAPE\": {\"IN\": [\"d+\", \"d+.d+\"]}},\n",
        "                        {\"LOWER\": {\"IN\": [\"mm\", \"cm\", \"m\", \"kg\", \"g\", \"ml\", \"l\"]}}\n",
        "                    ]},\n",
        "                    # Length measurements\n",
        "                    {\"label\": \"LENGTH\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"fork\", \"total\", \"standard\", \"body\", \"tail\"]}},\n",
        "                        {\"LOWER\": \"length\"}\n",
        "                    ]},\n",
        "                    # Habitats\n",
        "                    {\"label\": \"HABITAT\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"forest\", \"jungle\", \"savanna\", \"ocean\", \"lake\", \"river\", \"mountain\",\n",
        "                                        \"desert\", \"tundra\", \"reef\", \"farm\", \"pasture\", \"pen\", \"enclosure\",\n",
        "                                        \"cage\", \"burrow\", \"nest\", \"den\", \"wetland\", \"bog\", \"marsh\"]}}\n",
        "                    ]},\n",
        "                ]\n",
        "                ruler.add_patterns(patterns)\n",
        "\n",
        "            nlp_models[model_name] = nlp\n",
        "            logger.info(f\"SpaCy model {model_name} loaded successfully\")\n",
        "            return nlp\n",
        "    except OSError as e:\n",
        "        logger.warning(f\"Error loading model {model_path}: {str(e)}\")\n",
        "        if model_name.startswith(\"SpaCy\"):\n",
        "            logger.warning(f\"Please download it using: python -m spacy download {model_path}\")\n",
        "        elif model_name == \"BioBERT\":\n",
        "            logger.warning(\"Please install transformers: pip install transformers torch\")\n",
        "\n",
        "        # Create a blank model as fallback if it's a SpaCy model\n",
        "        if model_name.startswith(\"SpaCy\"):\n",
        "            nlp = spacy.blank(\"en\")\n",
        "            nlp_models[model_name] = nlp\n",
        "            return nlp\n",
        "        return None\n",
        "\n",
        "# Check which models are installed\n",
        "def get_available_models():\n",
        "    installed_models = []\n",
        "\n",
        "    # Check SpaCy models\n",
        "    for name, path in AVAILABLE_MODELS.items():\n",
        "        if name.startswith(\"SpaCy\"):\n",
        "            try:\n",
        "                spacy.load(path)\n",
        "                installed_models.append(name)\n",
        "            except OSError:\n",
        "                pass\n",
        "        elif name == \"BioBERT\":\n",
        "            try:\n",
        "                # Check if transformers is installed\n",
        "                import transformers\n",
        "\n",
        "                # Add BioBERT to available models regardless of whether it's downloaded\n",
        "                # We'll handle the download when it's first used\n",
        "                installed_models.append(name)\n",
        "\n",
        "                # Just log that we'll download when needed\n",
        "                logger.info(f\"BioBERT model will be downloaded automatically when first used.\")\n",
        "            except ImportError:\n",
        "                logger.warning(\"Transformers package not installed. BioBERT will not be available.\")\n",
        "\n",
        "    # Ensure at least one model is available\n",
        "    if not installed_models:\n",
        "        try:\n",
        "            spacy.load(\"en_core_web_sm\")\n",
        "            installed_models.append(\"SpaCy Small\")\n",
        "        except OSError:\n",
        "            installed_models.append(\"Blank Model\")\n",
        "            logger.warning(\"No SpaCy models installed. Please run: python -m spacy download en_core_web_sm\")\n",
        "            logger.warning(\"For BioBERT: pip install transformers torch\")\n",
        "\n",
        "    return installed_models\n",
        "\n",
        "# Function to extract text from PDF with better error handling\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    if not pdf_file or not os.path.exists(pdf_file):\n",
        "        return \"No file provided or file does not exist.\"\n",
        "\n",
        "    try:\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(pdf_file) as pdf:\n",
        "            total_pages = len(pdf.pages)\n",
        "            logger.info(f\"Processing PDF with {total_pages} pages\")\n",
        "\n",
        "            for i, page in enumerate(pdf.pages):\n",
        "                logger.info(f\"Extracting text from page {i+1}/{total_pages}\")\n",
        "                extracted_text = page.extract_text()\n",
        "                if extracted_text:\n",
        "                    text += extracted_text + \"\\n\"\n",
        "\n",
        "        if not text.strip():\n",
        "            return \"No text could be extracted from the PDF.\"\n",
        "\n",
        "        logger.info(f\"Successfully extracted {len(text)} characters of text\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing PDF: {str(e)}\", exc_info=True)\n",
        "        return f\"Error processing PDF: {str(e)}\"\n",
        "\n",
        "# Extract entities using BioBERT\n",
        "def extract_entities_biobert(text, entity_types):\n",
        "    try:\n",
        "        if \"BioBERT\" not in transformer_models:\n",
        "            model_path = AVAILABLE_MODELS.get(\"BioBERT\")\n",
        "            try:\n",
        "                logger.info(f\"BioBERT model not loaded yet. Downloading and initializing...\")\n",
        "\n",
        "                # Set download settings\n",
        "                logger.info(\"Setting up to download BioBERT model...\")\n",
        "\n",
        "                # Print info about what's happening\n",
        "                print(\"\\nDownloading BioBERT model. This may take a few minutes on first run...\")\n",
        "                print(\"Model: dmis-lab/biobert-base-cased-v1.1-named-entity-recognition\")\n",
        "\n",
        "                # Load tokenizer with progress reporting\n",
        "                print(\"Loading tokenizer...\")\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "                # Load model with progress reporting\n",
        "                print(\"Loading model weights (this might take a while)...\")\n",
        "                model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "\n",
        "                print(\"Creating NER pipeline...\")\n",
        "                ner_model = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)\n",
        "\n",
        "                transformer_models[\"BioBERT\"] = ner_model\n",
        "                print(\"BioBERT model loaded successfully!\\n\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading BioBERT model: {str(e)}\")\n",
        "                print(f\"\\nError: Could not load BioBERT model. Details: {str(e)}\")\n",
        "                print(\"Common solutions:\")\n",
        "                print(\"1. Check internet connection\")\n",
        "                print(\"2. Ensure 'transformers' and 'torch' are installed: pip install transformers torch\")\n",
        "                print(\"3. Try again or use a different model\\n\")\n",
        "                return f\"Error: Could not load BioBERT model: {str(e)}\"\n",
        "\n",
        "        ner_model = transformer_models[\"BioBERT\"]\n",
        "\n",
        "        # Process text in chunks to avoid CUDA out of memory errors\n",
        "        max_length = 512  # Maximum length for BERT models\n",
        "        chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
        "\n",
        "        logger.info(f\"Processing {len(chunks)} chunks with BioBERT\")\n",
        "\n",
        "        all_entities = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            if i % 10 == 0:\n",
        "                logger.info(f\"Processing chunk {i+1}/{len(chunks)}\")\n",
        "\n",
        "            if not chunk.strip():\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                chunk_entities = ner_model(chunk)\n",
        "                all_entities.extend(chunk_entities)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error processing chunk {i}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Map BioBERT entity types to our types and extract entities\n",
        "        extracted = {entity_type: [] for entity_type in entity_types}\n",
        "\n",
        "        # BioBERT's default entity types: 'B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'\n",
        "        # Map BioBERT entity types to our custom types\n",
        "        entity_mapping = {\n",
        "            'B-MISC': ['ANIMAL_BODY_PART', 'MEASUREMENT', 'LENGTH'],\n",
        "            'I-MISC': ['ANIMAL_BODY_PART', 'MEASUREMENT', 'LENGTH'],\n",
        "            'B-PER': ['SPECIES'],  # Sometimes species names are recognized as persons\n",
        "            'I-PER': ['SPECIES'],\n",
        "            'B-ORG': ['ANIMAL_GROUP'],\n",
        "            'I-ORG': ['ANIMAL_GROUP'],\n",
        "            'B-LOC': ['HABITAT'],\n",
        "            'I-LOC': ['HABITAT'],\n",
        "        }\n",
        "\n",
        "        # Process entities\n",
        "        for entity in all_entities:\n",
        "            entity_text = entity['word']\n",
        "            entity_type = entity['entity']\n",
        "            score = entity['score']\n",
        "\n",
        "            # Only consider entities with confidence above threshold\n",
        "            if score < 0.75:\n",
        "                continue\n",
        "\n",
        "            # Map BioBERT entity types to our types\n",
        "            mapped_types = entity_mapping.get(entity_type, [])\n",
        "\n",
        "            # Add entity to all compatible types\n",
        "            for mapped_type in mapped_types:\n",
        "                if mapped_type in entity_types:\n",
        "                    extracted[mapped_type].append(entity_text)\n",
        "\n",
        "        # Additional regex-based extraction for specific entity types\n",
        "        # Scientific species names (Genus species)\n",
        "        if \"SPECIES\" in entity_types:\n",
        "            species_pattern = r'\\b[A-Z][a-z]+\\s+[a-z]+\\b'\n",
        "            scientific_names = re.findall(species_pattern, text)\n",
        "            extracted[\"SPECIES\"].extend(scientific_names)\n",
        "\n",
        "            # Common animal names\n",
        "            common_animals = re.findall(r'\\b(?:African|Asian|American|European|Australian|Western|Eastern|Northern|Southern)?\\s*(?:Lion|Tiger|Bear|Elephant|Giraffe|Rhinoceros|Zebra|Cheetah|Leopard|Wolf|Fox|Deer|Moose|Eagle|Hawk|Owl|Parrot|Dolphin|Whale|Shark|Turtle|Snake|Crocodile|Alligator|Frog|Toad|Fish|Salmon|Trout|Bass|Butterfly|Bee|Ant|Spider)\\b', text)\n",
        "            extracted[\"SPECIES\"].extend(common_animals)\n",
        "\n",
        "        # Animal groups\n",
        "        if \"ANIMAL_GROUP\" in entity_types:\n",
        "            groups = re.findall(r'\\b(?:herd|flock|pack|pod|school|colony|pride|swarm|murder|gaggle|hive|brood|drift)\\s+of\\s+\\w+\\b', text, re.IGNORECASE)\n",
        "            extracted[\"ANIMAL_GROUP\"].extend(groups)\n",
        "\n",
        "        # Body parts\n",
        "        if \"ANIMAL_BODY_PART\" in entity_types:\n",
        "            body_parts = re.findall(r'\\b(?:paw|claw|hoof|beak|wing|fin|tail|horn|tusk|scale|feather|fur|mane|whisker|snout|bill|gills|tentacle|antenna)s?\\b', text, re.IGNORECASE)\n",
        "            extracted[\"ANIMAL_BODY_PART\"].extend(body_parts)\n",
        "\n",
        "        # Measurements\n",
        "        if \"MEASUREMENT\" in entity_types:\n",
        "            measurements = re.findall(r'\\b\\d+\\.?\\d*\\s*(?:mm|cm|m|kg|g|ml|l)\\b', text)\n",
        "            extracted[\"MEASUREMENT\"].extend(measurements)\n",
        "\n",
        "        # Length measurements\n",
        "        if \"LENGTH\" in entity_types:\n",
        "            length_patterns = re.findall(r'(?:fork|total|standard|body|tail)?\\s*length\\s*(?:of)?\\s*\\d+\\.?\\d*\\s*(?:mm|cm|m)', text, re.IGNORECASE)\n",
        "            extracted[\"LENGTH\"].extend(length_patterns)\n",
        "\n",
        "        # Habitats\n",
        "        if \"HABITAT\" in entity_types:\n",
        "            habitats = re.findall(r'\\b(?:forest|jungle|savanna|ocean|lake|river|mountain|desert|tundra|reef|farm|pasture|pen|enclosure|cage|burrow|nest|den|wetland|bog|marsh)\\b', text, re.IGNORECASE)\n",
        "            extracted[\"HABITAT\"].extend(habitats)\n",
        "\n",
        "        # Remove duplicates and sort\n",
        "        for entity_type in entity_types:\n",
        "            extracted[entity_type] = sorted(list(set([e.strip() for e in extracted[entity_type] if e.strip()])))\n",
        "            logger.info(f\"Found {len(extracted[entity_type])} unique {entity_type} entities with BioBERT\")\n",
        "\n",
        "        return extracted\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting entities with BioBERT: {str(e)}\", exc_info=True)\n",
        "        return f\"Error extracting entities with BioBERT: {str(e)}\"\n",
        "\n",
        "# Function to extract entities with better error handling and performance\n",
        "def extract_entities(text, entity_types, model_name):\n",
        "    if isinstance(text, str) and not text.startswith(\"Error\"):\n",
        "        try:\n",
        "            # Special handling for BioBERT model\n",
        "            if model_name == \"BioBERT\":\n",
        "                return extract_entities_biobert(text, entity_types)\n",
        "\n",
        "            # Get the specified SpaCy model\n",
        "            nlp = load_nlp_model(model_name)\n",
        "            if nlp is None:\n",
        "                return f\"Could not load NLP model: {model_name}\"\n",
        "            if nlp == \"transformer\":\n",
        "                return extract_entities_biobert(text, entity_types)\n",
        "\n",
        "            # Limit text size for SpaCy models to avoid memory issues\n",
        "            if len(text) > 100000:\n",
        "                logger.warning(f\"Text is very large ({len(text)} chars). Limiting to first 100k characters.\")\n",
        "                text = text[:100000]  # Limit to first 100k characters\n",
        "\n",
        "            logger.info(f\"Processing text with {model_name} ({len(text)} chars)\")\n",
        "            doc = nlp(text)\n",
        "\n",
        "            # Create a dictionary to store extracted entities\n",
        "            extracted = {entity_type: [] for entity_type in entity_types}\n",
        "\n",
        "            # Extract required entity types\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_ in entity_types:\n",
        "                    extracted[ent.label_].append(ent.text)\n",
        "\n",
        "            # Special case for animal scientific species names\n",
        "            if \"SPECIES\" in entity_types:\n",
        "                logger.info(\"Extracting scientific species names with regex\")\n",
        "\n",
        "                # Scientific names pattern (Genus species)\n",
        "                scientific_names = re.findall(r'\\b[A-Z][a-z]+\\s+[a-z]+\\b', text)\n",
        "                extracted[\"SPECIES\"].extend(scientific_names)\n",
        "\n",
        "                # Additional check for common animal names (with capitalized nouns)\n",
        "                common_animals = re.findall(r'\\b(?:African|Asian|American|European|Australian|Western|Eastern|Northern|Southern)?\\s*(?:Lion|Tiger|Bear|Elephant|Giraffe|Rhinoceros|Zebra|Cheetah|Leopard|Wolf|Fox|Deer|Moose|Eagle|Hawk|Owl|Parrot|Dolphin|Whale|Shark|Turtle|Snake|Crocodile|Alligator|Frog|Toad|Fish|Salmon|Trout|Bass|Butterfly|Bee|Ant|Spider)\\b', text)\n",
        "                extracted[\"SPECIES\"].extend(common_animals)\n",
        "\n",
        "            # Special case for animal groups\n",
        "            if \"ANIMAL_GROUP\" in entity_types:\n",
        "                logger.info(\"Extracting animal group terms with regex\")\n",
        "                groups = re.findall(r'\\b(?:herd|flock|pack|pod|school|colony|pride|swarm|murder|gaggle|hive|brood|drift)\\s+of\\s+\\w+\\b', text, re.IGNORECASE)\n",
        "                extracted[\"ANIMAL_GROUP\"].extend(groups)\n",
        "\n",
        "            # Special case for body parts\n",
        "            if \"ANIMAL_BODY_PART\" in entity_types:\n",
        "                logger.info(\"Extracting animal body parts with regex\")\n",
        "                body_parts = re.findall(r'\\b(?:paw|claw|hoof|beak|wing|fin|tail|horn|tusk|scale|feather|fur|mane|whisker|snout|bill|gills|tentacle|antenna)s?\\b', text, re.IGNORECASE)\n",
        "                extracted[\"ANIMAL_BODY_PART\"].extend(body_parts)\n",
        "\n",
        "            # Special case for measurements\n",
        "            if \"MEASUREMENT\" in entity_types:\n",
        "                logger.info(\"Extracting measurements with regex\")\n",
        "                measurements = re.findall(r'\\b\\d+\\.?\\d*\\s*(?:mm|cm|m|kg|g|ml|l)\\b', text)\n",
        "                extracted[\"MEASUREMENT\"].extend(measurements)\n",
        "\n",
        "            # Special case for length measurements\n",
        "            if \"LENGTH\" in entity_types:\n",
        "                logger.info(\"Extracting length measurements with regex\")\n",
        "                length_patterns = re.findall(r'(?:fork|total|standard|body|tail)?\\s*length\\s*(?:of)?\\s*\\d+\\.?\\d*\\s*(?:mm|cm|m)', text, re.IGNORECASE)\n",
        "                extracted[\"LENGTH\"].extend(length_patterns)\n",
        "\n",
        "            # Special case for habitats\n",
        "            if \"HABITAT\" in entity_types:\n",
        "                logger.info(\"Extracting habitat terms with regex\")\n",
        "                habitats = re.findall(r'\\b(?:forest|jungle|savanna|ocean|lake|river|mountain|desert|tundra|reef|farm|pasture|pen|enclosure|cage|burrow|nest|den|wetland|bog|marsh)\\b', text, re.IGNORECASE)\n",
        "                extracted[\"HABITAT\"].extend(habitats)\n",
        "\n",
        "            # Remove duplicates and sort\n",
        "            for entity_type in entity_types:\n",
        "                extracted[entity_type] = sorted(list(set([e.strip() for e in extracted[entity_type] if e.strip()])))\n",
        "                logger.info(f\"Found {len(extracted[entity_type])} unique {entity_type} entities with {model_name}\")\n",
        "\n",
        "            return extracted\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting entities with {model_name}: {str(e)}\", exc_info=True)\n",
        "            return f\"Error extracting entities with {model_name}: {str(e)}\"\n",
        "    else:\n",
        "        return f\"Cannot extract entities: {text}\"\n",
        "\n",
        "# Function to generate comparison visualization\n",
        "def generate_comparison_chart(extracted_by_model, entity_types, models, output_path=None):\n",
        "    try:\n",
        "        if output_path is None:\n",
        "            temp_dir = tempfile.mkdtemp()\n",
        "            chart_file = os.path.join(temp_dir, \"animal_comparison_chart.png\")\n",
        "        else:\n",
        "            chart_file = output_path\n",
        "\n",
        "        # Set up the plot\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "        # Prepare data\n",
        "        x = np.arange(len(entity_types))\n",
        "        width = 0.8 / len(models)\n",
        "\n",
        "        # Define a color palette\n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
        "\n",
        "        # Plot bars for each model\n",
        "        for i, model in enumerate(models):\n",
        "            if model in extracted_by_model:\n",
        "                counts = [len(extracted_by_model[model].get(entity_type, [])) for entity_type in entity_types]\n",
        "                offset = width * i - width * (len(models) - 1) / 2\n",
        "                ax.bar(x + offset, counts, width, label=model, color=colors[i % len(colors)])\n",
        "\n",
        "        # Add labels and title\n",
        "        ax.set_ylabel('Number of Entities', fontsize=12)\n",
        "        ax.set_title('Comparison of Entity Extraction Between Models', fontsize=14, fontweight='bold')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(entity_types, rotation=45, ha='right', fontsize=10)\n",
        "        ax.legend(loc='best')\n",
        "\n",
        "        # Add value labels on top of each bar\n",
        "        for i, model in enumerate(models):\n",
        "            if model in extracted_by_model:\n",
        "                counts = [len(extracted_by_model[model].get(entity_type, [])) for entity_type in entity_types]\n",
        "                offset = width * i - width * (len(models) - 1) / 2\n",
        "                for j, count in enumerate(counts):\n",
        "                    ax.text(x[j] + offset, count + 0.5, str(count), ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "        # Add a grid for better readability\n",
        "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Improve layout\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(chart_file, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        return chart_file\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating comparison chart: {str(e)}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "# Main function for processing PDFs with multiple models\n",
        "def process_pdf_multi_model(pdf_path, entity_types, selected_models, output_dir=None):\n",
        "    if not pdf_path or not os.path.exists(pdf_path):\n",
        "        return \"Please provide a valid PDF file path.\", None, None\n",
        "\n",
        "    if not entity_types:\n",
        "        return \"Please specify at least one entity type to extract.\", None, None\n",
        "\n",
        "    if not selected_models:\n",
        "        return \"Please specify at least one NLP model.\", None, None\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        if output_dir and not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        # Extract text from PDF\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        text_extraction_time = time.time() - start_time\n",
        "        logger.info(f\"Text extraction completed in {text_extraction_time:.2f} seconds\")\n",
        "\n",
        "        if isinstance(text, str) and text.startswith(\"Error\"):\n",
        "            return text, None, None\n",
        "\n",
        "        # Extract entities using each selected model\n",
        "        extracted_by_model = {}\n",
        "        entity_extraction_times = {}\n",
        "\n",
        "        for model_name in selected_models:\n",
        "            model_start_time = time.time()\n",
        "            extracted = extract_entities(text, entity_types, model_name)\n",
        "            extraction_time = time.time() - model_start_time\n",
        "\n",
        "            if isinstance(extracted, str) and extracted.startswith(\"Error\"):\n",
        "                extracted_by_model[model_name] = {\"ERROR\": [extracted]}\n",
        "            else:\n",
        "                extracted_by_model[model_name] = extracted\n",
        "\n",
        "            entity_extraction_times[model_name] = extraction_time\n",
        "            logger.info(f\"Entity extraction with {model_name} completed in {extraction_time:.2f} seconds\")\n",
        "\n",
        "        # Prepare comparison results\n",
        "        result_text = \"# Animal Entity Extraction Analysis\\n\\n\"\n",
        "        result_text += \"## Summary of Results\\n\\n\"\n",
        "\n",
        "        # Add summary table to the result text\n",
        "        result_text += \"### Entity Count by Model\\n\\n\"\n",
        "        result_text += \"| Entity Type | \" + \" | \".join(selected_models) + \" |\\n\"\n",
        "        result_text += \"|\" + \"---|\" * (len(selected_models) + 1) + \"\\n\"\n",
        "\n",
        "        for entity_type in entity_types:\n",
        "            result_text += f\"| {entity_type} |\"\n",
        "            for model_name in selected_models:\n",
        "                if model_name in extracted_by_model and \"ERROR\" not in extracted_by_model[model_name]:\n",
        "                    count = len(extracted_by_model[model_name].get(entity_type, []))\n",
        "                    result_text += f\" {count} |\"\n",
        "                else:\n",
        "                    result_text += \" Error |\"\n",
        "            result_text += \"\\n\"\n",
        "\n",
        "        result_text += \"\\n### Processing Times\\n\\n\"\n",
        "        result_text += \"| Model | Processing Time (seconds) |\\n\"\n",
        "        result_text += \"|---|---|\\n\"\n",
        "        for model_name, time_taken in entity_extraction_times.items():\n",
        "            result_text += f\"| {model_name} | {time_taken:.2f} |\\n\"\n",
        "\n",
        "        # Add detailed results for each model\n",
        "        for model_name in selected_models:\n",
        "            result_text += f\"\\n## Detailed Results: {model_name}\\n\\n\"\n",
        "\n",
        "            if model_name in extracted_by_model:\n",
        "                if \"ERROR\" in extracted_by_model[model_name]:\n",
        "                    result_text += f\"Error: {extracted_by_model[model_name]['ERROR'][0]}\\n\\n\"\n",
        "                    continue\n",
        "\n",
        "                for entity_type in entity_types:\n",
        "                    entities = extracted_by_model[model_name].get(entity_type, [])\n",
        "                    if entities:\n",
        "                        result_text += f\"### {entity_type} ({len(entities)} found)\\n\\n\"\n",
        "                        # Show all items\n",
        "                        for item in entities:\n",
        "                            result_text += f\"- {item}\\n\"\n",
        "                        result_text += \"\\n\"\n",
        "                    else:\n",
        "                        result_text += f\"### {entity_type}\\n\\n\"\n",
        "                        result_text += \"No entities of this type found.\\n\\n\"\n",
        "            else:\n",
        "                result_text += \"No results available.\\n\\n\"\n",
        "\n",
        "        # Find unique entities\n",
        "        if len(selected_models) > 1:\n",
        "            result_text += \"\\n## Unique and Common Entities\\n\\n\"\n",
        "\n",
        "            for entity_type in entity_types:\n",
        "                result_text += f\"### {entity_type}\\n\\n\"\n",
        "\n",
        "                # Count entities per model\n",
        "                entity_counts = Counter()\n",
        "                all_entities = set()\n",
        "\n",
        "                # Skip models with errors\n",
        "                valid_models = []\n",
        "                for model_name in selected_models:\n",
        "                    if model_name in extracted_by_model and \"ERROR\" not in extracted_by_model[model_name]:\n",
        "                        valid_models.append(model_name)\n",
        "                        entities = set(extracted_by_model[model_name].get(entity_type, []))\n",
        "                        all_entities.update(entities)\n",
        "                        entity_counts.update(entities)\n",
        "\n",
        "                # Find common entities (present in all valid models)\n",
        "                common_entities = [entity for entity, count in entity_counts.items()\n",
        "                                  if count == len(valid_models) and len(valid_models) > 0]\n",
        "\n",
        "                # Find unique entities (present in only one model)\n",
        "                unique_by_model = {}\n",
        "                for model_name in valid_models:\n",
        "                    model_entities = set(extracted_by_model[model_name].get(entity_type, []))\n",
        "                    unique_entities = [entity for entity in model_entities\n",
        "                                      if entity_counts[entity] == 1]\n",
        "                    if unique_entities:\n",
        "                        unique_by_model[model_name] = unique_entities\n",
        "\n",
        "                # Report common entities\n",
        "                if common_entities:\n",
        "                    result_text += f\"**Common across all models**: {len(common_entities)} entities\\n\\n\"\n",
        "                    # Show all common entities\n",
        "                    for entity in sorted(common_entities):\n",
        "                        result_text += f\"- {entity}\\n\"\n",
        "                    result_text += \"\\n\"\n",
        "                else:\n",
        "                    result_text += \"No entities found in common across all models.\\n\\n\"\n",
        "\n",
        "                # Report unique entities by model\n",
        "                for model_name, unique_entities in unique_by_model.items():\n",
        "                    result_text += f\"**Unique to {model_name}**: {len(unique_entities)} entities\\n\\n\"\n",
        "                    # Show all unique entities\n",
        "                    for entity in sorted(unique_entities):\n",
        "                        result_text += f\"- {entity}\\n\"\n",
        "                    result_text += \"\\n\"\n",
        "\n",
        "        # Create comparison chart\n",
        "        chart_output_path = None\n",
        "        if output_dir:\n",
        "            chart_output_path = os.path.join(output_dir, \"entity_extraction_comparison.png\")\n",
        "        print(extracted_by_model)\n",
        "        chart_file = generate_comparison_chart(extracted_by_model, entity_types, selected_models, chart_output_path)\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        result_text += f\"\\n## Performance Summary\\n\\n\"\n",
        "        result_text += f\"Total processing time: {total_time:.2f} seconds\\n\"\n",
        "        result_text += f\"- Text extraction: {text_extraction_time:.2f} seconds\\n\"\n",
        "\n",
        "        # List extraction times by model\n",
        "        for model_name, model_time in entity_extraction_times.items():\n",
        "            result_text += f\"- {model_name} extraction: {model_time:.2f} seconds\\n\"\n",
        "\n",
        "        # Provide a sample of the extracted text for reference\n",
        "        text_sample = text[:500] + \"...\" if len(text) > 500 else text\n",
        "        result_text += f\"\\n## Sample of Extracted Text\\n\\n```\\n{text_sample}\\n```\\n\"\n",
        "\n",
        "        # Add model comparison summary\n",
        "        if len(selected_models) > 1:\n",
        "            result_text += \"\\n## Model Comparison Summary\\n\\n\"\n",
        "\n",
        "            # Calculate total entities extracted by each model\n",
        "            model_totals = {}\n",
        "            for model_name in selected_models:\n",
        "                if model_name in extracted_by_model and \"ERROR\" not in extracted_by_model[model_name]:\n",
        "                    total = sum(len(extracted_by_model[model_name].get(entity_type, [])) for entity_type in entity_types)\n",
        "                    model_totals[model_name] = total\n",
        "\n",
        "            # Sort models by total entities found\n",
        "            sorted_models = sorted(model_totals.keys(), key=lambda x: model_totals[x], reverse=True)\n",
        "\n",
        "            for i, model_name in enumerate(sorted_models):\n",
        "                result_text += f\"{i+1}. **{model_name}**: {model_totals[model_name]} total entities\"\n",
        "                result_text += f\" (processed in {entity_extraction_times[model_name]:.2f} seconds)\\n\"\n",
        "\n",
        "                # Add the strongest entity types for this model\n",
        "                strengths = []\n",
        "                for entity_type in entity_types:\n",
        "                    count = len(extracted_by_model[model_name].get(entity_type, []))\n",
        "                    strengths.append((entity_type, count))\n",
        "\n",
        "                # Sort strengths by count\n",
        "                strengths.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "                # List the top 3 strengths or all if less than 3\n",
        "                top_strengths = strengths[:min(3, len(strengths))]\n",
        "                if top_strengths:\n",
        "                    strength_text = \", \".join([f\"{ent_type}: {count}\" for ent_type, count in top_strengths])\n",
        "                    result_text += f\"   - Strongest in: {strength_text}\\n\"\n",
        "\n",
        "            result_text += \"\\n\"\n",
        "\n",
        "        if chart_file:\n",
        "            logger.info(f\"Comparison chart saved to: {chart_file}\")\n",
        "            return result_text, text_sample, chart_file\n",
        "        else:\n",
        "            return result_text, text_sample, None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n",
        "        return f\"Unexpected error: {str(e)}\", None, None\n",
        "\n",
        "# Install missing models helper function\n",
        "def install_model_message():\n",
        "    print(\"\\n===== Installation Instructions =====\")\n",
        "    print(\"\\nTo install required models, use the following commands:\")\n",
        "    print(\"\\n1. SpaCy models:\")\n",
        "    print(\"   python -m spacy download en_core_web_sm  # Small model (fastest)\")\n",
        "    print(\"   python -m spacy download en_core_web_lg  # Large model (most accurate)\")\n",
        "\n",
        "    print(\"\\n2. BioBERT requirements:\")\n",
        "    print(\"   pip install transformers torch  # Required for BioBERT\")\n",
        "    print(\"   # BioBERT model will be downloaded automatically when first used\")\n",
        "\n",
        "    print(\"\\n3. Other dependencies:\")\n",
        "    print(\"   pip install pdfplumber matplotlib numpy\")\n",
        "\n",
        "    print(\"\\nNote: BioBERT requires approximately 500MB of disk space when downloaded.\")\n",
        "    print(\"The first time you use BioBERT, it will download automatically.\")\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    print(\"\\n===============================================\")\n",
        "    print(\"  PDF Animal Entity Extractor with BioBERT\")\n",
        "    print(\"===============================================\")\n",
        "    print(\"\\nThis tool extracts animal-related entities from PDF documents.\")\n",
        "    print(\"It supports multiple NLP models, including BioBERT for biomedical text.\")\n",
        "\n",
        "    # Print available models\n",
        "    available_models = get_available_models()\n",
        "    print(f\"\\nAvailable NLP models: {', '.join(available_models)}\")\n",
        "\n",
        "    # Check for transformers installation\n",
        "    try:\n",
        "        import transformers\n",
        "        has_transformers = True\n",
        "    except ImportError:\n",
        "        has_transformers = False\n",
        "\n",
        "    if not has_transformers and \"BioBERT\" in available_models:\n",
        "        print(\"\\nWARNING: BioBERT is listed as available, but the 'transformers' package\")\n",
        "        print(\"is not installed. To use BioBERT, install required packages:\")\n",
        "        print(\"  pip install transformers torch\")\n",
        "\n",
        "    if len(available_models) < 2:\n",
        "        print(\"\\nOnly one model is available. For comparison, install additional models.\")\n",
        "        install_model_message()\n",
        "\n",
        "    # Example usage\n",
        "    pdf_path = input(\"Enter the path to your PDF file: \")\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(\"File not found!\")\n",
        "        return\n",
        "\n",
        "    # Define animal-specific entity types\n",
        "    entity_types = [\"SPECIES\", \"ANIMAL_GROUP\", \"ANIMAL_BODY_PART\", \"MEASUREMENT\", \"LENGTH\", \"HABITAT\"]\n",
        "\n",
        "    # Let user select entity types\n",
        "    print(\"\\nAvailable entity types:\")\n",
        "    for i, entity_type in enumerate(entity_types):\n",
        "        print(f\"{i+1}. {entity_type}\")\n",
        "\n",
        "    entity_indices = input(\"\\nSelect entity types to extract (comma-separated numbers, e.g., 1,2,3 or 'all'): \")\n",
        "    selected_entity_types = []\n",
        "\n",
        "    if entity_indices.lower() == 'all':\n",
        "        selected_entity_types = entity_types\n",
        "    else:\n",
        "        try:\n",
        "            indices = [int(idx.strip()) for idx in entity_indices.split(\",\")]\n",
        "            for idx in indices:\n",
        "                if 1 <= idx <= len(entity_types):\n",
        "                    selected_entity_types.append(entity_types[idx-1])\n",
        "        except:\n",
        "            print(\"Invalid selection. Using all entity types.\")\n",
        "            selected_entity_types = entity_types\n",
        "\n",
        "    if not selected_entity_types:\n",
        "        selected_entity_types = entity_types\n",
        "\n",
        "    print(f\"Selected entity types: {', '.join(selected_entity_types)}\")\n",
        "\n",
        "    # Let user select models\n",
        "    print(\"\\nAvailable models:\")\n",
        "    for i, model in enumerate(available_models):\n",
        "        print(f\"{i+1}. {model}\")\n",
        "\n",
        "    model_indices = input(\"\\nSelect models (comma-separated numbers, e.g., 1,2 or 'all'): \")\n",
        "    selected_models = []\n",
        "\n",
        "    if model_indices.lower() == 'all':\n",
        "        selected_models = available_models\n",
        "    else:\n",
        "        try:\n",
        "            indices = [int(idx.strip()) for idx in model_indices.split(\",\")]\n",
        "            for idx in indices:\n",
        "                if 1 <= idx <= len(available_models):\n",
        "                    selected_models.append(available_models[idx-1])\n",
        "        except:\n",
        "            print(\"Invalid selection. Using the first available model.\")\n",
        "            selected_models = [available_models[0]]\n",
        "\n",
        "    if not selected_models:\n",
        "        selected_models = [available_models[0]]\n",
        "\n",
        "    # Define output directory\n",
        "    output_dir = input(\"\\nEnter output directory (leave blank for current directory): \")\n",
        "    if not output_dir:\n",
        "        output_dir = \".\"\n",
        "\n",
        "    # Process the PDF\n",
        "    print(f\"\\nProcessing {pdf_path} with models: {', '.join(selected_models)}\")\n",
        "    print(f\"Extracting entity types: {', '.join(selected_entity_types)}\")\n",
        "\n",
        "    results, text_sample, chart_file = process_pdf_multi_model(\n",
        "        pdf_path, selected_entity_types, selected_models, output_dir\n",
        "    )\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + results)\n",
        "\n",
        "    if chart_file:\n",
        "        print(f\"Comparison chart saved to: {chart_file}\")\n",
        "\n",
        "    # Save results to file\n",
        "    results_file = os.path.join(output_dir, \"entity_extraction_results.md\")\n",
        "    try:\n",
        "        with open(results_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(results)\n",
        "        print(f\"Results saved to: {results_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving results to file: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAo9wxfVBhXc",
        "outputId": "415bf3d8-22ab-4f04-8709-b7209596eabf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "  PDF Animal Entity Extractor with BioBERT\n",
            "===============================================\n",
            "\n",
            "This tool extracts animal-related entities from PDF documents.\n",
            "It supports multiple NLP models, including BioBERT for biomedical text.\n",
            "\n",
            "Available NLP models: SpaCy Small, SpaCy Large, BioBERT\n",
            "Enter the path to your PDF file: /content/story_frogs.pdf\n",
            "\n",
            "Available entity types:\n",
            "1. SPECIES\n",
            "2. ANIMAL_GROUP\n",
            "3. ANIMAL_BODY_PART\n",
            "4. MEASUREMENT\n",
            "5. LENGTH\n",
            "6. HABITAT\n",
            "\n",
            "Select entity types to extract (comma-separated numbers, e.g., 1,2,3 or 'all'): \n",
            "Invalid selection. Using all entity types.\n",
            "Selected entity types: SPECIES, ANIMAL_GROUP, ANIMAL_BODY_PART, MEASUREMENT, LENGTH, HABITAT\n",
            "\n",
            "Available models:\n",
            "1. SpaCy Small\n",
            "2. SpaCy Large\n",
            "3. BioBERT\n",
            "\n",
            "Select models (comma-separated numbers, e.g., 1,2 or 'all'): \n",
            "Invalid selection. Using the first available model.\n",
            "\n",
            "Enter output directory (leave blank for current directory): \n",
            "\n",
            "Processing /content/story_frogs.pdf with models: SpaCy Small\n",
            "Extracting entity types: SPECIES, ANIMAL_GROUP, ANIMAL_BODY_PART, MEASUREMENT, LENGTH, HABITAT\n",
            "{'SpaCy Small': {'SPECIES': ['Africa in', 'African\\nclawed', 'As this', 'Both of', 'Despite all', 'Each\\ncell', 'Frogs\\nhave', 'Frogs are', 'Frogs have', 'Fruitful fecundity', 'He did', 'He didn', 'He put', 'He saw', 'In many', 'In the', 'In this', 'It has', 'Italian scientist', 'Long before', 'Long live', 'Princely models', 'Prolific egg', 'Researchers also', 'Scientists knew', 'Scientists watch', 'Size matters', 'Spallanzani outfit', 'That discovery', 'The dark', 'The egg', 'The eggs', 'The frog', 'The process', 'Their size', 'These eggs', 'These frog', 'These legs', 'They also', 'They were', 'This four', 'This kind', 'Vive les', 'While it', 'Xenopus\\ntadpoles', 'Xenopus became', 'Xenopus cells', 'Xenopus eggs', 'Xenopus embryos', 'Xenopus female', 'Xenopus laevis', 'Xenopus produce', 'Xenopus tropicalis', 'Xenopus will', 'frog'], 'ANIMAL_GROUP': ['school'], 'ANIMAL_BODY_PART': ['claw'], 'MEASUREMENT': [], 'LENGTH': [], 'HABITAT': []}}\n",
            "\n",
            "# Animal Entity Extraction Analysis\n",
            "\n",
            "## Summary of Results\n",
            "\n",
            "### Entity Count by Model\n",
            "\n",
            "| Entity Type | SpaCy Small |\n",
            "|---|---|\n",
            "| SPECIES | 55 |\n",
            "| ANIMAL_GROUP | 1 |\n",
            "| ANIMAL_BODY_PART | 1 |\n",
            "| MEASUREMENT | 0 |\n",
            "| LENGTH | 0 |\n",
            "| HABITAT | 0 |\n",
            "\n",
            "### Processing Times\n",
            "\n",
            "| Model | Processing Time (seconds) |\n",
            "|---|---|\n",
            "| SpaCy Small | 1.24 |\n",
            "\n",
            "## Detailed Results: SpaCy Small\n",
            "\n",
            "### SPECIES (55 found)\n",
            "\n",
            "- Africa in\n",
            "- African\n",
            "clawed\n",
            "- As this\n",
            "- Both of\n",
            "- Despite all\n",
            "- Each\n",
            "cell\n",
            "- Frogs\n",
            "have\n",
            "- Frogs are\n",
            "- Frogs have\n",
            "- Fruitful fecundity\n",
            "- He did\n",
            "- He didn\n",
            "- He put\n",
            "- He saw\n",
            "- In many\n",
            "- In the\n",
            "- In this\n",
            "- It has\n",
            "- Italian scientist\n",
            "- Long before\n",
            "- Long live\n",
            "- Princely models\n",
            "- Prolific egg\n",
            "- Researchers also\n",
            "- Scientists knew\n",
            "- Scientists watch\n",
            "- Size matters\n",
            "- Spallanzani outfit\n",
            "- That discovery\n",
            "- The dark\n",
            "- The egg\n",
            "- The eggs\n",
            "- The frog\n",
            "- The process\n",
            "- Their size\n",
            "- These eggs\n",
            "- These frog\n",
            "- These legs\n",
            "- They also\n",
            "- They were\n",
            "- This four\n",
            "- This kind\n",
            "- Vive les\n",
            "- While it\n",
            "- Xenopus\n",
            "tadpoles\n",
            "- Xenopus became\n",
            "- Xenopus cells\n",
            "- Xenopus eggs\n",
            "- Xenopus embryos\n",
            "- Xenopus female\n",
            "- Xenopus laevis\n",
            "- Xenopus produce\n",
            "- Xenopus tropicalis\n",
            "- Xenopus will\n",
            "- frog\n",
            "\n",
            "### ANIMAL_GROUP (1 found)\n",
            "\n",
            "- school\n",
            "\n",
            "### ANIMAL_BODY_PART (1 found)\n",
            "\n",
            "- claw\n",
            "\n",
            "### MEASUREMENT\n",
            "\n",
            "No entities of this type found.\n",
            "\n",
            "### LENGTH\n",
            "\n",
            "No entities of this type found.\n",
            "\n",
            "### HABITAT\n",
            "\n",
            "No entities of this type found.\n",
            "\n",
            "\n",
            "## Performance Summary\n",
            "\n",
            "Total processing time: 3.83 seconds\n",
            "- Text extraction: 1.65 seconds\n",
            "- SpaCy Small extraction: 1.24 seconds\n",
            "\n",
            "## Sample of Extracted Text\n",
            "\n",
            "```\n",
            "Frogs: Princely models for science\n",
            "Frogs have no small claim to fame in biology. They’ve provided us with the first reliable\n",
            "pregnancy tests, donned rubber pants in the name of research, and appeared in many\n",
            "high school biology dissections. Frogs\n",
            "have helped us learn about the con-\n",
            "nection between nerves and muscles,\n",
            "taught us how to do skin grafts, and\n",
            "have contributed to our understand-\n",
            "ing of fundamental biological process-\n",
            "es such as fertilization, cell division,\n",
            "and development.\n",
            "What’s so s...\n",
            "```\n",
            "\n",
            "Comparison chart saved to: ./entity_extraction_comparison.png\n",
            "Results saved to: ./entity_extraction_results.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import spacy\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import logging\n",
        "import time\n",
        "import os\n",
        "import tempfile\n",
        "from io import BytesIO\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline, AutoModel\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from scipy.sparse import csr_matrix\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import warnings\n",
        "from IPython.display import display, HTML, Image\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.StreamHandler()]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Dictionary to store loaded models\n",
        "nlp_models = {}\n",
        "transformer_models = {}\n",
        "attention_models = {}\n",
        "\n",
        "# Available NLP models\n",
        "AVAILABLE_MODELS = {\n",
        "    \"SpaCy Small\": \"en_core_web_sm\",\n",
        "    \"SpaCy Large\": \"en_core_web_lg\",\n",
        "    \"BioBERT\": \"dmis-lab/biobert-base-cased-v1.1-named-entity-recognition\",\n",
        "    \"BERT-base\": \"bert-base-uncased\"  # Added for attention-based relationship extraction\n",
        "}\n",
        "\n",
        "# Function to load NLP model\n",
        "def load_nlp_model(model_name):\n",
        "    if model_name in nlp_models:\n",
        "        return nlp_models[model_name]\n",
        "\n",
        "    model_path = AVAILABLE_MODELS.get(model_name)\n",
        "    if not model_path:\n",
        "        logger.error(f\"Unknown model: {model_name}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # BioBERT uses HuggingFace Transformers\n",
        "        if model_name == \"BioBERT\":\n",
        "            logger.info(f\"Loading BioBERT model: {model_path}\")\n",
        "            try:\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "                model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "\n",
        "                # Create NER pipeline\n",
        "                ner_model = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)\n",
        "                transformer_models[model_name] = ner_model\n",
        "                nlp_models[model_name] = \"transformer\"\n",
        "                logger.info(f\"BioBERT model loaded successfully\")\n",
        "                return \"transformer\"\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading BioBERT model: {str(e)}\")\n",
        "                logger.warning(\"Please install transformers: pip install transformers torch\")\n",
        "                return None\n",
        "        elif model_name == \"BERT-base\":\n",
        "            logger.info(f\"Loading BERT-base model for attention: {model_path}\")\n",
        "            try:\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "                model = AutoModel.from_pretrained(model_path, output_attentions=True)\n",
        "\n",
        "                attention_models[model_name] = {\n",
        "                    \"tokenizer\": tokenizer,\n",
        "                    \"model\": model\n",
        "                }\n",
        "                nlp_models[model_name] = \"attention\"\n",
        "                logger.info(f\"BERT-base model loaded successfully for attention analysis\")\n",
        "                return \"attention\"\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading BERT-base model: {str(e)}\")\n",
        "                logger.warning(\"Please install transformers: pip install transformers torch\")\n",
        "                return None\n",
        "        else:\n",
        "            # SpaCy models\n",
        "            logger.info(f\"Loading model: {model_name} ({model_path})\")\n",
        "            nlp = spacy.load(model_path)\n",
        "\n",
        "            # Add animal entity patterns\n",
        "            if \"entity_ruler\" not in nlp.pipe_names:\n",
        "                ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "                patterns = [\n",
        "                    # Improved SPECIES pattern for scientific names (Genus species)\n",
        "                    {\"label\": \"SPECIES\", \"pattern\": [\n",
        "                        {\"TEXT\": {\"REGEX\": \"^[A-Z][a-z]+$\"}},  # Genus - capitalized word\n",
        "                        {\"TEXT\": {\"REGEX\": \"^[a-z]+$\"}}        # species - lowercase word\n",
        "                    ]},\n",
        "                    # Common animal names pattern\n",
        "                    {\"label\": \"SPECIES\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"dog\", \"cat\", \"horse\", \"cow\", \"pig\", \"sheep\", \"goat\", \"chicken\",\n",
        "                                        \"fish\", \"salmon\", \"trout\", \"bass\", \"butterfly\", \"bee\", \"ant\", \"spider\",\n",
        "                                        \"lion\", \"tiger\", \"bear\", \"elephant\", \"giraffe\", \"rhinoceros\", \"zebra\",\n",
        "                                        \"cheetah\", \"leopard\", \"wolf\", \"fox\", \"deer\", \"moose\", \"eagle\", \"hawk\",\n",
        "                                        \"owl\", \"parrot\", \"dolphin\", \"whale\", \"shark\", \"turtle\", \"snake\",\n",
        "                                        \"crocodile\", \"alligator\", \"frog\", \"toad\"]}}\n",
        "                    ]},\n",
        "                    # Animal groups with adjectives\n",
        "                    {\"label\": \"SPECIES\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"african\", \"asian\", \"american\", \"european\", \"australian\",\n",
        "                                         \"western\", \"eastern\", \"northern\", \"southern\"]}},\n",
        "                        {\"LOWER\": {\"IN\": [\"lion\", \"tiger\", \"bear\", \"elephant\", \"giraffe\", \"rhinoceros\",\n",
        "                                         \"zebra\", \"cheetah\", \"leopard\", \"wolf\", \"fox\", \"deer\", \"moose\",\n",
        "                                         \"eagle\", \"hawk\", \"owl\", \"parrot\", \"dolphin\", \"whale\", \"shark\",\n",
        "                                         \"turtle\", \"snake\", \"crocodile\", \"alligator\"]}}\n",
        "                    ]},\n",
        "                    # Animal groups\n",
        "                    {\"label\": \"ANIMAL_GROUP\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"herd\", \"flock\", \"pack\", \"pod\", \"school\", \"colony\", \"pride\",\n",
        "                                         \"swarm\", \"murder\", \"gaggle\", \"hive\", \"brood\", \"drift\"]}}\n",
        "                    ]},\n",
        "                    # Animal body parts\n",
        "                    {\"label\": \"ANIMAL_BODY_PART\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"paw\", \"claw\", \"hoof\", \"beak\", \"wing\", \"fin\", \"tail\", \"horn\",\n",
        "                                         \"tusk\", \"scale\", \"feather\", \"fur\", \"mane\", \"whisker\", \"snout\",\n",
        "                                         \"bill\", \"gills\", \"tentacle\", \"antenna\"]}}\n",
        "                    ]},\n",
        "                    # Measurements\n",
        "                    {\"label\": \"MEASUREMENT\", \"pattern\": [\n",
        "                        {\"SHAPE\": {\"IN\": [\"d+\", \"d+.d+\"]}},\n",
        "                        {\"LOWER\": {\"IN\": [\"mm\", \"cm\", \"m\", \"kg\", \"g\", \"ml\", \"l\"]}}\n",
        "                    ]},\n",
        "                    # Length measurements\n",
        "                    {\"label\": \"LENGTH\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"fork\", \"total\", \"standard\", \"body\", \"tail\"]}},\n",
        "                        {\"LOWER\": \"length\"}\n",
        "                    ]},\n",
        "                    # Habitats\n",
        "                    {\"label\": \"HABITAT\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"forest\", \"jungle\", \"savanna\", \"ocean\", \"lake\", \"river\", \"mountain\",\n",
        "                                        \"desert\", \"tundra\", \"reef\", \"farm\", \"pasture\", \"pen\", \"enclosure\",\n",
        "                                        \"cage\", \"burrow\", \"nest\", \"den\", \"wetland\", \"bog\", \"marsh\"]}}\n",
        "                    ]},\n",
        "                ]\n",
        "                ruler.add_patterns(patterns)\n",
        "\n",
        "            nlp_models[model_name] = nlp\n",
        "            logger.info(f\"SpaCy model {model_name} loaded successfully\")\n",
        "            return nlp\n",
        "    except OSError as e:\n",
        "        logger.warning(f\"Error loading model {model_path}: {str(e)}\")\n",
        "        if model_name.startswith(\"SpaCy\"):\n",
        "            logger.warning(f\"Please download it using: python -m spacy download {model_path}\")\n",
        "        elif model_name in [\"BioBERT\", \"BERT-base\"]:\n",
        "            logger.warning(\"Please install transformers: pip install transformers torch\")\n",
        "\n",
        "        # Create a blank model as fallback if it's a SpaCy model\n",
        "        if model_name.startswith(\"SpaCy\"):\n",
        "            nlp = spacy.blank(\"en\")\n",
        "            nlp_models[model_name] = nlp\n",
        "            return nlp\n",
        "        return None\n",
        "\n",
        "# Check which models are installed\n",
        "def get_available_models():\n",
        "    installed_models = []\n",
        "\n",
        "    # Check SpaCy models\n",
        "    for name, path in AVAILABLE_MODELS.items():\n",
        "        if name.startswith(\"SpaCy\"):\n",
        "            try:\n",
        "                spacy.load(path)\n",
        "                installed_models.append(name)\n",
        "            except OSError:\n",
        "                pass\n",
        "        elif name in [\"BioBERT\", \"BERT-base\"]:\n",
        "            try:\n",
        "                # Check if transformers is installed\n",
        "                import transformers\n",
        "\n",
        "                # Add transformer models to available models regardless of whether they're downloaded\n",
        "                # We'll handle the download when it's first used\n",
        "                installed_models.append(name)\n",
        "\n",
        "                # Just log that we'll download when needed\n",
        "                logger.info(f\"{name} model will be downloaded automatically when first used.\")\n",
        "            except ImportError:\n",
        "                logger.warning(\"Transformers package not installed. Transformer models will not be available.\")\n",
        "\n",
        "    # Ensure at least one model is available\n",
        "    if not installed_models:\n",
        "        try:\n",
        "            spacy.load(\"en_core_web_sm\")\n",
        "            installed_models.append(\"SpaCy Small\")\n",
        "        except OSError:\n",
        "            installed_models.append(\"Blank Model\")\n",
        "            logger.warning(\"No SpaCy models installed. Please run: python -m spacy download en_core_web_sm\")\n",
        "            logger.warning(\"For transformer models: pip install transformers torch\")\n",
        "\n",
        "    return installed_models\n",
        "\n",
        "# Function to extract text from PDF with better error handling\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    if not pdf_file or not os.path.exists(pdf_file):\n",
        "        return \"No file provided or file does not exist.\"\n",
        "\n",
        "    try:\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(pdf_file) as pdf:\n",
        "            total_pages = len(pdf.pages)\n",
        "            logger.info(f\"Processing PDF with {total_pages} pages\")\n",
        "\n",
        "            for i, page in enumerate(pdf.pages):\n",
        "                logger.info(f\"Extracting text from page {i+1}/{total_pages}\")\n",
        "                extracted_text = page.extract_text()\n",
        "                if extracted_text:\n",
        "                    text += extracted_text + \"\\n\"\n",
        "\n",
        "        if not text.strip():\n",
        "            return \"No text could be extracted from the PDF.\"\n",
        "\n",
        "        logger.info(f\"Successfully extracted {len(text)} characters of text\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing PDF: {str(e)}\", exc_info=True)\n",
        "        return f\"Error processing PDF: {str(e)}\"\n",
        "\n",
        "# Extract entities using BioBERT\n",
        "def extract_entities_biobert(text, entity_types):\n",
        "    try:\n",
        "        if \"BioBERT\" not in transformer_models:\n",
        "            model_path = AVAILABLE_MODELS.get(\"BioBERT\")\n",
        "            try:\n",
        "                logger.info(f\"BioBERT model not loaded yet. Downloading and initializing...\")\n",
        "\n",
        "                # Set download settings\n",
        "                logger.info(\"Setting up to download BioBERT model...\")\n",
        "\n",
        "                # Print info about what's happening\n",
        "                print(\"\\nDownloading BioBERT model. This may take a few minutes on first run...\")\n",
        "                print(\"Model: dmis-lab/biobert-base-cased-v1.1-named-entity-recognition\")\n",
        "\n",
        "                # Load tokenizer with progress reporting\n",
        "                print(\"Loading tokenizer...\")\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "                # Load model with progress reporting\n",
        "                print(\"Loading model weights (this might take a while)...\")\n",
        "                model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "\n",
        "                print(\"Creating NER pipeline...\")\n",
        "                ner_model = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)\n",
        "\n",
        "                transformer_models[\"BioBERT\"] = ner_model\n",
        "                print(\"BioBERT model loaded successfully!\\n\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading BioBERT model: {str(e)}\")\n",
        "                print(f\"\\nError: Could not load BioBERT model. Details: {str(e)}\")\n",
        "                print(\"Common solutions:\")\n",
        "                print(\"1. Check internet connection\")\n",
        "                print(\"2. Ensure 'transformers' and 'torch' are installed: pip install transformers torch\")\n",
        "                print(\"3. Try again or use a different model\\n\")\n",
        "                return f\"Error: Could not load BioBERT model: {str(e)}\"\n",
        "\n",
        "        ner_model = transformer_models[\"BioBERT\"]\n",
        "\n",
        "        # Process text in chunks to avoid CUDA out of memory errors\n",
        "        max_length = 512  # Maximum length for BERT models\n",
        "        chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
        "\n",
        "        logger.info(f\"Processing {len(chunks)} chunks with BioBERT\")\n",
        "\n",
        "        all_entities = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            if i % 10 == 0:\n",
        "                logger.info(f\"Processing chunk {i+1}/{len(chunks)}\")\n",
        "\n",
        "            if not chunk.strip():\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                chunk_entities = ner_model(chunk)\n",
        "                all_entities.extend(chunk_entities)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error processing chunk {i}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Map BioBERT entity types to our types and extract entities\n",
        "        extracted = {entity_type: [] for entity_type in entity_types}\n",
        "\n",
        "        # BioBERT's default entity types: 'B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'\n",
        "        # Map BioBERT entity types to our custom types\n",
        "        entity_mapping = {\n",
        "            'B-MISC': ['ANIMAL_BODY_PART', 'MEASUREMENT', 'LENGTH'],\n",
        "            'I-MISC': ['ANIMAL_BODY_PART', 'MEASUREMENT', 'LENGTH'],\n",
        "            'B-PER': ['SPECIES'],  # Sometimes species names are recognized as persons\n",
        "            'I-PER': ['SPECIES'],\n",
        "            'B-ORG': ['ANIMAL_GROUP'],\n",
        "            'I-ORG': ['ANIMAL_GROUP'],\n",
        "            'B-LOC': ['HABITAT'],\n",
        "            'I-LOC': ['HABITAT'],\n",
        "        }\n",
        "\n",
        "        # Process entities\n",
        "        for entity in all_entities:\n",
        "            entity_text = entity['word']\n",
        "            entity_type = entity['entity']\n",
        "            score = entity['score']\n",
        "\n",
        "            # Only consider entities with confidence above threshold\n",
        "            if score < 0.75:\n",
        "                continue\n",
        "\n",
        "            # Map BioBERT entity types to our types\n",
        "            mapped_types = entity_mapping.get(entity_type, [])\n",
        "\n",
        "            # Add entity to all compatible types\n",
        "            for mapped_type in mapped_types:\n",
        "                if mapped_type in entity_types:\n",
        "                    extracted[mapped_type].append(entity_text)\n",
        "\n",
        "        # Additional regex-based extraction for specific entity types\n",
        "        # Scientific species names (Genus species)\n",
        "        if \"SPECIES\" in entity_types:\n",
        "            species_pattern = r'\\b[A-Z][a-z]+\\s+[a-z]+\\b'\n",
        "            scientific_names = re.findall(species_pattern, text)\n",
        "            extracted[\"SPECIES\"].extend(scientific_names)\n",
        "\n",
        "            # Common animal names\n",
        "            common_animals = re.findall(r'\\b(?:African|Asian|American|European|Australian|Western|Eastern|Northern|Southern)?\\s*(?:Lion|Tiger|Bear|Elephant|Giraffe|Rhinoceros|Zebra|Cheetah|Leopard|Wolf|Fox|Deer|Moose|Eagle|Hawk|Owl|Parrot|Dolphin|Whale|Shark|Turtle|Snake|Crocodile|Alligator|Frog|Toad|Fish|Salmon|Trout|Bass|Butterfly|Bee|Ant|Spider)\\b', text)\n",
        "            extracted[\"SPECIES\"].extend(common_animals)\n",
        "\n",
        "        # Animal groups\n",
        "        if \"ANIMAL_GROUP\" in entity_types:\n",
        "            groups = re.findall(r'\\b(?:herd|flock|pack|pod|school|colony|pride|swarm|murder|gaggle|hive|brood|drift)\\s+of\\s+\\w+\\b', text, re.IGNORECASE)\n",
        "            extracted[\"ANIMAL_GROUP\"].extend(groups)\n",
        "\n",
        "        # Body parts\n",
        "        if \"ANIMAL_BODY_PART\" in entity_types:\n",
        "            body_parts = re.findall(r'\\b(?:paw|claw|hoof|beak|wing|fin|tail|horn|tusk|scale|feather|fur|mane|whisker|snout|bill|gills|tentacle|antenna)s?\\b', text, re.IGNORECASE)\n",
        "            extracted[\"ANIMAL_BODY_PART\"].extend(body_parts)\n",
        "\n",
        "        # Measurements\n",
        "        if \"MEASUREMENT\" in entity_types:\n",
        "            measurements = re.findall(r'\\b\\d+\\.?\\d*\\s*(?:mm|cm|m|kg|g|ml|l)\\b', text)\n",
        "            extracted[\"MEASUREMENT\"].extend(measurements)\n",
        "\n",
        "        # Length measurements\n",
        "        if \"LENGTH\" in entity_types:\n",
        "            length_patterns = re.findall(r'(?:fork|total|standard|body|tail)?\\s*length\\s*(?:of)?\\s*\\d+\\.?\\d*\\s*(?:mm|cm|m)', text, re.IGNORECASE)\n",
        "            extracted[\"LENGTH\"].extend(length_patterns)\n",
        "\n",
        "        # Habitats\n",
        "        if \"HABITAT\" in entity_types:\n",
        "            habitats = re.findall(r'\\b(?:forest|jungle|savanna|ocean|lake|river|mountain|desert|tundra|reef|farm|pasture|pen|enclosure|cage|burrow|nest|den|wetland|bog|marsh)\\b', text, re.IGNORECASE)\n",
        "            extracted[\"HABITAT\"].extend(habitats)\n",
        "\n",
        "        # Remove duplicates and sort\n",
        "        for entity_type in entity_types:\n",
        "            extracted[entity_type] = sorted(list(set([e.strip() for e in extracted[entity_type] if e.strip()])))\n",
        "            logger.info(f\"Found {len(extracted[entity_type])} unique {entity_type} entities with BioBERT\")\n",
        "\n",
        "        return extracted\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting entities with BioBERT: {str(e)}\", exc_info=True)\n",
        "        return f\"Error extracting entities with BioBERT: {str(e)}\"\n",
        "\n",
        "# Function to extract entities with better error handling and performance\n",
        "def extract_entities(text, entity_types, model_name):\n",
        "    if isinstance(text, str) and not text.startswith(\"Error\"):\n",
        "        try:\n",
        "            # Special handling for BioBERT model\n",
        "            if model_name == \"BioBERT\":\n",
        "                return extract_entities_biobert(text, entity_types)\n",
        "\n",
        "            # Get the specified SpaCy model\n",
        "            nlp = load_nlp_model(model_name)\n",
        "            if nlp is None:\n",
        "                return f\"Could not load NLP model: {model_name}\"\n",
        "            if nlp == \"transformer\":\n",
        "                return extract_entities_biobert(text, entity_types)\n",
        "            if nlp == \"attention\":\n",
        "                # Attention models are not used for entity extraction directly\n",
        "                # but instead for relationship extraction\n",
        "                logger.warning(f\"{model_name} is an attention model, not used for entity extraction\")\n",
        "                return {entity_type: [] for entity_type in entity_types}\n",
        "\n",
        "            # Limit text size for SpaCy models to avoid memory issues\n",
        "            if len(text) > 100000:\n",
        "                logger.warning(f\"Text is very large ({len(text)} chars). Limiting to first 100k characters.\")\n",
        "                text = text[:100000]  # Limit to first 100k characters\n",
        "\n",
        "            logger.info(f\"Processing text with {model_name} ({len(text)} chars)\")\n",
        "            doc = nlp(text)\n",
        "\n",
        "            # Create a dictionary to store extracted entities\n",
        "            extracted = {entity_type: [] for entity_type in entity_types}\n",
        "\n",
        "            # Extract required entity types\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_ in entity_types:\n",
        "                    extracted[ent.label_].append(ent.text)\n",
        "\n",
        "            # Special case for animal scientific species names\n",
        "            if \"SPECIES\" in entity_types:\n",
        "                logger.info(\"Extracting scientific species names with regex\")\n",
        "\n",
        "                # Scientific names pattern (Genus species)\n",
        "                scientific_names = re.findall(r'\\b[A-Z][a-z]+\\s+[a-z]+\\b', text)\n",
        "                extracted[\"SPECIES\"].extend(scientific_names)\n",
        "\n",
        "                # Additional check for common animal names (with capitalized nouns)\n",
        "                common_animals = re.findall(r'\\b(?:African|Asian|American|European|Australian|Western|Eastern|Northern|Southern)?\\s*(?:Lion|Tiger|Bear|Elephant|Giraffe|Rhinoceros|Zebra|Cheetah|Leopard|Wolf|Fox|Deer|Moose|Eagle|Hawk|Owl|Parrot|Dolphin|Whale|Shark|Turtle|Snake|Crocodile|Alligator|Frog|Toad|Fish|Salmon|Trout|Bass|Butterfly|Bee|Ant|Spider)\\b', text)\n",
        "                extracted[\"SPECIES\"].extend(common_animals)\n",
        "\n",
        "            # Special case for animal groups\n",
        "            if \"ANIMAL_GROUP\" in entity_types:\n",
        "                logger.info(\"Extracting animal group terms with regex\")\n",
        "                groups = re.findall(r'\\b(?:herd|flock|pack|pod|school|colony|pride|swarm|murder|gaggle|hive|brood|drift)\\s+of\\s+\\w+\\b', text, re.IGNORECASE)\n",
        "                extracted[\"ANIMAL_GROUP\"].extend(groups)\n",
        "\n",
        "            # Special case for body parts\n",
        "            if \"ANIMAL_BODY_PART\" in entity_types:\n",
        "                logger.info(\"Extracting animal body parts with regex\")\n",
        "                body_parts = re.findall(r'\\b(?:paw|claw|hoof|beak|wing|fin|tail|horn|tusk|scale|feather|fur|mane|whisker|snout|bill|gills|tentacle|antenna)s?\\b', text, re.IGNORECASE)\n",
        "                extracted[\"ANIMAL_BODY_PART\"].extend(body_parts)\n",
        "\n",
        "            # Special case for measurements\n",
        "            if \"MEASUREMENT\" in entity_types:\n",
        "                logger.info(\"Extracting measurements with regex\")\n",
        "                measurements = re.findall(r'\\b\\d+\\.?\\d*\\s*(?:mm|cm|m|kg|g|ml|l)\\b', text)\n",
        "                extracted[\"MEASUREMENT\"].extend(measurements)\n",
        "\n",
        "            # Special case for length measurements\n",
        "            if \"LENGTH\" in entity_types:\n",
        "                logger.info(\"Extracting length measurements with regex\")\n",
        "                length_patterns = re.findall(r'(?:fork|total|standard|body|tail)?\\s*length\\s*(?:of)?\\s*\\d+\\.?\\d*\\s*(?:mm|cm|m)', text, re.IGNORECASE)\n",
        "                extracted[\"LENGTH\"].extend(length_patterns)\n",
        "\n",
        "            # Special case for habitats\n",
        "            if \"HABITAT\" in entity_types:\n",
        "                logger.info(\"Extracting habitat terms with regex\")\n",
        "                habitats = re.findall(r'\\b(?:forest|jungle|savanna|ocean|lake|river|mountain|desert|tundra|reef|farm|pasture|pen|enclosure|cage|burrow|nest|den|wetland|bog|marsh)\\b', text, re.IGNORECASE)\n",
        "                extracted[\"HABITAT\"].extend(habitats)\n",
        "\n",
        "            # Remove duplicates and sort\n",
        "            for entity_type in entity_types:\n",
        "                extracted[entity_type] = sorted(list(set([e.strip() for e in extracted[entity_type] if e.strip()])))\n",
        "                logger.info(f\"Found {len(extracted[entity_type])} unique {entity_type} entities with {model_name}\")\n",
        "\n",
        "            return extracted\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting entities with {model_name}: {str(e)}\", exc_info=True)\n",
        "            return f\"Error extracting entities with {model_name}: {str(e)}\"\n",
        "    else:\n",
        "        return f\"Cannot extract entities: {text}\"\n",
        "\n",
        "# Function to generate comparison visualization\n",
        "def generate_comparison_chart(extracted_by_model, entity_types, models, output_path=None):\n",
        "    try:\n",
        "        if output_path is None:\n",
        "            temp_dir = tempfile.mkdtemp()\n",
        "            chart_file = os.path.join(temp_dir, \"animal_comparison_chart.png\")\n",
        "        else:\n",
        "            chart_file = output_path\n",
        "\n",
        "        # Set up the plot\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "        # Prepare data\n",
        "        x = np.arange(len(entity_types))\n",
        "        width = 0.8 / len(models)\n",
        "\n",
        "        # Define a color palette\n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
        "\n",
        "        # Plot bars for each model\n",
        "        for i, model in enumerate(models):\n",
        "            if model in extracted_by_model:\n",
        "                counts = [len(extracted_by_model[model].get(entity_type, [])) for entity_type in entity_types]\n",
        "                offset = width * i - width * (len(models) - 1) / 2\n",
        "                ax.bar(x + offset, counts, width, label=model, color=colors[i % len(colors)])\n",
        "\n",
        "        # Add labels and title\n",
        "        ax.set_ylabel('Number of Entities', fontsize=12)\n",
        "        ax.set_title('Comparison of Entity Extraction Between Models', fontsize=14, fontweight='bold')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(entity_types, rotation=45, ha='right', fontsize=10)\n",
        "        ax.legend(loc='best')\n",
        "\n",
        "        # Add value labels on top of each bar\n",
        "        for i, model in enumerate(models):\n",
        "            if model in extracted_by_model:\n",
        "                counts = [len(extracted_by_model[model].get(entity_type, [])) for entity_type in entity_types]\n",
        "                offset = width * i - width * (len(models) - 1) / 2\n",
        "                for j, count in enumerate(counts):\n",
        "                    ax.text(x[j] + offset, count + 0.5, str(count), ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "        # Add a grid for better readability\n",
        "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Improve layout\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(chart_file, dpi=300, bbox_inches='tight')\n",
        "\n",
        "        # Display in notebook if in notebook environment\n",
        "        try:\n",
        "            from IPython import get_ipython\n",
        "            if get_ipython() is not None:\n",
        "                plt.show()\n",
        "        except (ImportError, NameError):\n",
        "            pass\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "        return chart_file\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating comparison chart: {str(e)}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "# NEW FUNCTION: Extract entity relationships using pattern-based approach\n",
        "def extract_entity_relationships(text, extracted_entities, nlp_model):\n",
        "    \"\"\"\n",
        "    Extract relationships between different entities using dependency parsing and co-occurrence.\n",
        "\n",
        "    Args:\n",
        "        text: The text content from the PDF\n",
        "        extracted_entities: Dictionary of extracted entities by type\n",
        "        nlp_model: The loaded SpaCy NLP model (must be SpaCy, not BioBERT)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of relationship types and their instances\n",
        "    \"\"\"\n",
        "    if isinstance(nlp_model, str):\n",
        "        return {\"error\": [\"Relationship extraction requires a SpaCy model\"]}\n",
        "\n",
        "    relationships = {\n",
        "        \"species_habitat\": [],       # Which species live in which habitats\n",
        "        \"species_body_part\": [],     # Which body parts belong to which species\n",
        "        \"species_measurement\": [],   # Measurements associated with species\n",
        "        \"species_group\": []          # Species associated with animal groups\n",
        "    }\n",
        "\n",
        "    # Flatten entity lists into entity-type mappings for fast lookup\n",
        "    entity_map = {}\n",
        "    for entity_type, entities in extracted_entities.items():\n",
        "        for entity in entities:\n",
        "            entity_map[entity.lower()] = entity_type\n",
        "\n",
        "    # Process text in manageable chunks to avoid memory issues\n",
        "    max_chunk_size = 5000\n",
        "    chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
        "\n",
        "    logger.info(f\"Processing {len(chunks)} chunks for entity relationships\")\n",
        "\n",
        "    # Process each chunk\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        if i % 10 == 0:\n",
        "            logger.info(f\"Processing relationship chunk {i+1}/{len(chunks)}\")\n",
        "\n",
        "        if not chunk.strip():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Process with spaCy\n",
        "            doc = nlp_model(chunk)\n",
        "\n",
        "            # Extract sentences\n",
        "            sentences = list(doc.sents)\n",
        "\n",
        "            # Analyze each sentence for entity relationships\n",
        "            for sentence in sentences:\n",
        "                sentence_text = sentence.text.lower()\n",
        "\n",
        "                # Find entities in this sentence\n",
        "                sentence_entities = {}\n",
        "                for entity, entity_type in entity_map.items():\n",
        "                    if entity.lower() in sentence_text:\n",
        "                        start = sentence_text.find(entity.lower())\n",
        "                        if start >= 0:\n",
        "                            sentence_entities[(start, start + len(entity))] = (entity, entity_type)\n",
        "\n",
        "                # If we have multiple entity types in the sentence, check for relationships\n",
        "                entity_types_in_sent = set(et for _, et in sentence_entities.values())\n",
        "\n",
        "                # Species and habitat relationship\n",
        "                if \"SPECIES\" in entity_types_in_sent and \"HABITAT\" in entity_types_in_sent:\n",
        "                    # Look for habitat relationship patterns\n",
        "                    habitat_patterns = [\n",
        "                        \"found in\", \"lives in\", \"inhabits\", \"native to\", \"occurs in\",\n",
        "                        \"present in\", \"located in\", \"resides in\", \"dwells in\", \"occupies\"\n",
        "                    ]\n",
        "\n",
        "                    for pattern in habitat_patterns:\n",
        "                        if pattern in sentence_text:\n",
        "                            # Find closest species and habitat entities\n",
        "                            pattern_pos = sentence_text.find(pattern)\n",
        "                            species_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                               in sentence_entities.items()\n",
        "                                               if entity_type == \"SPECIES\"]\n",
        "                            habitat_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                              in sentence_entities.items()\n",
        "                                              if entity_type == \"HABITAT\"]\n",
        "\n",
        "                            if species_entities and habitat_entities:\n",
        "                                # Find closest species before the pattern\n",
        "                                species_before = [(abs(pattern_pos - (pos + len(entity[0]))), entity[0])\n",
        "                                               for pos, entity in species_entities\n",
        "                                               if pos < pattern_pos]\n",
        "\n",
        "                                # Find closest habitat after the pattern\n",
        "                                habitat_after = [(abs(pos - (pattern_pos + len(pattern))), entity[0])\n",
        "                                              for pos, entity in habitat_entities\n",
        "                                              if pos > pattern_pos]\n",
        "\n",
        "                                if species_before and habitat_after:\n",
        "                                    species = min(species_before, key=lambda x: x[0])[1]\n",
        "                                    habitat = min(habitat_after, key=lambda x: x[0])[1]\n",
        "                                    relationship = (species, habitat, sentence.text)\n",
        "                                    if relationship not in relationships[\"species_habitat\"]:\n",
        "                                        relationships[\"species_habitat\"].append(relationship)\n",
        "\n",
        "                # Species and body part relationship\n",
        "                if \"SPECIES\" in entity_types_in_sent and \"ANIMAL_BODY_PART\" in entity_types_in_sent:\n",
        "                    # Find possession patterns\n",
        "                    possession_patterns = [\n",
        "                        \"has\", \"with\", \"possesses\", \"featuring\", \"characterized by\",\n",
        "                        \"'s\", \"of the\", \"of its\", \"their\", \"its\"\n",
        "                    ]\n",
        "\n",
        "                    for pattern in possession_patterns:\n",
        "                        if pattern in sentence_text:\n",
        "                            pattern_pos = sentence_text.find(pattern)\n",
        "                            species_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                               in sentence_entities.items()\n",
        "                                               if entity_type == \"SPECIES\"]\n",
        "                            body_part_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                                in sentence_entities.items()\n",
        "                                                if entity_type == \"ANIMAL_BODY_PART\"]\n",
        "\n",
        "                            if species_entities and body_part_entities:\n",
        "                                species_before = [(abs(pattern_pos - (pos + len(entity[0]))), entity[0])\n",
        "                                               for pos, entity in species_entities\n",
        "                                               if pos < pattern_pos]\n",
        "\n",
        "                                body_part_after = [(abs(pos - (pattern_pos + len(pattern))), entity[0])\n",
        "                                              for pos, entity in body_part_entities\n",
        "                                              if pos > pattern_pos]\n",
        "\n",
        "                                if species_before and body_part_after:\n",
        "                                    species = min(species_before, key=lambda x: x[0])[1]\n",
        "                                    body_part = min(body_part_after, key=lambda x: x[0])[1]\n",
        "                                    relationship = (species, body_part, sentence.text)\n",
        "                                    if relationship not in relationships[\"species_body_part\"]:\n",
        "                                        relationships[\"species_body_part\"].append(relationship)\n",
        "\n",
        "                # Species and measurements\n",
        "                if \"SPECIES\" in entity_types_in_sent and (\"MEASUREMENT\" in entity_types_in_sent or \"LENGTH\" in entity_types_in_sent):\n",
        "                    # Look for measurement patterns\n",
        "                    measurement_patterns = [\n",
        "                        \"measures\", \"weighs\", \"length of\", \"size of\", \"weight of\",\n",
        "                        \"is about\", \"approximately\", \"typically\", \"averages\", \"ranging from\"\n",
        "                    ]\n",
        "\n",
        "                    measurement_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                           in sentence_entities.items()\n",
        "                                           if entity_type in [\"MEASUREMENT\", \"LENGTH\"]]\n",
        "\n",
        "                    species_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                       in sentence_entities.items()\n",
        "                                       if entity_type == \"SPECIES\"]\n",
        "\n",
        "                    if species_entities and measurement_entities:\n",
        "                        for species_pos, species_entity in species_entities:\n",
        "                            for measurement_pos, measurement_entity in measurement_entities:\n",
        "                                # Check if they're close to each other (within 50 characters)\n",
        "                                if abs(species_pos - measurement_pos) < 50:\n",
        "                                    relationship = (species_entity[0], measurement_entity[0], sentence.text)\n",
        "                                    if relationship not in relationships[\"species_measurement\"]:\n",
        "                                        relationships[\"species_measurement\"].append(relationship)\n",
        "\n",
        "                # Species and group relationships\n",
        "                if \"SPECIES\" in entity_types_in_sent and \"ANIMAL_GROUP\" in entity_types_in_sent:\n",
        "                    group_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                     in sentence_entities.items()\n",
        "                                     if entity_type == \"ANIMAL_GROUP\"]\n",
        "\n",
        "                    species_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                       in sentence_entities.items()\n",
        "                                       if entity_type == \"SPECIES\"]\n",
        "\n",
        "                    if species_entities and group_entities:\n",
        "                        for group_pos, group_entity in group_entities:\n",
        "                            # Group terms usually have \"of X\" structure, check for the species after \"of\"\n",
        "                            if \"of \" in group_entity[0]:\n",
        "                                group_text = group_entity[0]\n",
        "                                after_of = group_text.split(\"of \")[1].strip().lower()\n",
        "\n",
        "                                # Check if any species appears in or matches the group description\n",
        "                                for _, species_entity in species_entities:\n",
        "                                    species_text = species_entity[0].lower()\n",
        "                                    if species_text in after_of or after_of in species_text:\n",
        "                                        relationship = (species_entity[0], group_entity[0], sentence.text)\n",
        "                                        if relationship not in relationships[\"species_group\"]:\n",
        "                                            relationships[\"species_group\"].append(relationship)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error processing chunk {i} for relationships: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Log results\n",
        "    for relation_type, relations in relationships.items():\n",
        "        logger.info(f\"Found {len(relations)} {relation_type} relationships\")\n",
        "\n",
        "    return relationships\n",
        "\n",
        "# NEW FUNCTION: Extract entity relationships with transformer attention\n",
        "def extract_entity_relationships_with_attention(text, extracted_entities, model_name=\"BERT-base\"):\n",
        "    \"\"\"\n",
        "    Extract relationships between entities using transformer attention mechanisms.\n",
        "\n",
        "    Args:\n",
        "        text: The text content from the PDF\n",
        "        extracted_entities: Dictionary of extracted entities by type\n",
        "        model_name: Pretrained transformer model to use\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of relationship types and their instances with attention scores\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if model_name not in attention_models:\n",
        "            # Load the model if not already loaded\n",
        "            load_nlp_model(model_name)\n",
        "\n",
        "            if model_name not in attention_models:\n",
        "                return {\"error\": [f\"Could not load attention model: {model_name}\"]}\n",
        "\n",
        "        tokenizer = attention_models[model_name][\"tokenizer\"]\n",
        "        model = attention_models[model_name][\"model\"]\n",
        "\n",
        "        # Prepare relationships dictionary\n",
        "        relationships = {\n",
        "            \"species_habitat\": [],\n",
        "            \"species_body_part\": [],\n",
        "            \"species_measurement\": [],\n",
        "            \"species_group\": []\n",
        "        }\n",
        "\n",
        "        # Prepare entity mapping for lookup\n",
        "        entity_spans = []\n",
        "        entity_types = []\n",
        "\n",
        "        for entity_type, entities in extracted_entities.items():\n",
        "            for entity in entities:\n",
        "                entity_spans.append((entity, entity_type))\n",
        "\n",
        "        # Process text in chunks that fit within model's max context\n",
        "        max_length = tokenizer.model_max_length\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "        logger.info(f\"Processing {len(sentences)} sentences with attention model\")\n",
        "\n",
        "        # Process sentences to find relationships\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            if i % 100 == 0:\n",
        "                logger.info(f\"Processing sentence {i+1}/{len(sentences)} with attention\")\n",
        "\n",
        "            if not sentence.strip() or len(sentence) > max_length:\n",
        "                continue\n",
        "\n",
        "            # Find entities in this sentence\n",
        "            entities_in_sentence = []\n",
        "            for entity, entity_type in entity_spans:\n",
        "                if entity.lower() in sentence.lower():\n",
        "                    entities_in_sentence.append((entity, entity_type))\n",
        "\n",
        "            # Skip if less than 2 entities or no species in the sentence\n",
        "            if len(entities_in_sentence) < 2:\n",
        "                continue\n",
        "\n",
        "            has_species = any(et == \"SPECIES\" for _, et in entities_in_sentence)\n",
        "            if not has_species:\n",
        "                continue\n",
        "\n",
        "            # Tokenize sentence\n",
        "            inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "            # Get model outputs with attention weights\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            # Extract attention matrices from the last layer\n",
        "            # Shape: [batch_size, num_heads, seq_length, seq_length]\n",
        "            attention = outputs.attentions[-1].mean(dim=1).squeeze(0)  # Average over attention heads\n",
        "\n",
        "            # Map tokens back to entities\n",
        "            token_to_entity = {}\n",
        "            for entity, entity_type in entities_in_sentence:\n",
        "                entity_tokens = tokenizer.encode(entity, add_special_tokens=False)\n",
        "\n",
        "                # Find the entity tokens in the sentence tokens\n",
        "                sentence_tokens = inputs.input_ids[0].tolist()\n",
        "                for i in range(len(sentence_tokens) - len(entity_tokens) + 1):\n",
        "                    if sentence_tokens[i:i+len(entity_tokens)] == entity_tokens:\n",
        "                        for j in range(i, i+len(entity_tokens)):\n",
        "                            token_to_entity[j] = (entity, entity_type)\n",
        "\n",
        "            # Build entity attention matrix\n",
        "            entity_attention = {}\n",
        "            for i in range(attention.shape[0]):\n",
        "                if i in token_to_entity:\n",
        "                    entity_i, type_i = token_to_entity[i]\n",
        "                    for j in range(attention.shape[1]):\n",
        "                        if j in token_to_entity and i != j:\n",
        "                            entity_j, type_j = token_to_entity[j]\n",
        "\n",
        "                            # Skip if same entity\n",
        "                            if entity_i == entity_j:\n",
        "                                continue\n",
        "\n",
        "                            # Get attention score\n",
        "                            att_score = float(attention[i, j])\n",
        "\n",
        "                            key = ((entity_i, type_i), (entity_j, type_j))\n",
        "                            if key not in entity_attention or att_score > entity_attention[key][0]:\n",
        "                                entity_attention[key] = (att_score, sentence)\n",
        "\n",
        "            # Extract relationships based on entity types and attention scores\n",
        "            for ((entity1, type1), (entity2, type2)), (score, context) in entity_attention.items():\n",
        "                # Only consider high attention scores (threshold can be adjusted)\n",
        "                if score < 0.1:\n",
        "                    continue\n",
        "\n",
        "                # Define relationships based on entity types and attention score\n",
        "                if type1 == \"SPECIES\" and type2 == \"HABITAT\" and score > 0.3:\n",
        "                    relationship = (entity1, entity2, context, score)\n",
        "                    relationships[\"species_habitat\"].append(relationship)\n",
        "\n",
        "                elif type1 == \"SPECIES\" and type2 == \"ANIMAL_BODY_PART\" and score > 0.4:\n",
        "                    relationship = (entity1, entity2, context, score)\n",
        "                    relationships[\"species_body_part\"].append(relationship)\n",
        "\n",
        "                elif type1 == \"SPECIES\" and type2 in [\"MEASUREMENT\", \"LENGTH\"] and score > 0.3:\n",
        "                    relationship = (entity1, entity2, context, score)\n",
        "                    relationships[\"species_measurement\"].append(relationship)\n",
        "\n",
        "                elif type1 == \"SPECIES\" and type2 == \"ANIMAL_GROUP\" and score > 0.3:\n",
        "                    relationship = (entity1, entity2, context, score)\n",
        "                    relationships[\"species_group\"].append(relationship)\n",
        "\n",
        "                # Check reverse direction for asymmetric relationships\n",
        "                elif type2 == \"SPECIES\" and type1 == \"HABITAT\" and score > 0.3:\n",
        "                    relationship = (entity2, entity1, context, score)\n",
        "                    relationships[\"species_habitat\"].append(relationship)\n",
        "\n",
        "                elif type2 == \"SPECIES\" and type1 == \"ANIMAL_BODY_PART\" and score > 0.4:\n",
        "                    relationship = (entity2, entity1, context, score)\n",
        "                    relationships[\"species_body_part\"].append(relationship)\n",
        "\n",
        "                elif type2 == \"SPECIES\" and type1 in [\"MEASUREMENT\", \"LENGTH\"] and score > 0.3:\n",
        "                    relationship = (entity2, entity1, context, score)\n",
        "                    relationships[\"species_measurement\"].append(relationship)\n",
        "\n",
        "                elif type2 == \"SPECIES\" and type1 == \"ANIMAL_GROUP\" and score > 0.3:\n",
        "                    relationship = (entity2, entity1, context, score)\n",
        "                    relationships[\"species_group\"].append(relationship)\n",
        "\n",
        "        # Remove duplicates and sort by score\n",
        "        for relation_type in relationships:\n",
        "            unique_relations = {}\n",
        "            for entity1, entity2, context, score in relationships[relation_type]:\n",
        "                key = (entity1, entity2)\n",
        "                if key not in unique_relations or score > unique_relations[key][2]:\n",
        "                    unique_relations[key] = (entity1, entity2, score, context)\n",
        "\n",
        "            # Sort by attention score\n",
        "            relationships[relation_type] = [(e1, e2, ctx, score) for e1, e2, score, ctx in sorted(\n",
        "                unique_relations.values(), key=lambda x: x[2], reverse=True\n",
        "            )]\n",
        "\n",
        "            logger.info(f\"Found {len(relationships[relation_type])} unique {relation_type} relationships with attention\")\n",
        "\n",
        "        return relationships\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting relationships with attention: {str(e)}\", exc_info=True)\n",
        "        return {\"error\": [f\"Error extracting relationships with attention: {str(e)}\"]}\n",
        "\n",
        "# Function to create a relationship graph\n",
        "def create_relationship_graph(relationships, entity_types, min_weight=1, use_attention_scores=False):\n",
        "    \"\"\"\n",
        "    Create a NetworkX graph representing entity relationships.\n",
        "\n",
        "    Args:\n",
        "        relationships: Dictionary of relationship types and their instances\n",
        "        entity_types: List of entity types to include\n",
        "        min_weight: Minimum weight to include an edge\n",
        "        use_attention_scores: Whether to use attention scores for edge weights\n",
        "\n",
        "    Returns:\n",
        "        NetworkX graph\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes with entity type information\n",
        "    entity_nodes = set()\n",
        "\n",
        "    # Process each relationship type\n",
        "    for relation_type, relations in relationships.items():\n",
        "        for relation in relations:\n",
        "            if use_attention_scores and len(relation) == 4:\n",
        "                entity1, entity2, context, score = relation\n",
        "                weight = score  # Use attention score as weight\n",
        "            else:\n",
        "                if len(relation) == 3:\n",
        "                    entity1, entity2, context = relation\n",
        "                    weight = 1  # Default weight\n",
        "                else:\n",
        "                    continue  # Skip invalid relations\n",
        "\n",
        "            # Determine entity types\n",
        "            entity1_type = None\n",
        "            entity2_type = None\n",
        "\n",
        "            if relation_type == \"species_habitat\":\n",
        "                entity1_type = \"SPECIES\"\n",
        "                entity2_type = \"HABITAT\"\n",
        "            elif relation_type == \"species_body_part\":\n",
        "                entity1_type = \"SPECIES\"\n",
        "                entity2_type = \"ANIMAL_BODY_PART\"\n",
        "            elif relation_type == \"species_measurement\":\n",
        "                entity1_type = \"SPECIES\"\n",
        "                entity2_type = \"MEASUREMENT\"\n",
        "            elif relation_type == \"species_group\":\n",
        "                entity1_type = \"SPECIES\"\n",
        "                entity2_type = \"ANIMAL_GROUP\"\n",
        "\n",
        "            # Add nodes if they don't exist\n",
        "            if entity1 not in entity_nodes:\n",
        "                G.add_node(entity1, type=entity1_type)\n",
        "                entity_nodes.add(entity1)\n",
        "\n",
        "            if entity2 not in entity_nodes:\n",
        "                G.add_node(entity2, type=entity2_type)\n",
        "                entity_nodes.add(entity2)\n",
        "\n",
        "            # Add or update edge\n",
        "            if G.has_edge(entity1, entity2):\n",
        "                if use_attention_scores:\n",
        "                    # Use maximum attention score if multiple relations exist\n",
        "                    G[entity1][entity2]['weight'] = max(G[entity1][entity2]['weight'], weight)\n",
        "                else:\n",
        "                    G[entity1][entity2]['weight'] += 1\n",
        "\n",
        "                G[entity1][entity2]['examples'].append(context)\n",
        "                G[entity1][entity2]['types'].add(relation_type)\n",
        "            else:\n",
        "                G.add_edge(\n",
        "                    entity1,\n",
        "                    entity2,\n",
        "                    weight=weight,\n",
        "                    examples=[context],\n",
        "                    types={relation_type}\n",
        "                )\n",
        "\n",
        "    # Remove edges below the minimum weight\n",
        "    edges_to_remove = [(u, v) for u, v, d in G.edges(data=True) if d['weight'] < min_weight]\n",
        "    G.remove_edges_from(edges_to_remove)\n",
        "\n",
        "    # Remove isolated nodes\n",
        "    G.remove_nodes_from(list(nx.isolates(G)))\n",
        "\n",
        "    return G\n",
        "\n",
        "# Function to visualize entity relationships\n",
        "def visualize_entity_relationships(G, output_path=None, max_nodes=50, title=\"Entity Relationship Network\"):\n",
        "    \"\"\"\n",
        "    Create a visualization of entity relationships using NetworkX.\n",
        "\n",
        "    Args:\n",
        "        G: NetworkX graph of entity relationships\n",
        "        output_path: Path to save the visualization\n",
        "        max_nodes: Maximum number of nodes to include in the visualization\n",
        "        title: Title for the visualization\n",
        "\n",
        "    Returns:\n",
        "        Path to the saved visualization\n",
        "    \"\"\"\n",
        "    if output_path is None:\n",
        "        temp_dir = tempfile.mkdtemp()\n",
        "        output_file = os.path.join(temp_dir, \"entity_relationships.png\")\n",
        "    else:\n",
        "        output_file = output_path\n",
        "\n",
        "    # If graph is too large, take a subgraph\n",
        "    if len(G.nodes()) > max_nodes:\n",
        "        # Sort edges by weight\n",
        "        edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)\n",
        "\n",
        "        # Take top N edges\n",
        "        top_edges = edges[:max_nodes]\n",
        "\n",
        "        # Create subgraph\n",
        "        nodes = set()\n",
        "        for u, v, _ in top_edges:\n",
        "            nodes.add(u)\n",
        "            nodes.add(v)\n",
        "\n",
        "        G = G.subgraph(nodes)\n",
        "\n",
        "    # Set up plot\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Define node colors by type\n",
        "    color_map = {\n",
        "        'SPECIES': '#1f77b4',       # Blue\n",
        "        'HABITAT': '#2ca02c',       # Green\n",
        "        'ANIMAL_BODY_PART': '#d62728',  # Red\n",
        "        'MEASUREMENT': '#9467bd',   # Purple\n",
        "        'LENGTH': '#8c564b',        # Brown\n",
        "        'ANIMAL_GROUP': '#e377c2'   # Pink\n",
        "    }\n",
        "\n",
        "    # Get node colors\n",
        "    node_colors = [color_map.get(G.nodes[node]['type'], '#7f7f7f') for node in G.nodes()]\n",
        "\n",
        "    # Get edge weights for line thickness\n",
        "    edge_weights = [G[u][v]['weight'] * 2 for u, v in G.edges()]\n",
        "\n",
        "    # Create position layout\n",
        "    pos = nx.spring_layout(G, k=0.3, iterations=50, seed=42)\n",
        "\n",
        "    # Draw the graph\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=800, node_color=node_colors, alpha=0.8)\n",
        "\n",
        "    nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.6, edge_color='gray')\n",
        "\n",
        "    # Draw labels with smaller font size for better readability\n",
        "    nx.draw_networkx_labels(G, pos, font_size=8, font_family='sans-serif')\n",
        "\n",
        "    # Create legend for node types\n",
        "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w',\n",
        "                                 label=entity_type,\n",
        "                                 markerfacecolor=color, markersize=10)\n",
        "                      for entity_type, color in color_map.items()]\n",
        "\n",
        "    plt.legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save figure\n",
        "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Display in notebook if in notebook environment\n",
        "    try:\n",
        "        from IPython import get_ipython\n",
        "        if get_ipython() is not None:\n",
        "            plt.show()\n",
        "    except (ImportError, NameError):\n",
        "        pass\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "    return output_file\n",
        "\n",
        "# Function to create a heatmap visualization of entity co-occurrences\n",
        "def visualize_cooccurrence_matrix(cooccurrence_df, output_path=None, max_items=50):\n",
        "    \"\"\"\n",
        "    Create a heatmap visualization of entity co-occurrences.\n",
        "\n",
        "    Args:\n",
        "        cooccurrence_df: DataFrame with co-occurrence counts\n",
        "        output_path: Path to save the visualization\n",
        "        max_items: Maximum number of items to include in the visualization\n",
        "\n",
        "    Returns:\n",
        "        Path to the saved visualization\n",
        "    \"\"\"\n",
        "    if output_path is None:\n",
        "        temp_dir = tempfile.mkdtemp()\n",
        "        output_file = os.path.join(temp_dir, \"entity_cooccurrence.png\")\n",
        "    else:\n",
        "        output_file = output_path\n",
        "\n",
        "    # If matrix is too large, take a subset with highest values\n",
        "    if len(cooccurrence_df) > max_items:\n",
        "        # Get the sum of co-occurrences for each entity\n",
        "        row_sums = cooccurrence_df.sum(axis=1)\n",
        "\n",
        "        # Get the top entities\n",
        "        top_entities = row_sums.nlargest(max_items).index\n",
        "\n",
        "        # Filter the matrix\n",
        "        cooccurrence_df = cooccurrence_df.loc[top_entities, top_entities]\n",
        "\n",
        "    # Set up figure\n",
        "    plt.figure(figsize=(16, 14))\n",
        "\n",
        "    # Create a custom colormap from white to blue\n",
        "    colors = [(1, 1, 1), (0.12, 0.47, 0.71)]\n",
        "    cmap = LinearSegmentedColormap.from_list(\"custom_blue\", colors, N=100)\n",
        "\n",
        "    # Create heatmap\n",
        "    ax = sns.heatmap(cooccurrence_df, cmap=cmap, linewidths=0.5,\n",
        "                     linecolor='gray', square=True, cbar_kws={\"shrink\": 0.8})\n",
        "\n",
        "    # Rotate x-axis labels for better readability\n",
        "    plt.xticks(rotation=90, fontsize=8)\n",
        "    plt.yticks(fontsize=8)\n",
        "\n",
        "    plt.title(\"Entity Co-occurrence Matrix\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save figure\n",
        "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Display in notebook if in notebook environment\n",
        "    try:\n",
        "        from IPython import get_ipython\n",
        "        if get_ipython() is not None:\n",
        "            plt.show()\n",
        "    except (ImportError, NameError):\n",
        "        pass\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "    return output_file"
      ],
      "metadata": {
        "id": "OGYpWFD0CfCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced function for processing PDFs with relationship detection\n",
        "def process_pdf_with_relationships(pdf_path, entity_types, selected_models, output_dir=None, use_attention=True):\n",
        "    \"\"\"\n",
        "    Process a PDF file to extract entities and their relationships using multiple models.\n",
        "\n",
        "    Args:\n",
        "        pdf_path: Path to the PDF file\n",
        "        entity_types: List of entity types to extract\n",
        "        selected_models: List of NLP models to use\n",
        "        output_dir: Directory to save outputs\n",
        "        use_attention: Whether to use transformer attention for relationship detection\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (result_text, text_sample, files)\n",
        "    \"\"\"\n",
        "    if not pdf_path or not os.path.exists(pdf_path):\n",
        "        return \"Please provide a valid PDF file path.\", None, {}\n",
        "\n",
        "    if not entity_types:\n",
        "        return \"Please specify at least one entity type to extract.\", None, {}\n",
        "\n",
        "    if not selected_models:\n",
        "        return \"Please specify at least one NLP model.\", None, {}\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        files = {}\n",
        "\n",
        "        if output_dir and not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        # Extract text from PDF\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        text_extraction_time = time.time() - start_time\n",
        "        logger.info(f\"Text extraction completed in {text_extraction_time:.2f} seconds\")\n",
        "\n",
        "        if isinstance(text, str) and text.startswith(\"Error\"):\n",
        "            return text, None, {}\n",
        "\n",
        "        # Extract entities using each selected model\n",
        "        extracted_by_model = {}\n",
        "        entity_extraction_times = {}\n",
        "        relationships_by_model = {}\n",
        "        relationship_times = {}\n",
        "\n",
        "        # First, extract entities with all models\n",
        "        for model_name in selected_models:\n",
        "            if model_name == \"BERT-base\" and \"BERT-base\" not in selected_models:\n",
        "                # Skip attention-only models for entity extraction\n",
        "                continue\n",
        "\n",
        "            model_start_time = time.time()\n",
        "            extracted = extract_entities(text, entity_types, model_name)\n",
        "            extraction_time = time.time() - model_start_time\n",
        "\n",
        "            if isinstance(extracted, str) and extracted.startswith(\"Error\"):\n",
        "                extracted_by_model[model_name] = {\"ERROR\": [extracted]}\n",
        "            else:\n",
        "                extracted_by_model[model_name] = extracted\n",
        "\n",
        "            entity_extraction_times[model_name] = extraction_time\n",
        "            logger.info(f\"Entity extraction with {model_name} completed in {extraction_time:.2f} seconds\")\n",
        "\n",
        "        # Then, extract relationships for each model that has valid entities\n",
        "        for model_name in selected_models:\n",
        "            if model_name not in extracted_by_model or \"ERROR\" in extracted_by_model[model_name]:\n",
        "                continue\n",
        "\n",
        "            # Skip relationship extraction if no entities were found\n",
        "            if sum(len(entities) for entities in extracted_by_model[model_name].values()) == 0:\n",
        "                relationships_by_model[model_name] = {\n",
        "                    \"species_habitat\": [],\n",
        "                    \"species_body_part\": [],\n",
        "                    \"species_measurement\": [],\n",
        "                    \"species_group\": []\n",
        "                }\n",
        "                relationship_times[model_name] = 0\n",
        "                continue\n",
        "\n",
        "            # Extract relationships\n",
        "            relationship_start_time = time.time()\n",
        "\n",
        "            # Use attention-based extraction if requested and model is not already an attention model\n",
        "            if use_attention and model_name != \"BERT-base\":\n",
        "                logger.info(f\"Extracting relationships with attention model for {model_name} entities\")\n",
        "                relationships = extract_entity_relationships_with_attention(\n",
        "                    text, extracted_by_model[model_name], \"BERT-base\"\n",
        "                )\n",
        "            elif model_name.startswith(\"SpaCy\"):\n",
        "                # Use pattern-based extraction for SpaCy models\n",
        "                logger.info(f\"Extracting relationships with pattern-based approach for {model_name}\")\n",
        "                relationships = extract_entity_relationships(\n",
        "                    text, extracted_by_model[model_name], nlp_models[model_name]\n",
        "                )\n",
        "            else:\n",
        "                # Skip relationship extraction for non-SpaCy models without attention\n",
        "                logger.warning(f\"Skipping relationship extraction for {model_name} - not supported\")\n",
        "                relationships = {\n",
        "                    \"species_habitat\": [],\n",
        "                    \"species_body_part\": [],\n",
        "                    \"species_measurement\": [],\n",
        "                    \"species_group\": []\n",
        "                }\n",
        "\n",
        "            relationship_time = time.time() - relationship_start_time\n",
        "\n",
        "            if \"error\" in relationships:\n",
        "                logger.error(f\"Error extracting relationships: {relationships['error']}\")\n",
        "                relationships = {\n",
        "                    \"species_habitat\": [],\n",
        "                    \"species_body_part\": [],\n",
        "                    \"species_measurement\": [],\n",
        "                    \"species_group\": []\n",
        "                }\n",
        "\n",
        "            relationships_by_model[model_name] = relationships\n",
        "            relationship_times[model_name] = relationship_time\n",
        "\n",
        "            logger.info(f\"Relationship extraction for {model_name} completed in {relationship_time:.2f} seconds\")\n",
        "\n",
        "            # Create visualizations for each model\n",
        "            if sum(len(relations) for relations in relationships.values()) > 0:\n",
        "                # Create relationship graph\n",
        "                graph_method = \"_attention\" if use_attention else \"_pattern\"\n",
        "                if output_dir:\n",
        "                    graph_file = os.path.join(output_dir, f\"{model_name}_relationship_graph{graph_method}.png\")\n",
        "                else:\n",
        "                    graph_file = None\n",
        "\n",
        "                use_scores = graph_method == \"_attention\"\n",
        "                G = create_relationship_graph(relationships, entity_types, min_weight=0.2 if use_scores else 1,\n",
        "                                             use_attention_scores=use_scores)\n",
        "\n",
        "                if G.number_of_nodes() > 0:\n",
        "                    graph_viz = visualize_entity_relationships(\n",
        "                        G, graph_file,\n",
        "                        title=f\"{model_name} Entity Relationships ({graph_method.strip('_')})\"\n",
        "                    )\n",
        "\n",
        "                    if graph_viz:\n",
        "                        files[f\"{model_name}_graph{graph_method}\"] = graph_viz\n",
        "\n",
        "                # Generate co-occurrence matrix for the entities\n",
        "                matrix_df = generate_cooccurrence_matrix(extracted_by_model[model_name], entity_types)\n",
        "\n",
        "                if not matrix_df.empty:\n",
        "                    if output_dir:\n",
        "                        matrix_file = os.path.join(output_dir, f\"{model_name}_cooccurrence_matrix.png\")\n",
        "                    else:\n",
        "                        matrix_file = None\n",
        "\n",
        "                    matrix_viz = visualize_cooccurrence_matrix(matrix_df, matrix_file)\n",
        "\n",
        "                    if matrix_viz:\n",
        "                        files[f\"{model_name}_matrix\"] = matrix_viz\n",
        "\n",
        "        # Create comparison chart for entities\n",
        "        if output_dir:\n",
        "            chart_output_path = os.path.join(output_dir, \"entity_extraction_comparison.png\")\n",
        "        else:\n",
        "            chart_output_path = None\n",
        "\n",
        "        chart_file = generate_comparison_chart(extracted_by_model, entity_types,\n",
        "                                             [m for m in selected_models if m != \"BERT-base\"],\n",
        "                                             chart_output_path)\n",
        "\n",
        "        if chart_file:\n",
        "            files[\"entity_comparison\"] = chart_file\n",
        "\n",
        "        # Prepare the results document\n",
        "        result_text = \"# Animal Entity Extraction and Relationship Analysis\\n\\n\"\n",
        "        result_text += \"## Summary of Results\\n\\n\"\n",
        "\n",
        "        # Entity counts by model\n",
        "        result_text += \"### Entity Count by Model\\n\\n\"\n",
        "        result_text += \"| Entity Type | \" + \" | \".join([m for m in selected_models if m != \"BERT-base\"]) + \" |\\n\"\n",
        "        result_text += \"|\" + \"---|\" * (len([m for m in selected_models if m != \"BERT-base\"]) + 1) + \"\\n\"\n",
        "\n",
        "        for entity_type in entity_types:\n",
        "            result_text += f\"| {entity_type} |\"\n",
        "            for model_name in selected_models:\n",
        "                if model_name == \"BERT-base\":\n",
        "                    continue\n",
        "                if model_name in extracted_by_model and \"ERROR\" not in extracted_by_model[model_name]:\n",
        "                    count = len(extracted_by_model[model_name].get(entity_type, []))\n",
        "                    result_text += f\" {count} |\"\n",
        "                else:\n",
        "                    result_text += \" Error |\"\n",
        "            result_text += \"\\n\"\n",
        "\n",
        "        # Relationship counts by model\n",
        "        result_text += \"\\n### Relationship Count by Model\\n\\n\"\n",
        "        result_text += \"| Relationship Type | \" + \" | \".join(selected_models) + \" |\\n\"\n",
        "        result_text += \"|\" + \"---|\" * (len(selected_models) + 1) + \"\\n\"\n",
        "\n",
        "        for rel_type in [\"species_habitat\", \"species_body_part\", \"species_measurement\", \"species_group\"]:\n",
        "            result_text += f\"| {rel_type.replace('_', ' ').title()} |\"\n",
        "            for model_name in selected_models:\n",
        "                if model_name in relationships_by_model:\n",
        "                    count = len(relationships_by_model[model_name].get(rel_type, []))\n",
        "                    result_text += f\" {count} |\"\n",
        "                else:\n",
        "                    result_text += \" N/A |\"\n",
        "            result_text += \"\\n\"\n",
        "\n",
        "        # Processing times\n",
        "        result_text += \"\\n### Processing Times\\n\\n\"\n",
        "        result_text += \"| Model | Entity Extraction (s) | Relationship Extraction (s) |\\n\"\n",
        "        result_text += \"|---|---|---|\\n\"\n",
        "        for model_name in selected_models:\n",
        "            if model_name == \"BERT-base\":\n",
        "                continue\n",
        "            entity_time = entity_extraction_times.get(model_name, \"N/A\")\n",
        "            if not isinstance(entity_time, str):\n",
        "                entity_time = f\"{entity_time:.2f}\"\n",
        "\n",
        "            rel_time = relationship_times.get(model_name, \"N/A\")\n",
        "            if not isinstance(rel_time, str):\n",
        "                rel_time = f\"{rel_time:.2f}\"\n",
        "\n",
        "            result_text += f\"| {model_name} | {entity_time} | {rel_time} |\\n\"\n",
        "\n",
        "        # Add attention model time if used\n",
        "        if use_attention and \"BERT-base\" in selected_models:\n",
        "            result_text += f\"| BERT-base (attention) | N/A | N/A |\\n\"\n",
        "\n",
        "        # Add detailed entity results for each model\n",
        "        for model_name in selected_models:\n",
        "            if model_name == \"BERT-base\":\n",
        "                continue\n",
        "\n",
        "            result_text += f\"\\n## Entity Extraction Results: {model_name}\\n\\n\"\n",
        "\n",
        "            if model_name in extracted_by_model:\n",
        "                if \"ERROR\" in extracted_by_model[model_name]:\n",
        "                    result_text += f\"Error: {extracted_by_model[model_name]['ERROR'][0]}\\n\\n\"\n",
        "                    continue\n",
        "\n",
        "                for entity_type in entity_types:\n",
        "                    entities = extracted_by_model[model_name].get(entity_type, [])\n",
        "                    if entities:\n",
        "                        result_text += f\"### {entity_type} ({len(entities)} found)\\n\\n\"\n",
        "                        # Show up to 20 items, then summarize the rest\n",
        "                        for item in entities[:20]:\n",
        "                            result_text += f\"- {item}\\n\"\n",
        "\n",
        "                        if len(entities) > 20:\n",
        "                            result_text += f\"- *...and {len(entities) - 20} more*\\n\"\n",
        "                        result_text += \"\\n\"\n",
        "                    else:\n",
        "                        result_text += f\"### {entity_type}\\n\\n\"\n",
        "                        result_text += \"No entities of this type found.\\n\\n\"\n",
        "            else:\n",
        "                result_text += \"No results available.\\n\\n\"\n",
        "\n",
        "        # Add relationship results for each model\n",
        "        for model_name in selected_models:\n",
        "            if model_name not in relationships_by_model:\n",
        "                continue\n",
        "\n",
        "            method = \"Attention\" if use_attention and model_name != \"BERT-base\" else \"Pattern\"\n",
        "            result_text += f\"\\n## Relationship Results: {model_name} ({method}-based)\\n\\n\"\n",
        "\n",
        "            relationships = relationships_by_model[model_name]\n",
        "            total_relationships = sum(len(relations) for relations in relationships.values())\n",
        "\n",
        "            if total_relationships == 0:\n",
        "                result_text += \"No relationships found.\\n\\n\"\n",
        "                continue\n",
        "\n",
        "            result_text += f\"Found {total_relationships} total relationships.\\n\\n\"\n",
        "\n",
        "            # Report on each relationship type\n",
        "            for relation_type, relations in relationships.items():\n",
        "                if not relations:\n",
        "                    continue\n",
        "\n",
        "                relation_name = relation_type.replace('_', ' ').title()\n",
        "                result_text += f\"### {relation_name} ({len(relations)} found)\\n\\n\"\n",
        "\n",
        "                # Show top examples (up to 10)\n",
        "                max_examples = min(10, len(relations))\n",
        "\n",
        "                for idx, relation in enumerate(relations[:max_examples]):\n",
        "                    if use_attention and len(relation) == 4:\n",
        "                        entity1, entity2, context, score = relation\n",
        "                        result_text += f\"{idx+1}. **{entity1}** → **{entity2}** (score: {score:.2f})\\n\"\n",
        "                    else:\n",
        "                        entity1, entity2, context = relation\n",
        "                        result_text += f\"{idx+1}. **{entity1}** → **{entity2}**\\n\"\n",
        "\n",
        "                    # Show a snippet of the context\n",
        "                    context_snippet = context[:100] + \"...\" if len(context) > 100 else context\n",
        "                    result_text += f\"   *Context: \\\"{context_snippet.strip()}\\\"*\\n\\n\"\n",
        "\n",
        "                if len(relations) > max_examples:\n",
        "                    result_text += f\"*...and {len(relations) - max_examples} more relationships*\\n\\n\"\n",
        "\n",
        "        # Add visualization information\n",
        "        if files:\n",
        "            result_text += \"\\n## Visualizations\\n\\n\"\n",
        "            for name, file_path in files.items():\n",
        "                result_text += f\"- **{name}**: {file_path}\\n\"\n",
        "\n",
        "            # Add description for each type of visualization\n",
        "            result_text += \"\\n### Visualization Types\\n\\n\"\n",
        "            result_text += \"1. **Entity Comparison**: Bar chart comparing entity counts across models\\n\"\n",
        "            result_text += \"2. **Relationship Graph**: Network visualization showing connections between entities\\n\"\n",
        "            result_text += \"3. **Co-occurrence Matrix**: Heatmap showing which entity types appear together\\n\\n\"\n",
        "\n",
        "        # Performance summary\n",
        "        total_time = time.time() - start_time\n",
        "        result_text += f\"\\n## Performance Summary\\n\\n\"\n",
        "        result_text += f\"Total processing time: {total_time:.2f} seconds\\n\"\n",
        "        result_text += f\"- Text extraction: {text_extraction_time:.2f} seconds\\n\"\n",
        "\n",
        "        # Text sample\n",
        "        text_sample = text[:500] + \"...\" if len(text) > 500 else text\n",
        "        result_text += f\"\\n## Sample of Extracted Text\\n\\n```\\n{text_sample}\\n```\\n\"\n",
        "\n",
        "        return result_text, text_sample, files\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n",
        "        return f\"Unexpected error: {str(e)}\", None, {}\n",
        "\n",
        "# Function to generate a co-occurrence matrix of entities\n",
        "def generate_cooccurrence_matrix(extracted_entities, entity_types):\n",
        "    \"\"\"\n",
        "    Generate a co-occurrence matrix of entities based on entity types.\n",
        "\n",
        "    Args:\n",
        "        extracted_entities: Dictionary of extracted entities by type\n",
        "        entity_types: List of entity types to include\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with co-occurrence counts\n",
        "    \"\"\"\n",
        "    # Create a flat list of all entities with their types\n",
        "    all_entities = []\n",
        "    for entity_type in entity_types:\n",
        "        entities = extracted_entities.get(entity_type, [])\n",
        "        all_entities.extend([(entity, entity_type) for entity in entities])\n",
        "\n",
        "    # Create a dictionary to store co-occurrence counts\n",
        "    cooccurrence = defaultdict(int)\n",
        "\n",
        "    # Count co-occurrences by looking at all pairs\n",
        "    for (entity1, type1), (entity2, type2) in combinations(all_entities, 2):\n",
        "        if type1 != type2:  # Only interested in relationships between different types\n",
        "            key = (f\"{type1}: {entity1}\", f\"{type2}: {entity2}\")\n",
        "            cooccurrence[key] += 1\n",
        "\n",
        "    # Convert to a matrix format\n",
        "    unique_labeled_entities = sorted(set([f\"{type}: {entity}\" for entity, type in all_entities]))\n",
        "    matrix_size = len(unique_labeled_entities)\n",
        "\n",
        "    # Map entity labels to indices\n",
        "    entity_to_idx = {entity: i for i, entity in enumerate(unique_labeled_entities)}\n",
        "\n",
        "    # Create a sparse matrix\n",
        "    row, col, data = [], [], []\n",
        "\n",
        "    for (entity1, entity2), count in cooccurrence.items():\n",
        "        if entity1 in entity_to_idx and entity2 in entity_to_idx:\n",
        "            i, j = entity_to_idx[entity1], entity_to_idx[entity2]\n",
        "            row.append(i)\n",
        "            col.append(j)\n",
        "            data.append(count)\n",
        "            # Make it symmetric\n",
        "            row.append(j)\n",
        "            col.append(i)\n",
        "            data.append(count)\n",
        "\n",
        "    # Create the sparse matrix\n",
        "    if not row:  # Empty matrix\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    matrix = csr_matrix((data, (row, col)), shape=(matrix_size, matrix_size))\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    cooccurrence_df = pd.DataFrame(matrix.toarray(), index=unique_labeled_entities, columns=unique_labeled_entities)\n",
        "\n",
        "    return cooccurrence_df\n",
        "import pdfplumber\n",
        "import spacy\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import logging\n",
        "import time\n",
        "import os\n",
        "import tempfile\n",
        "from io import BytesIO\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline, AutoModel\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from scipy.sparse import csr_matrix\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import warnings\n",
        "from IPython.display import display, HTML, Image\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.StreamHandler()]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Dictionary to store loaded models\n",
        "nlp_models = {}\n",
        "transformer_models = {}\n",
        "attention_models = {}\n",
        "\n",
        "# Available NLP models\n",
        "AVAILABLE_MODELS = {\n",
        "    \"SpaCy Small\": \"en_core_web_sm\",\n",
        "    \"SpaCy Large\": \"en_core_web_lg\",\n",
        "    \"BioBERT\": \"dmis-lab/biobert-base-cased-v1.1-named-entity-recognition\",\n",
        "    \"BERT-base\": \"bert-base-uncased\"  # Added for attention-based relationship extraction\n",
        "}\n",
        "\n",
        "# Function to load NLP model\n",
        "def load_nlp_model(model_name):\n",
        "    if model_name in nlp_models:\n",
        "        return nlp_models[model_name]\n",
        "\n",
        "    model_path = AVAILABLE_MODELS.get(model_name)\n",
        "    if not model_path:\n",
        "        logger.error(f\"Unknown model: {model_name}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # BioBERT uses HuggingFace Transformers\n",
        "        if model_name == \"BioBERT\":\n",
        "            logger.info(f\"Loading BioBERT model: {model_path}\")\n",
        "            try:\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "                model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "\n",
        "                # Create NER pipeline\n",
        "                ner_model = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)\n",
        "                transformer_models[model_name] = ner_model\n",
        "                nlp_models[model_name] = \"transformer\"\n",
        "                logger.info(f\"BioBERT model loaded successfully\")\n",
        "                return \"transformer\"\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading BioBERT model: {str(e)}\")\n",
        "                logger.warning(\"Please install transformers: pip install transformers torch\")\n",
        "                return None\n",
        "        elif model_name == \"BERT-base\":\n",
        "            logger.info(f\"Loading BERT-base model for attention: {model_path}\")\n",
        "            try:\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "                model = AutoModel.from_pretrained(model_path, output_attentions=True)\n",
        "\n",
        "                attention_models[model_name] = {\n",
        "                    \"tokenizer\": tokenizer,\n",
        "                    \"model\": model\n",
        "                }\n",
        "                nlp_models[model_name] = \"attention\"\n",
        "                logger.info(f\"BERT-base model loaded successfully for attention analysis\")\n",
        "                return \"attention\"\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading BERT-base model: {str(e)}\")\n",
        "                logger.warning(\"Please install transformers: pip install transformers torch\")\n",
        "                return None\n",
        "        else:\n",
        "            # SpaCy models\n",
        "            logger.info(f\"Loading model: {model_name} ({model_path})\")\n",
        "            nlp = spacy.load(model_path)\n",
        "\n",
        "            # Add animal entity patterns\n",
        "            if \"entity_ruler\" not in nlp.pipe_names:\n",
        "                ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "                patterns = [\n",
        "                    # Improved SPECIES pattern for scientific names (Genus species)\n",
        "                    {\"label\": \"SPECIES\", \"pattern\": [\n",
        "                        {\"TEXT\": {\"REGEX\": \"^[A-Z][a-z]+$\"}},  # Genus - capitalized word\n",
        "                        {\"TEXT\": {\"REGEX\": \"^[a-z]+$\"}}        # species - lowercase word\n",
        "                    ]},\n",
        "                    # Common animal names pattern\n",
        "                    {\"label\": \"SPECIES\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"dog\", \"cat\", \"horse\", \"cow\", \"pig\", \"sheep\", \"goat\", \"chicken\",\n",
        "                                        \"fish\", \"salmon\", \"trout\", \"bass\", \"butterfly\", \"bee\", \"ant\", \"spider\",\n",
        "                                        \"lion\", \"tiger\", \"bear\", \"elephant\", \"giraffe\", \"rhinoceros\", \"zebra\",\n",
        "                                        \"cheetah\", \"leopard\", \"wolf\", \"fox\", \"deer\", \"moose\", \"eagle\", \"hawk\",\n",
        "                                        \"owl\", \"parrot\", \"dolphin\", \"whale\", \"shark\", \"turtle\", \"snake\",\n",
        "                                        \"crocodile\", \"alligator\", \"frog\", \"toad\"]}}\n",
        "                    ]},\n",
        "                    # Animal groups with adjectives\n",
        "                    {\"label\": \"SPECIES\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"african\", \"asian\", \"american\", \"european\", \"australian\",\n",
        "                                         \"western\", \"eastern\", \"northern\", \"southern\"]}},\n",
        "                        {\"LOWER\": {\"IN\": [\"lion\", \"tiger\", \"bear\", \"elephant\", \"giraffe\", \"rhinoceros\",\n",
        "                                         \"zebra\", \"cheetah\", \"leopard\", \"wolf\", \"fox\", \"deer\", \"moose\",\n",
        "                                         \"eagle\", \"hawk\", \"owl\", \"parrot\", \"dolphin\", \"whale\", \"shark\",\n",
        "                                         \"turtle\", \"snake\", \"crocodile\", \"alligator\"]}}\n",
        "                    ]},\n",
        "                    # Animal groups\n",
        "                    {\"label\": \"ANIMAL_GROUP\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"herd\", \"flock\", \"pack\", \"pod\", \"school\", \"colony\", \"pride\",\n",
        "                                         \"swarm\", \"murder\", \"gaggle\", \"hive\", \"brood\", \"drift\"]}}\n",
        "                    ]},\n",
        "                    # Animal body parts\n",
        "                    {\"label\": \"ANIMAL_BODY_PART\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"paw\", \"claw\", \"hoof\", \"beak\", \"wing\", \"fin\", \"tail\", \"horn\",\n",
        "                                         \"tusk\", \"scale\", \"feather\", \"fur\", \"mane\", \"whisker\", \"snout\",\n",
        "                                         \"bill\", \"gills\", \"tentacle\", \"antenna\"]}}\n",
        "                    ]},\n",
        "                    # Measurements\n",
        "                    {\"label\": \"MEASUREMENT\", \"pattern\": [\n",
        "                        {\"SHAPE\": {\"IN\": [\"d+\", \"d+.d+\"]}},\n",
        "                        {\"LOWER\": {\"IN\": [\"mm\", \"cm\", \"m\", \"kg\", \"g\", \"ml\", \"l\"]}}\n",
        "                    ]},\n",
        "                    # Length measurements\n",
        "                    {\"label\": \"LENGTH\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"fork\", \"total\", \"standard\", \"body\", \"tail\"]}},\n",
        "                        {\"LOWER\": \"length\"}\n",
        "                    ]},\n",
        "                    # Habitats\n",
        "                    {\"label\": \"HABITAT\", \"pattern\": [\n",
        "                        {\"LOWER\": {\"IN\": [\"forest\", \"jungle\", \"savanna\", \"ocean\", \"lake\", \"river\", \"mountain\",\n",
        "                                        \"desert\", \"tundra\", \"reef\", \"farm\", \"pasture\", \"pen\", \"enclosure\",\n",
        "                                        \"cage\", \"burrow\", \"nest\", \"den\", \"wetland\", \"bog\", \"marsh\"]}}\n",
        "                    ]},\n",
        "                ]\n",
        "                ruler.add_patterns(patterns)\n",
        "\n",
        "            nlp_models[model_name] = nlp\n",
        "            logger.info(f\"SpaCy model {model_name} loaded successfully\")\n",
        "            return nlp\n",
        "    except OSError as e:\n",
        "        logger.warning(f\"Error loading model {model_path}: {str(e)}\")\n",
        "        if model_name.startswith(\"SpaCy\"):\n",
        "            logger.warning(f\"Please download it using: python -m spacy download {model_path}\")\n",
        "        elif model_name in [\"BioBERT\", \"BERT-base\"]:\n",
        "            logger.warning(\"Please install transformers: pip install transformers torch\")\n",
        "\n",
        "        # Create a blank model as fallback if it's a SpaCy model\n",
        "        if model_name.startswith(\"SpaCy\"):\n",
        "            nlp = spacy.blank(\"en\")\n",
        "            nlp_models[model_name] = nlp\n",
        "            return nlp\n",
        "        return None\n",
        "\n",
        "# Check which models are installed\n",
        "def get_available_models():\n",
        "    installed_models = []\n",
        "\n",
        "    # Check SpaCy models\n",
        "    for name, path in AVAILABLE_MODELS.items():\n",
        "        if name.startswith(\"SpaCy\"):\n",
        "            try:\n",
        "                spacy.load(path)\n",
        "                installed_models.append(name)\n",
        "            except OSError:\n",
        "                pass\n",
        "        elif name in [\"BioBERT\", \"BERT-base\"]:\n",
        "            try:\n",
        "                # Check if transformers is installed\n",
        "                import transformers\n",
        "\n",
        "                # Add transformer models to available models regardless of whether they're downloaded\n",
        "                # We'll handle the download when it's first used\n",
        "                installed_models.append(name)\n",
        "\n",
        "                # Just log that we'll download when needed\n",
        "                logger.info(f\"{name} model will be downloaded automatically when first used.\")\n",
        "            except ImportError:\n",
        "                logger.warning(\"Transformers package not installed. Transformer models will not be available.\")\n",
        "\n",
        "    # Ensure at least one model is available\n",
        "    if not installed_models:\n",
        "        try:\n",
        "            spacy.load(\"en_core_web_sm\")\n",
        "            installed_models.append(\"SpaCy Small\")\n",
        "        except OSError:\n",
        "            installed_models.append(\"Blank Model\")\n",
        "            logger.warning(\"No SpaCy models installed. Please run: python -m spacy download en_core_web_sm\")\n",
        "            logger.warning(\"For transformer models: pip install transformers torch\")\n",
        "\n",
        "    return installed_models\n",
        "\n",
        "# Function to extract text from PDF with better error handling\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    if not pdf_file or not os.path.exists(pdf_file):\n",
        "        return \"No file provided or file does not exist.\"\n",
        "\n",
        "    try:\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(pdf_file) as pdf:\n",
        "            total_pages = len(pdf.pages)\n",
        "            logger.info(f\"Processing PDF with {total_pages} pages\")\n",
        "\n",
        "            for i, page in enumerate(pdf.pages):\n",
        "                logger.info(f\"Extracting text from page {i+1}/{total_pages}\")\n",
        "                extracted_text = page.extract_text()\n",
        "                if extracted_text:\n",
        "                    text += extracted_text + \"\\n\"\n",
        "\n",
        "        if not text.strip():\n",
        "            return \"No text could be extracted from the PDF.\"\n",
        "\n",
        "        logger.info(f\"Successfully extracted {len(text)} characters of text\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing PDF: {str(e)}\", exc_info=True)\n",
        "        return f\"Error processing PDF: {str(e)}\"\n",
        "\n",
        "# Extract entities using BioBERT\n",
        "def extract_entities_biobert(text, entity_types):\n",
        "    try:\n",
        "        if \"BioBERT\" not in transformer_models:\n",
        "            model_path = AVAILABLE_MODELS.get(\"BioBERT\")\n",
        "            try:\n",
        "                logger.info(f\"BioBERT model not loaded yet. Downloading and initializing...\")\n",
        "\n",
        "                # Set download settings\n",
        "                logger.info(\"Setting up to download BioBERT model...\")\n",
        "\n",
        "                # Print info about what's happening\n",
        "                print(\"\\nDownloading BioBERT model. This may take a few minutes on first run...\")\n",
        "                print(\"Model: dmis-lab/biobert-base-cased-v1.1-named-entity-recognition\")\n",
        "\n",
        "                # Load tokenizer with progress reporting\n",
        "                print(\"Loading tokenizer...\")\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "                # Load model with progress reporting\n",
        "                print(\"Loading model weights (this might take a while)...\")\n",
        "                model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "\n",
        "                print(\"Creating NER pipeline...\")\n",
        "                ner_model = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)\n",
        "\n",
        "                transformer_models[\"BioBERT\"] = ner_model\n",
        "                print(\"BioBERT model loaded successfully!\\n\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading BioBERT model: {str(e)}\")\n",
        "                print(f\"\\nError: Could not load BioBERT model. Details: {str(e)}\")\n",
        "                print(\"Common solutions:\")\n",
        "                print(\"1. Check internet connection\")\n",
        "                print(\"2. Ensure 'transformers' and 'torch' are installed: pip install transformers torch\")\n",
        "                print(\"3. Try again or use a different model\\n\")\n",
        "                return f\"Error: Could not load BioBERT model: {str(e)}\"\n",
        "\n",
        "        ner_model = transformer_models[\"BioBERT\"]\n",
        "\n",
        "        # Process text in chunks to avoid CUDA out of memory errors\n",
        "        max_length = 512  # Maximum length for BERT models\n",
        "        chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
        "\n",
        "        logger.info(f\"Processing {len(chunks)} chunks with BioBERT\")\n",
        "\n",
        "        all_entities = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            if i % 10 == 0:\n",
        "                logger.info(f\"Processing chunk {i+1}/{len(chunks)}\")\n",
        "\n",
        "            if not chunk.strip():\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                chunk_entities = ner_model(chunk)\n",
        "                all_entities.extend(chunk_entities)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error processing chunk {i}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Map BioBERT entity types to our types and extract entities\n",
        "        extracted = {entity_type: [] for entity_type in entity_types}\n",
        "\n",
        "        # BioBERT's default entity types: 'B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'\n",
        "        # Map BioBERT entity types to our custom types\n",
        "        entity_mapping = {\n",
        "            'B-MISC': ['ANIMAL_BODY_PART', 'MEASUREMENT', 'LENGTH'],\n",
        "            'I-MISC': ['ANIMAL_BODY_PART', 'MEASUREMENT', 'LENGTH'],\n",
        "            'B-PER': ['SPECIES'],  # Sometimes species names are recognized as persons\n",
        "            'I-PER': ['SPECIES'],\n",
        "            'B-ORG': ['ANIMAL_GROUP'],\n",
        "            'I-ORG': ['ANIMAL_GROUP'],\n",
        "            'B-LOC': ['HABITAT'],\n",
        "            'I-LOC': ['HABITAT'],\n",
        "        }\n",
        "\n",
        "        # Process entities\n",
        "        for entity in all_entities:\n",
        "            entity_text = entity['word']\n",
        "            entity_type = entity['entity']\n",
        "            score = entity['score']\n",
        "\n",
        "            # Only consider entities with confidence above threshold\n",
        "            if score < 0.75:\n",
        "                continue\n",
        "\n",
        "            # Map BioBERT entity types to our types\n",
        "            mapped_types = entity_mapping.get(entity_type, [])\n",
        "\n",
        "            # Add entity to all compatible types\n",
        "            for mapped_type in mapped_types:\n",
        "                if mapped_type in entity_types:\n",
        "                    extracted[mapped_type].append(entity_text)\n",
        "\n",
        "        # Additional regex-based extraction for specific entity types\n",
        "        # Scientific species names (Genus species)\n",
        "        if \"SPECIES\" in entity_types:\n",
        "            species_pattern = r'\\b[A-Z][a-z]+\\s+[a-z]+\\b'\n",
        "            scientific_names = re.findall(species_pattern, text)\n",
        "            extracted[\"SPECIES\"].extend(scientific_names)\n",
        "\n",
        "            # Common animal names\n",
        "            common_animals = re.findall(r'\\b(?:African|Asian|American|European|Australian|Western|Eastern|Northern|Southern)?\\s*(?:Lion|Tiger|Bear|Elephant|Giraffe|Rhinoceros|Zebra|Cheetah|Leopard|Wolf|Fox|Deer|Moose|Eagle|Hawk|Owl|Parrot|Dolphin|Whale|Shark|Turtle|Snake|Crocodile|Alligator|Frog|Toad|Fish|Salmon|Trout|Bass|Butterfly|Bee|Ant|Spider)\\b', text)\n",
        "            extracted[\"SPECIES\"].extend(common_animals)\n",
        "\n",
        "        # Animal groups\n",
        "        if \"ANIMAL_GROUP\" in entity_types:\n",
        "            groups = re.findall(r'\\b(?:herd|flock|pack|pod|school|colony|pride|swarm|murder|gaggle|hive|brood|drift)\\s+of\\s+\\w+\\b', text, re.IGNORECASE)\n",
        "            extracted[\"ANIMAL_GROUP\"].extend(groups)\n",
        "\n",
        "        # Body parts\n",
        "        if \"ANIMAL_BODY_PART\" in entity_types:\n",
        "            body_parts = re.findall(r'\\b(?:paw|claw|hoof|beak|wing|fin|tail|horn|tusk|scale|feather|fur|mane|whisker|snout|bill|gills|tentacle|antenna)s?\\b', text, re.IGNORECASE)\n",
        "            extracted[\"ANIMAL_BODY_PART\"].extend(body_parts)\n",
        "\n",
        "        # Measurements\n",
        "        if \"MEASUREMENT\" in entity_types:\n",
        "            measurements = re.findall(r'\\b\\d+\\.?\\d*\\s*(?:mm|cm|m|kg|g|ml|l)\\b', text)\n",
        "            extracted[\"MEASUREMENT\"].extend(measurements)\n",
        "\n",
        "        # Length measurements\n",
        "        if \"LENGTH\" in entity_types:\n",
        "            length_patterns = re.findall(r'(?:fork|total|standard|body|tail)?\\s*length\\s*(?:of)?\\s*\\d+\\.?\\d*\\s*(?:mm|cm|m)', text, re.IGNORECASE)\n",
        "            extracted[\"LENGTH\"].extend(length_patterns)\n",
        "\n",
        "        # Habitats\n",
        "        if \"HABITAT\" in entity_types:\n",
        "            habitats = re.findall(r'\\b(?:forest|jungle|savanna|ocean|lake|river|mountain|desert|tundra|reef|farm|pasture|pen|enclosure|cage|burrow|nest|den|wetland|bog|marsh)\\b', text, re.IGNORECASE)\n",
        "            extracted[\"HABITAT\"].extend(habitats)\n",
        "\n",
        "        # Remove duplicates and sort\n",
        "        for entity_type in entity_types:\n",
        "            extracted[entity_type] = sorted(list(set([e.strip() for e in extracted[entity_type] if e.strip()])))\n",
        "            logger.info(f\"Found {len(extracted[entity_type])} unique {entity_type} entities with BioBERT\")\n",
        "\n",
        "        return extracted\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting entities with BioBERT: {str(e)}\", exc_info=True)\n",
        "        return f\"Error extracting entities with BioBERT: {str(e)}\"\n",
        "\n",
        "# Function to extract entities with better error handling and performance\n",
        "def extract_entities(text, entity_types, model_name):\n",
        "    if isinstance(text, str) and not text.startswith(\"Error\"):\n",
        "        try:\n",
        "            # Special handling for BioBERT model\n",
        "            if model_name == \"BioBERT\":\n",
        "                return extract_entities_biobert(text, entity_types)\n",
        "\n",
        "            # Get the specified SpaCy model\n",
        "            nlp = load_nlp_model(model_name)\n",
        "            if nlp is None:\n",
        "                return f\"Could not load NLP model: {model_name}\"\n",
        "            if nlp == \"transformer\":\n",
        "                return extract_entities_biobert(text, entity_types)\n",
        "            if nlp == \"attention\":\n",
        "                # Attention models are not used for entity extraction directly\n",
        "                # but instead for relationship extraction\n",
        "                logger.warning(f\"{model_name} is an attention model, not used for entity extraction\")\n",
        "                return {entity_type: [] for entity_type in entity_types}\n",
        "\n",
        "            # Limit text size for SpaCy models to avoid memory issues\n",
        "            if len(text) > 100000:\n",
        "                logger.warning(f\"Text is very large ({len(text)} chars). Limiting to first 100k characters.\")\n",
        "                text = text[:100000]  # Limit to first 100k characters\n",
        "\n",
        "            logger.info(f\"Processing text with {model_name} ({len(text)} chars)\")\n",
        "            doc = nlp(text)\n",
        "\n",
        "            # Create a dictionary to store extracted entities\n",
        "            extracted = {entity_type: [] for entity_type in entity_types}\n",
        "\n",
        "            # Extract required entity types\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_ in entity_types:\n",
        "                    extracted[ent.label_].append(ent.text)\n",
        "\n",
        "            # Special case for animal scientific species names\n",
        "            if \"SPECIES\" in entity_types:\n",
        "                logger.info(\"Extracting scientific species names with regex\")\n",
        "\n",
        "                # Scientific names pattern (Genus species)\n",
        "                scientific_names = re.findall(r'\\b[A-Z][a-z]+\\s+[a-z]+\\b', text)\n",
        "                extracted[\"SPECIES\"].extend(scientific_names)\n",
        "\n",
        "                # Additional check for common animal names (with capitalized nouns)\n",
        "                common_animals = re.findall(r'\\b(?:African|Asian|American|European|Australian|Western|Eastern|Northern|Southern)?\\s*(?:Lion|Tiger|Bear|Elephant|Giraffe|Rhinoceros|Zebra|Cheetah|Leopard|Wolf|Fox|Deer|Moose|Eagle|Hawk|Owl|Parrot|Dolphin|Whale|Shark|Turtle|Snake|Crocodile|Alligator|Frog|Toad|Fish|Salmon|Trout|Bass|Butterfly|Bee|Ant|Spider)\\b', text)\n",
        "                extracted[\"SPECIES\"].extend(common_animals)\n",
        "\n",
        "            # Special case for animal groups\n",
        "            if \"ANIMAL_GROUP\" in entity_types:\n",
        "                logger.info(\"Extracting animal group terms with regex\")\n",
        "                groups = re.findall(r'\\b(?:herd|flock|pack|pod|school|colony|pride|swarm|murder|gaggle|hive|brood|drift)\\s+of\\s+\\w+\\b', text, re.IGNORECASE)\n",
        "                extracted[\"ANIMAL_GROUP\"].extend(groups)\n",
        "\n",
        "            # Special case for body parts\n",
        "            if \"ANIMAL_BODY_PART\" in entity_types:\n",
        "                logger.info(\"Extracting animal body parts with regex\")\n",
        "                body_parts = re.findall(r'\\b(?:paw|claw|hoof|beak|wing|fin|tail|horn|tusk|scale|feather|fur|mane|whisker|snout|bill|gills|tentacle|antenna)s?\\b', text, re.IGNORECASE)\n",
        "                extracted[\"ANIMAL_BODY_PART\"].extend(body_parts)\n",
        "\n",
        "            # Special case for measurements\n",
        "            if \"MEASUREMENT\" in entity_types:\n",
        "                logger.info(\"Extracting measurements with regex\")\n",
        "                measurements = re.findall(r'\\b\\d+\\.?\\d*\\s*(?:mm|cm|m|kg|g|ml|l)\\b', text)\n",
        "                extracted[\"MEASUREMENT\"].extend(measurements)\n",
        "\n",
        "            # Special case for length measurements\n",
        "            if \"LENGTH\" in entity_types:\n",
        "                logger.info(\"Extracting length measurements with regex\")\n",
        "                length_patterns = re.findall(r'(?:fork|total|standard|body|tail)?\\s*length\\s*(?:of)?\\s*\\d+\\.?\\d*\\s*(?:mm|cm|m)', text, re.IGNORECASE)\n",
        "                extracted[\"LENGTH\"].extend(length_patterns)\n",
        "\n",
        "            # Special case for habitats\n",
        "            if \"HABITAT\" in entity_types:\n",
        "                logger.info(\"Extracting habitat terms with regex\")\n",
        "                habitats = re.findall(r'\\b(?:forest|jungle|savanna|ocean|lake|river|mountain|desert|tundra|reef|farm|pasture|pen|enclosure|cage|burrow|nest|den|wetland|bog|marsh)\\b', text, re.IGNORECASE)\n",
        "                extracted[\"HABITAT\"].extend(habitats)\n",
        "\n",
        "            # Remove duplicates and sort\n",
        "            for entity_type in entity_types:\n",
        "                extracted[entity_type] = sorted(list(set([e.strip() for e in extracted[entity_type] if e.strip()])))\n",
        "                logger.info(f\"Found {len(extracted[entity_type])} unique {entity_type} entities with {model_name}\")\n",
        "\n",
        "            return extracted\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting entities with {model_name}: {str(e)}\", exc_info=True)\n",
        "            return f\"Error extracting entities with {model_name}: {str(e)}\"\n",
        "    else:\n",
        "        return f\"Cannot extract entities: {text}\"\n",
        "\n",
        "# Function to generate comparison visualization\n",
        "def generate_comparison_chart(extracted_by_model, entity_types, models, output_path=None):\n",
        "    try:\n",
        "        if output_path is None:\n",
        "            temp_dir = tempfile.mkdtemp()\n",
        "            chart_file = os.path.join(temp_dir, \"animal_comparison_chart.png\")\n",
        "        else:\n",
        "            chart_file = output_path\n",
        "\n",
        "        # Set up the plot\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "        # Prepare data\n",
        "        x = np.arange(len(entity_types))\n",
        "        width = 0.8 / len(models)\n",
        "\n",
        "        # Define a color palette\n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
        "\n",
        "        # Plot bars for each model\n",
        "        for i, model in enumerate(models):\n",
        "            if model in extracted_by_model:\n",
        "                counts = [len(extracted_by_model[model].get(entity_type, [])) for entity_type in entity_types]\n",
        "                offset = width * i - width * (len(models) - 1) / 2\n",
        "                ax.bar(x + offset, counts, width, label=model, color=colors[i % len(colors)])\n",
        "\n",
        "        # Add labels and title\n",
        "        ax.set_ylabel('Number of Entities', fontsize=12)\n",
        "        ax.set_title('Comparison of Entity Extraction Between Models', fontsize=14, fontweight='bold')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(entity_types, rotation=45, ha='right', fontsize=10)\n",
        "        ax.legend(loc='best')\n",
        "\n",
        "        # Add value labels on top of each bar\n",
        "        for i, model in enumerate(models):\n",
        "            if model in extracted_by_model:\n",
        "                counts = [len(extracted_by_model[model].get(entity_type, [])) for entity_type in entity_types]\n",
        "                offset = width * i - width * (len(models) - 1) / 2\n",
        "                for j, count in enumerate(counts):\n",
        "                    ax.text(x[j] + offset, count + 0.5, str(count), ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "        # Add a grid for better readability\n",
        "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Improve layout\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(chart_file, dpi=300, bbox_inches='tight')\n",
        "\n",
        "        # Display in notebook if in notebook environment\n",
        "        try:\n",
        "            from IPython import get_ipython\n",
        "            if get_ipython() is not None:\n",
        "                plt.show()\n",
        "        except (ImportError, NameError):\n",
        "            pass\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "        return chart_file\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating comparison chart: {str(e)}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "# NEW FUNCTION: Extract entity relationships using pattern-based approach\n",
        "def extract_entity_relationships(text, extracted_entities, nlp_model):\n",
        "    \"\"\"\n",
        "    Extract relationships between different entities using dependency parsing and co-occurrence.\n",
        "\n",
        "    Args:\n",
        "        text: The text content from the PDF\n",
        "        extracted_entities: Dictionary of extracted entities by type\n",
        "        nlp_model: The loaded SpaCy NLP model (must be SpaCy, not BioBERT)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of relationship types and their instances\n",
        "    \"\"\"\n",
        "    if isinstance(nlp_model, str):\n",
        "        return {\"error\": [\"Relationship extraction requires a SpaCy model\"]}\n",
        "\n",
        "    relationships = {\n",
        "        \"species_habitat\": [],       # Which species live in which habitats\n",
        "        \"species_body_part\": [],     # Which body parts belong to which species\n",
        "        \"species_measurement\": [],   # Measurements associated with species\n",
        "        \"species_group\": []          # Species associated with animal groups\n",
        "    }\n",
        "\n",
        "    # Flatten entity lists into entity-type mappings for fast lookup\n",
        "    entity_map = {}\n",
        "    for entity_type, entities in extracted_entities.items():\n",
        "        for entity in entities:\n",
        "            entity_map[entity.lower()] = entity_type\n",
        "\n",
        "    # Process text in manageable chunks to avoid memory issues\n",
        "    max_chunk_size = 5000\n",
        "    chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
        "\n",
        "    logger.info(f\"Processing {len(chunks)} chunks for entity relationships\")\n",
        "\n",
        "    # Process each chunk\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        if i % 10 == 0:\n",
        "            logger.info(f\"Processing relationship chunk {i+1}/{len(chunks)}\")\n",
        "\n",
        "        if not chunk.strip():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Process with spaCy\n",
        "            doc = nlp_model(chunk)\n",
        "\n",
        "            # Extract sentences\n",
        "            sentences = list(doc.sents)\n",
        "\n",
        "            # Analyze each sentence for entity relationships\n",
        "            for sentence in sentences:\n",
        "                sentence_text = sentence.text.lower()\n",
        "\n",
        "                # Find entities in this sentence\n",
        "                sentence_entities = {}\n",
        "                for entity, entity_type in entity_map.items():\n",
        "                    if entity.lower() in sentence_text:\n",
        "                        start = sentence_text.find(entity.lower())\n",
        "                        if start >= 0:\n",
        "                            sentence_entities[(start, start + len(entity))] = (entity, entity_type)\n",
        "\n",
        "                # If we have multiple entity types in the sentence, check for relationships\n",
        "                entity_types_in_sent = set(et for _, et in sentence_entities.values())\n",
        "\n",
        "                # Species and habitat relationship\n",
        "                if \"SPECIES\" in entity_types_in_sent and \"HABITAT\" in entity_types_in_sent:\n",
        "                    # Look for habitat relationship patterns\n",
        "                    habitat_patterns = [\n",
        "                        \"found in\", \"lives in\", \"inhabits\", \"native to\", \"occurs in\",\n",
        "                        \"present in\", \"located in\", \"resides in\", \"dwells in\", \"occupies\"\n",
        "                    ]\n",
        "\n",
        "                    for pattern in habitat_patterns:\n",
        "                        if pattern in sentence_text:\n",
        "                            # Find closest species and habitat entities\n",
        "                            pattern_pos = sentence_text.find(pattern)\n",
        "                            species_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                               in sentence_entities.items()\n",
        "                                               if entity_type == \"SPECIES\"]\n",
        "                            habitat_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                              in sentence_entities.items()\n",
        "                                              if entity_type == \"HABITAT\"]\n",
        "\n",
        "                            if species_entities and habitat_entities:\n",
        "                                # Find closest species before the pattern\n",
        "                                species_before = [(abs(pattern_pos - (pos + len(entity[0]))), entity[0])\n",
        "                                               for pos, entity in species_entities\n",
        "                                               if pos < pattern_pos]\n",
        "\n",
        "                                # Find closest habitat after the pattern\n",
        "                                habitat_after = [(abs(pos - (pattern_pos + len(pattern))), entity[0])\n",
        "                                              for pos, entity in habitat_entities\n",
        "                                              if pos > pattern_pos]\n",
        "\n",
        "                                if species_before and habitat_after:\n",
        "                                    species = min(species_before, key=lambda x: x[0])[1]\n",
        "                                    habitat = min(habitat_after, key=lambda x: x[0])[1]\n",
        "                                    relationship = (species, habitat, sentence.text)\n",
        "                                    if relationship not in relationships[\"species_habitat\"]:\n",
        "                                        relationships[\"species_habitat\"].append(relationship)\n",
        "\n",
        "                # Species and body part relationship\n",
        "                if \"SPECIES\" in entity_types_in_sent and \"ANIMAL_BODY_PART\" in entity_types_in_sent:\n",
        "                    # Find possession patterns\n",
        "                    possession_patterns = [\n",
        "                        \"has\", \"with\", \"possesses\", \"featuring\", \"characterized by\",\n",
        "                        \"'s\", \"of the\", \"of its\", \"their\", \"its\"\n",
        "                    ]\n",
        "\n",
        "                    for pattern in possession_patterns:\n",
        "                        if pattern in sentence_text:\n",
        "                            pattern_pos = sentence_text.find(pattern)\n",
        "                            species_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                               in sentence_entities.items()\n",
        "                                               if entity_type == \"SPECIES\"]\n",
        "                            body_part_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                                in sentence_entities.items()\n",
        "                                                if entity_type == \"ANIMAL_BODY_PART\"]\n",
        "\n",
        "                            if species_entities and body_part_entities:\n",
        "                                species_before = [(abs(pattern_pos - (pos + len(entity[0]))), entity[0])\n",
        "                                               for pos, entity in species_entities\n",
        "                                               if pos < pattern_pos]\n",
        "\n",
        "                                body_part_after = [(abs(pos - (pattern_pos + len(pattern))), entity[0])\n",
        "                                              for pos, entity in body_part_entities\n",
        "                                              if pos > pattern_pos]\n",
        "\n",
        "                                if species_before and body_part_after:\n",
        "                                    species = min(species_before, key=lambda x: x[0])[1]\n",
        "                                    body_part = min(body_part_after, key=lambda x: x[0])[1]\n",
        "                                    relationship = (species, body_part, sentence.text)\n",
        "                                    if relationship not in relationships[\"species_body_part\"]:\n",
        "                                        relationships[\"species_body_part\"].append(relationship)\n",
        "\n",
        "                # Species and measurements\n",
        "                if \"SPECIES\" in entity_types_in_sent and (\"MEASUREMENT\" in entity_types_in_sent or \"LENGTH\" in entity_types_in_sent):\n",
        "                    # Look for measurement patterns\n",
        "                    measurement_patterns = [\n",
        "                        \"measures\", \"weighs\", \"length of\", \"size of\", \"weight of\",\n",
        "                        \"is about\", \"approximately\", \"typically\", \"averages\", \"ranging from\"\n",
        "                    ]\n",
        "\n",
        "                    measurement_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                           in sentence_entities.items()\n",
        "                                           if entity_type in [\"MEASUREMENT\", \"LENGTH\"]]\n",
        "\n",
        "                    species_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                       in sentence_entities.items()\n",
        "                                       if entity_type == \"SPECIES\"]\n",
        "\n",
        "                    if species_entities and measurement_entities:\n",
        "                        for species_pos, species_entity in species_entities:\n",
        "                            for measurement_pos, measurement_entity in measurement_entities:\n",
        "                                # Check if they're close to each other (within 50 characters)\n",
        "                                if abs(species_pos - measurement_pos) < 50:\n",
        "                                    relationship = (species_entity[0], measurement_entity[0], sentence.text)\n",
        "                                    if relationship not in relationships[\"species_measurement\"]:\n",
        "                                        relationships[\"species_measurement\"].append(relationship)\n",
        "\n",
        "                # Species and group relationships\n",
        "                if \"SPECIES\" in entity_types_in_sent and \"ANIMAL_GROUP\" in entity_types_in_sent:\n",
        "                    group_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                     in sentence_entities.items()\n",
        "                                     if entity_type == \"ANIMAL_GROUP\"]\n",
        "\n",
        "                    species_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                       in sentence_entities.items()\n",
        "                                       if entity_type == \"SPECIES\"]\n",
        "\n",
        "                    if species_entities and group_entities:\n",
        "                        for group_pos, group_entity in group_entities:\n",
        "                            # Group terms usually have \"of X\" structure, check for the species after \"of\"\n",
        "                            if \"of \" in group_entity[0]:\n",
        "                                group_text = group_entity[0]\n",
        "                                after_of = group_text.split(\"of \")[1].strip().lower()\n",
        "\n",
        "                                # Check if any species appears in or matches the group description\n",
        "                                for _, species_entity in species_entities:\n",
        "                                    species_text = species_entity[0].lower()\n",
        "                                    if species_text in after_of or after_of in species_text:\n",
        "                                        relationship = (species_entity[0], group_entity[0], sentence.text)\n",
        "                                        if relationship not in relationships[\"species_group\"]:\n",
        "                                            relationships[\"species_group\"].append(relationship)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error processing chunk {i} for relationships: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Log results\n",
        "    for relation_type, relations in relationships.items():\n",
        "        logger.info(f\"Found {len(relations)} {relation_type} relationships\")\n",
        "\n",
        "    return relationships\n",
        "\n",
        "# NEW FUNCTION: Extract entity relationships with transformer attention\n",
        "def extract_entity_relationships_with_attention(text, extracted_entities, model_name=\"BERT-base\"):\n",
        "    \"\"\"\n",
        "    Extract relationships between entities using transformer attention mechanisms.\n",
        "\n",
        "    Args:\n",
        "        text: The text content from the PDF\n",
        "        extracted_entities: Dictionary of extracted entities by type\n",
        "        model_name: Pretrained transformer model to use\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of relationship types and their instances with attention scores\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if model_name not in attention_models:\n",
        "            # Load the model if not already loaded\n",
        "            load_nlp_model(model_name)\n",
        "\n",
        "            if model_name not in attention_models:\n",
        "                return {\"error\": [f\"Could not load attention model: {model_name}\"]}\n",
        "\n",
        "        tokenizer = attention_models[model_name][\"tokenizer\"]\n",
        "        model = attention_models[model_name][\"model\"]\n",
        "\n",
        "        # Prepare relationships dictionary\n",
        "        relationships = {\n",
        "            \"species_habitat\": [],\n",
        "            \"species_body_part\": [],\n",
        "            \"species_measurement\": [],\n",
        "            \"species_group\": []\n",
        "        }\n",
        "\n",
        "        # Prepare entity mapping for lookup\n",
        "        entity_spans = []\n",
        "        entity_types = []\n",
        "\n",
        "        for entity_type, entities in extracted_entities.items():\n",
        "            for entity in entities:\n",
        "                entity_spans.append((entity, entity_type))\n",
        "\n",
        "        # Process text in chunks that fit within model's max context\n",
        "        max_length = tokenizer.model_max_length\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "        logger.info(f\"Processing {len(sentences)} sentences with attention model\")\n",
        "\n",
        "        # Process sentences to find relationships\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            if i % 100 == 0:\n",
        "                logger.info(f\"Processing sentence {i+1}/{len(sentences)} with attention\")\n",
        "\n",
        "            if not sentence.strip() or len(sentence) > max_length:\n",
        "                continue\n",
        "\n",
        "            # Find entities in this sentence\n",
        "            entities_in_sentence = []\n",
        "            for entity, entity_type in entity_spans:\n",
        "                if entity.lower() in sentence.lower():\n",
        "                    entities_in_sentence.append((entity, entity_type))\n",
        "\n",
        "            # Skip if less than 2 entities or no species in the sentence\n",
        "            if len(entities_in_sentence) < 2:\n",
        "                continue\n",
        "\n",
        "            has_species = any(et == \"SPECIES\" for _, et in entities_in_sentence)\n",
        "            if not has_species:\n",
        "                continue\n",
        "\n",
        "            # Tokenize sentence\n",
        "            inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "            # Get model outputs with attention weights\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            # Extract attention matrices from the last layer\n",
        "            # Shape: [batch_size, num_heads, seq_length, seq_length]\n",
        "            attention = outputs.attentions[-1].mean(dim=1).squeeze(0)  # Average over attention heads\n",
        "\n",
        "            # Map tokens back to entities\n",
        "            token_to_entity = {}\n",
        "            for entity, entity_type in entities_in_sentence:\n",
        "                entity_tokens = tokenizer.encode(entity, add_special_tokens=False)\n",
        "\n",
        "                # Find the entity tokens in the sentence tokens\n",
        "                sentence_tokens = inputs.input_ids[0].tolist()\n",
        "                for i in range(len(sentence_tokens) - len(entity_tokens) + 1):\n",
        "                    if sentence_tokens[i:i+len(entity_tokens)] == entity_tokens:\n",
        "                        for j in range(i, i+len(entity_tokens)):\n",
        "                            token_to_entity[j] = (entity, entity_type)\n",
        "\n",
        "            # Build entity attention matrix\n",
        "            entity_attention = {}\n",
        "            for i in range(attention.shape[0]):\n",
        "                if i in token_to_entity:\n",
        "                    entity_i, type_i = token_to_entity[i]\n",
        "                    for j in range(attention.shape[1]):\n",
        "                        if j in token_to_entity and i != j:\n",
        "                            entity_j, type_j = token_to_entity[j]\n",
        "\n",
        "                            # Skip if same entity\n",
        "                            if entity_i == entity_j:\n",
        "                                continue\n",
        "\n",
        "                            # Get attention score\n",
        "                            att_score = float(attention[i, j])\n",
        "\n",
        "                            key = ((entity_i, type_i), (entity_j, type_j))\n",
        "                            if key not in entity_attention or att_score > entity_attention[key][0]:\n",
        "                                entity_attention[key] = (att_score, sentence)\n",
        "\n",
        "            # Extract relationships based on entity types and attention scores\n",
        "            for ((entity1, type1), (entity2, type2)), (score, context) in entity_attention.items():\n",
        "                # Only consider high attention scores (threshold can be adjusted)\n",
        "                if score < 0.1:\n",
        "                    continue\n",
        "\n",
        "                # Define relationships based on entity types and attention score\n",
        "                if type1 == \"SPECIES\" and type2 == \"HABITAT\" and score > 0.3:\n",
        "                    relationship = (entity1, entity2, context, score)\n",
        "                    relationships[\"species_habitat\"].append(relationship)\n",
        "\n",
        "                elif type1 == \"SPECIES\" and type2 == \"ANIMAL_BODY_PART\" and score > 0.4:\n",
        "                    relationship = (entity1, entity2, context, score)\n",
        "                    relationships[\"species_body_part\"].append(relationship)\n",
        "\n",
        "                elif type1 == \"SPECIES\" and type2 in [\"MEASUREMENT\", \"LENGTH\"] and score > 0.3:\n",
        "                    relationship = (entity1, entity2, context, score)\n",
        "                    relationships[\"species_measurement\"].append(relationship)\n",
        "\n",
        "                elif type1 == \"SPECIES\" and type2 == \"ANIMAL_GROUP\" and score > 0.3:\n",
        "                    relationship = (entity1, entity2, context, score)\n",
        "                    relationships[\"species_group\"].append(relationship)\n",
        "\n",
        "                # Check reverse direction for asymmetric relationships\n",
        "                elif type2 == \"SPECIES\" and type1 == \"HABITAT\" and score > 0.3:\n",
        "                    relationship = (entity2, entity1, context, score)\n",
        "                    relationships[\"species_habitat\"].append(relationship)\n",
        "\n",
        "                elif type2 == \"SPECIES\" and type1 == \"ANIMAL_BODY_PART\" and score > 0.4:\n",
        "                    relationship = (entity2, entity1, context, score)\n",
        "                    relationships[\"species_body_part\"].append(relationship)\n",
        "\n",
        "                elif type2 == \"SPECIES\" and type1 in [\"MEASUREMENT\", \"LENGTH\"] and score > 0.3:\n",
        "                    relationship = (entity2, entity1, context, score)\n",
        "                    relationships[\"species_measurement\"].append(relationship)\n",
        "\n",
        "                elif type2 == \"SPECIES\" and type1 == \"ANIMAL_GROUP\" and score > 0.3:\n",
        "                    relationship = (entity2, entity1, context, score)\n",
        "                    relationships[\"species_group\"].append(relationship)\n",
        "\n",
        "        # Remove duplicates and sort by score\n",
        "        for relation_type in relationships:\n",
        "            unique_relations = {}\n",
        "            for entity1, entity2, context, score in relationships[relation_type]:\n",
        "                key = (entity1, entity2)\n",
        "                if key not in unique_relations or score > unique_relations[key][2]:\n",
        "                    unique_relations[key] = (entity1, entity2, score, context)\n",
        "\n",
        "            # Sort by attention score\n",
        "            relationships[relation_type] = [(e1, e2, ctx, score) for e1, e2, score, ctx in sorted(\n",
        "                unique_relations.values(), key=lambda x: x[2], reverse=True\n",
        "            )]\n",
        "\n",
        "            logger.info(f\"Found {len(relationships[relation_type])} unique {relation_type} relationships with attention\")\n",
        "\n",
        "        return relationships\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting relationships with attention: {str(e)}\", exc_info=True)\n",
        "        return {\"error\": [f\"Error extracting relationships with attention: {str(e)}\"]}\n",
        "\n",
        "# Function to create a relationship graph\n",
        "def create_relationship_graph(relationships, entity_types, min_weight=1, use_attention_scores=False):\n",
        "    \"\"\"\n",
        "    Create a NetworkX graph representing entity relationships.\n",
        "\n",
        "    Args:\n",
        "        relationships: Dictionary of relationship types and their instances\n",
        "        entity_types: List of entity types to include\n",
        "        min_weight: Minimum weight to include an edge\n",
        "        use_attention_scores: Whether to use attention scores for edge weights\n",
        "\n",
        "    Returns:\n",
        "        NetworkX graph\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes with entity type information\n",
        "    entity_nodes = set()\n",
        "\n",
        "    # Process each relationship type\n",
        "    for relation_type, relations in relationships.items():\n",
        "        for relation in relations:\n",
        "            if use_attention_scores and len(relation) == 4:\n",
        "                entity1, entity2, context, score = relation\n",
        "                weight = score  # Use attention score as weight\n",
        "            else:\n",
        "                if len(relation) == 3:\n",
        "                    entity1, entity2, context = relation\n",
        "                    weight = 1  # Default weight\n",
        "                else:\n",
        "                    continue  # Skip invalid relations\n",
        "\n",
        "            # Determine entity types\n",
        "            entity1_type = None\n",
        "            entity2_type = None\n",
        "\n",
        "            if relation_type == \"species_habitat\":\n",
        "                entity1_type = \"SPECIES\"\n",
        "                entity2_type = \"HABITAT\"\n",
        "            elif relation_type == \"species_body_part\":\n",
        "                entity1_type = \"SPECIES\"\n",
        "                entity2_type = \"ANIMAL_BODY_PART\"\n",
        "            elif relation_type == \"species_measurement\":\n",
        "                entity1_type = \"SPECIES\"\n",
        "                entity2_type = \"MEASUREMENT\"\n",
        "            elif relation_type == \"species_group\":\n",
        "                entity1_type = \"SPECIES\"\n",
        "                entity2_type = \"ANIMAL_GROUP\"\n",
        "\n",
        "            # Add nodes if they don't exist\n",
        "            if entity1 not in entity_nodes:\n",
        "                G.add_node(entity1, type=entity1_type)\n",
        "                entity_nodes.add(entity1)\n",
        "\n",
        "            if entity2 not in entity_nodes:\n",
        "                G.add_node(entity2, type=entity2_type)\n",
        "                entity_nodes.add(entity2)\n",
        "\n",
        "            # Add or update edge\n",
        "            if G.has_edge(entity1, entity2):\n",
        "                if use_attention_scores:\n",
        "                    # Use maximum attention score if multiple relations exist\n",
        "                    G[entity1][entity2]['weight'] = max(G[entity1][entity2]['weight'], weight)\n",
        "                else:\n",
        "                    G[entity1][entity2]['weight'] += 1\n",
        "\n",
        "                G[entity1][entity2]['examples'].append(context)\n",
        "                G[entity1][entity2]['types'].add(relation_type)\n",
        "            else:\n",
        "                G.add_edge(\n",
        "                    entity1,\n",
        "                    entity2,\n",
        "                    weight=weight,\n",
        "                    examples=[context],\n",
        "                    types={relation_type}\n",
        "                )\n",
        "\n",
        "    # Remove edges below the minimum weight\n",
        "    edges_to_remove = [(u, v) for u, v, d in G.edges(data=True) if d['weight'] < min_weight]\n",
        "    G.remove_edges_from(edges_to_remove)\n",
        "\n",
        "    # Remove isolated nodes\n",
        "    G.remove_nodes_from(list(nx.isolates(G)))\n",
        "\n",
        "    return G\n",
        "\n",
        "# Function to visualize entity relationships\n",
        "def visualize_entity_relationships(G, output_path=None, max_nodes=50, title=\"Entity Relationship Network\"):\n",
        "    \"\"\"\n",
        "    Create a visualization of entity relationships using NetworkX.\n",
        "\n",
        "    Args:\n",
        "        G: NetworkX graph of entity relationships\n",
        "        output_path: Path to save the visualization\n",
        "        max_nodes: Maximum number of nodes to include in the visualization\n",
        "        title: Title for the visualization\n",
        "\n",
        "    Returns:\n",
        "        Path to the saved visualization\n",
        "    \"\"\"\n",
        "    if output_path is None:\n",
        "        temp_dir = tempfile.mkdtemp()\n",
        "        output_file = os.path.join(temp_dir, \"entity_relationships.png\")\n",
        "    else:\n",
        "        output_file = output_path\n",
        "\n",
        "    # If graph is too large, take a subgraph\n",
        "    if len(G.nodes()) > max_nodes:\n",
        "        # Sort edges by weight\n",
        "        edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)\n",
        "\n",
        "        # Take top N edges\n",
        "        top_edges = edges[:max_nodes]\n",
        "\n",
        "        # Create subgraph\n",
        "        nodes = set()\n",
        "        for u, v, _ in top_edges:\n",
        "            nodes.add(u)\n",
        "            nodes.add(v)\n",
        "\n",
        "        G = G.subgraph(nodes)\n",
        "\n",
        "    # Set up plot\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Define node colors by type\n",
        "    color_map = {\n",
        "        'SPECIES': '#1f77b4',       # Blue\n",
        "        'HABITAT': '#2ca02c',       # Green\n",
        "        'ANIMAL_BODY_PART': '#d62728',  # Red\n",
        "        'MEASUREMENT': '#9467bd',   # Purple\n",
        "        'LENGTH': '#8c564b',        # Brown\n",
        "        'ANIMAL_GROUP': '#e377c2'   # Pink\n",
        "    }\n",
        "\n",
        "    # Get node colors\n",
        "    node_colors = [color_map.get(G.nodes[node]['type'], '#7f7f7f') for node in G.nodes()]\n",
        "\n",
        "    # Get edge weights for line thickness\n",
        "    edge_weights = [G[u][v]['weight'] * 2 for u, v in G.edges()]\n",
        "\n",
        "    # Create position layout\n",
        "    pos = nx.spring_layout(G, k=0.3, iterations=50, seed=42)\n",
        "\n",
        "    # Draw the graph\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=800, node_color=node_colors, alpha=0.8)\n",
        "\n",
        "    nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.6, edge_color='gray')\n",
        "\n",
        "    # Draw labels with smaller font size for better readability\n",
        "    nx.draw_networkx_labels(G, pos, font_size=8, font_family='sans-serif')\n",
        "\n",
        "    # Create legend for node types\n",
        "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w',\n",
        "                                 label=entity_type,\n",
        "                                 markerfacecolor=color, markersize=10)\n",
        "                      for entity_type, color in color_map.items()]\n",
        "\n",
        "    plt.legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save figure\n",
        "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Display in notebook if in notebook environment\n",
        "    try:\n",
        "        from IPython import get_ipython\n",
        "        if get_ipython() is not None:\n",
        "            plt.show()\n",
        "    except (ImportError, NameError):\n",
        "        pass\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "    return output_file\n",
        "\n",
        "# Function to create a heatmap visualization of entity co-occurrences\n",
        "def visualize_cooccurrence_matrix(cooccurrence_df, output_path=None, max_items=50):\n",
        "    \"\"\"\n",
        "    Create a heatmap visualization of entity co-occurrences.\n",
        "\n",
        "    Args:\n",
        "        cooccurrence_df: DataFrame with co-occurrence counts\n",
        "        output_path: Path to save the visualization\n",
        "        max_items: Maximum number of items to include in the visualization\n",
        "\n",
        "    Returns:\n",
        "        Path to the saved visualization\n",
        "    \"\"\"\n",
        "    if output_path is None:\n",
        "        temp_dir = tempfile.mkdtemp()\n",
        "        output_file = os.path.join(temp_dir, \"entity_cooccurrence.png\")\n",
        "    else:\n",
        "        output_file = output_path\n",
        "\n",
        "    # If matrix is too large, take a subset with highest values\n",
        "    if len(cooccurrence_df) > max_items:\n",
        "        # Get the sum of co-occurrences for each entity\n",
        "        row_sums = cooccurrence_df.sum(axis=1)\n",
        "\n",
        "        # Get the top entities\n",
        "        top_entities = row_sums.nlargest(max_items).index\n",
        "\n",
        "        # Filter the matrix\n",
        "        cooccurrence_df = cooccurrence_df.loc[top_entities, top_entities]\n",
        "\n",
        "    # Set up figure\n",
        "    plt.figure(figsize=(16, 14))\n",
        "\n",
        "    # Create a custom colormap from white to blue\n",
        "    colors = [(1, 1, 1), (0.12, 0.47, 0.71)]\n",
        "    cmap = LinearSegmentedColormap.from_list(\"custom_blue\", colors, N=100)\n",
        "\n",
        "    # Create heatmap\n",
        "    ax = sns.heatmap(cooccurrence_df, cmap=cmap, linewidths=0.5,\n",
        "                     linecolor='gray', square=True, cbar_kws={\"shrink\": 0.8})\n",
        "\n",
        "    # Rotate x-axis labels for better readability\n",
        "    plt.xticks(rotation=90, fontsize=8)\n",
        "    plt.yticks(fontsize=8)\n",
        "\n",
        "    plt.title(\"Entity Co-occurrence Matrix\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save figure\n",
        "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Display in notebook if in notebook environment\n",
        "    try:\n",
        "        from IPython import get_ipython\n",
        "        if get_ipython() is not None:\n",
        "            plt.show()\n",
        "    except (ImportError, NameError):\n",
        "        pass\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "    return output_file\n",
        "# Main function to run the animal entity extractor\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the enhanced animal entity extractor with relationship detection.\n",
        "    This function provides a command-line interface to extract entities and relationships\n",
        "    from PDF files using multiple NLP models and visualize the results.\n",
        "    \"\"\"\n",
        "    print(\"\\n===============================================\")\n",
        "    print(\"  PDF Animal Entity Extractor with Relationship Detection\")\n",
        "    print(\"===============================================\")\n",
        "    print(\"\\nThis tool extracts animal-related entities from PDF documents\")\n",
        "    print(\"and identifies relationships between entities using attention mechanisms.\")\n",
        "\n",
        "    # Print available models\n",
        "    available_models = get_available_models()\n",
        "    print(f\"\\nAvailable NLP models: {', '.join(available_models)}\")\n",
        "\n",
        "    # Check for transformers installation\n",
        "    try:\n",
        "        import transformers\n",
        "        has_transformers = True\n",
        "    except ImportError:\n",
        "        has_transformers = False\n",
        "\n",
        "    if not has_transformers and any(m in available_models for m in [\"BioBERT\", \"BERT-base\"]):\n",
        "        print(\"\\nWARNING: Transformer models are listed as available, but the 'transformers' package\")\n",
        "        print(\"is not installed. To use these models, install required packages:\")\n",
        "        print(\"  pip install transformers torch\")\n",
        "\n",
        "    if len(available_models) < 2:\n",
        "        print(\"\\nOnly one model is available. For comparison, install additional models.\")\n",
        "        install_model_message()\n",
        "\n",
        "    # Get PDF path\n",
        "    pdf_path = input(\"Enter the path to your PDF file: \")\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(\"File not found!\")\n",
        "        return\n",
        "\n",
        "    # Define animal-specific entity types\n",
        "    entity_types = [\"SPECIES\", \"ANIMAL_GROUP\", \"ANIMAL_BODY_PART\", \"MEASUREMENT\", \"LENGTH\", \"HABITAT\"]\n",
        "\n",
        "    # Let user select entity types\n",
        "    print(\"\\nAvailable entity types:\")\n",
        "    for i, entity_type in enumerate(entity_types):\n",
        "        print(f\"{i+1}. {entity_type}\")\n",
        "\n",
        "    entity_indices = input(\"\\nSelect entity types to extract (comma-separated numbers, e.g., 1,2,3 or 'all'): \")\n",
        "    selected_entity_types = []\n",
        "\n",
        "    if entity_indices.lower() == 'all':\n",
        "        selected_entity_types = entity_types\n",
        "    else:\n",
        "        try:\n",
        "            indices = [int(idx.strip()) for idx in entity_indices.split(\",\")]\n",
        "            for idx in indices:\n",
        "                if 1 <= idx <= len(entity_types):\n",
        "                    selected_entity_types.append(entity_types[idx-1])\n",
        "        except:\n",
        "            print(\"Invalid selection. Using all entity types.\")\n",
        "            selected_entity_types = entity_types\n",
        "\n",
        "    if not selected_entity_types:\n",
        "        selected_entity_types = entity_types\n",
        "\n",
        "    print(f\"Selected entity types: {', '.join(selected_entity_types)}\")\n",
        "\n",
        "    # Let user select models\n",
        "    print(\"\\nAvailable models:\")\n",
        "    for i, model in enumerate(available_models):\n",
        "        print(f\"{i+1}. {model}\")\n",
        "\n",
        "    model_indices = input(\"\\nSelect models (comma-separated numbers, e.g., 1,2 or 'all'): \")\n",
        "    selected_models = []\n",
        "\n",
        "    if model_indices.lower() == 'all':\n",
        "        selected_models = available_models\n",
        "    else:\n",
        "        try:\n",
        "            indices = [int(idx.strip()) for idx in model_indices.split(\",\")]\n",
        "            for idx in indices:\n",
        "                if 1 <= idx <= len(available_models):\n",
        "                    selected_models.append(available_models[idx-1])\n",
        "        except:\n",
        "            print(\"Invalid selection. Using the first available model.\")\n",
        "            selected_models = [available_models[0]]\n",
        "\n",
        "    if not selected_models:\n",
        "        selected_models = [available_models[0]]\n",
        "\n",
        "    # Ask about relationship detection\n",
        "    use_attention = False\n",
        "    extract_relationships = input(\"\\nExtract entity relationships? (y/n, default: y): \").lower() != 'n'\n",
        "\n",
        "    if extract_relationships:\n",
        "        # Add BERT model for attention if not already selected\n",
        "        if \"BERT-base\" not in selected_models and \"BERT-base\" in available_models and has_transformers:\n",
        "            print(\"\\nAdding BERT-base model for attention-based relationship extraction.\")\n",
        "            selected_models.append(\"BERT-base\")\n",
        "            use_attention = input(\"Use transformer attention for relationship detection? (y/n, default: y): \").lower() != 'n'\n",
        "        else:\n",
        "            print(\"\\nUsing pattern-based relationship extraction.\")\n",
        "\n",
        "    # Define output directory\n",
        "    output_dir = input(\"\\nEnter output directory (leave blank for current directory): \")\n",
        "    if not output_dir:\n",
        "        output_dir = \".\"\n",
        "\n",
        "    # Process the PDF with relationship detection\n",
        "    print(f\"\\nProcessing {pdf_path} with models: {', '.join(selected_models)}\")\n",
        "    print(f\"Extracting entity types: {', '.join(selected_entity_types)}\")\n",
        "\n",
        "    if extract_relationships:\n",
        "        print(\"Extracting entity relationships...\")\n",
        "        if use_attention:\n",
        "            print(\"Using transformer attention for relationship detection.\")\n",
        "\n",
        "        results, text_sample, files = process_pdf_with_relationships(\n",
        "            pdf_path, selected_entity_types, selected_models, output_dir, use_attention\n",
        "        )\n",
        "\n",
        "        # Print relationship visualization information\n",
        "        if files:\n",
        "            print(\"\\nVisualizations created:\")\n",
        "            for name, file_path in files.items():\n",
        "                print(f\"- {name}: {file_path}\")\n",
        "\n",
        "                # Attempt to display images in Jupyter notebook if running in one\n",
        "                try:\n",
        "                    from IPython.display import display, Image\n",
        "                    from IPython import get_ipython\n",
        "                    if get_ipython() is not None:\n",
        "                        if \"graph\" in name or \"matrix\" in name:\n",
        "                            display(Image(filename=file_path))\n",
        "                except (ImportError, NameError):\n",
        "                    pass\n",
        "    else:\n",
        "        # Just do entity extraction without relationships\n",
        "        results, text_sample, chart_file = process_pdf_multi_model(\n",
        "            pdf_path, selected_entity_types, selected_models, output_dir\n",
        "        )\n",
        "\n",
        "        if chart_file:\n",
        "            print(f\"\\nComparison chart saved to: {chart_file}\")\n",
        "\n",
        "            # Attempt to display in Jupyter notebook\n",
        "            try:\n",
        "                from IPython.display import display, Image\n",
        "                from IPython import get_ipython\n",
        "                if get_ipython() is not None:\n",
        "                    display(Image(filename=chart_file))\n",
        "            except (ImportError, NameError):\n",
        "                pass\n",
        "\n",
        "    # Save results to file\n",
        "    results_file = os.path.join(output_dir, \"animal_entity_analysis_results.md\")\n",
        "    try:\n",
        "        with open(results_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(results)\n",
        "        print(f\"\\nResults saved to: {results_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError saving results to file: {str(e)}\")\n",
        "\n",
        "    print(\"\\nAnalysis complete!\")\n",
        "\n",
        "    # Offer to open visualizations\n",
        "    try:\n",
        "        show_viz = input(\"\\nWould you like to open visualization files now? (y/n): \").lower() == 'y'\n",
        "        if show_viz:\n",
        "            import webbrowser\n",
        "            import platform\n",
        "\n",
        "            if extract_relationships and files:\n",
        "                for name, file_path in files.items():\n",
        "                    if os.path.exists(file_path):\n",
        "                        if platform.system() == 'Darwin':  # macOS\n",
        "                            os.system(f\"open {file_path}\")\n",
        "                        elif platform.system() == 'Windows':  # Windows\n",
        "                            os.system(f\"start {file_path}\")\n",
        "                        else:  # Linux\n",
        "                            os.system(f\"xdg-open {file_path}\")\n",
        "            elif not extract_relationships and chart_file and os.path.exists(chart_file):\n",
        "                if platform.system() == 'Darwin':  # macOS\n",
        "                    os.system(f\"open {chart_file}\")\n",
        "                elif platform.system() == 'Windows':  # Windows\n",
        "                    os.system(f\"start {chart_file}\")\n",
        "                else:  # Linux\n",
        "                    os.system(f\"xdg-open {chart_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error opening visualization files: {str(e)}\")\n",
        "\n",
        "    # Ask if user wants to run again\n",
        "    run_again = input(\"\\nWould you like to process another PDF? (y/n): \").lower() == 'y'\n",
        "    if run_again:\n",
        "        main()\n",
        "    else:\n",
        "        print(\"\\nThank you for using the Animal Entity Extractor!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GkaMY3PuC04C",
        "outputId": "3188733f-3598-4027-e7ba-b9d69dbc9ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "  PDF Animal Entity Extractor with Relationship Detection\n",
            "===============================================\n",
            "\n",
            "This tool extracts animal-related entities from PDF documents\n",
            "and identifies relationships between entities using attention mechanisms.\n",
            "\n",
            "Available NLP models: SpaCy Small, BioBERT, BERT-base\n",
            "Enter the path to your PDF file: /story_frogs.pdf\n",
            "\n",
            "Available entity types:\n",
            "1. SPECIES\n",
            "2. ANIMAL_GROUP\n",
            "3. ANIMAL_BODY_PART\n",
            "4. MEASUREMENT\n",
            "5. LENGTH\n",
            "6. HABITAT\n",
            "\n",
            "Select entity types to extract (comma-separated numbers, e.g., 1,2,3 or 'all'): \n",
            "Invalid selection. Using all entity types.\n",
            "Selected entity types: SPECIES, ANIMAL_GROUP, ANIMAL_BODY_PART, MEASUREMENT, LENGTH, HABITAT\n",
            "\n",
            "Available models:\n",
            "1. SpaCy Small\n",
            "2. BioBERT\n",
            "3. BERT-base\n",
            "\n",
            "Select models (comma-separated numbers, e.g., 1,2 or 'all'): \n",
            "Invalid selection. Using the first available model.\n",
            "\n",
            "Extract entity relationships? (y/n, default: y): \n",
            "\n",
            "Adding BERT-base model for attention-based relationship extraction.\n",
            "Use transformer attention for relationship detection? (y/n, default: y): \n",
            "\n",
            "Enter output directory (leave blank for current directory): \n",
            "\n",
            "Processing /story_frogs.pdf with models: SpaCy Small, BERT-base\n",
            "Extracting entity types: SPECIES, ANIMAL_GROUP, ANIMAL_BODY_PART, MEASUREMENT, LENGTH, HABITAT\n",
            "Extracting entity relationships...\n",
            "Using transformer attention for relationship detection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:BERT-base is an attention model, not used for entity extraction\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizations created:\n",
            "- entity_comparison: ./entity_extraction_comparison.png\n",
            "\n",
            "Results saved to: ./animal_entity_analysis_results.md\n",
            "\n",
            "Analysis complete!\n",
            "\n",
            "Would you like to open visualization files now? (y/n): \n",
            "\n",
            "Would you like to process another PDF? (y/n): \n",
            "\n",
            "Thank you for using the Animal Entity Extractor!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New imports for relationship extraction and visualization\n",
        "import networkx as nx\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "# New function to extract relationships between entities\n",
        "def extract_entity_relationships(text, extracted_entities, nlp_model):\n",
        "    \"\"\"\n",
        "    Extract relationships between different entities using dependency parsing and co-occurrence.\n",
        "\n",
        "    Args:\n",
        "        text: The text content from the PDF\n",
        "        extracted_entities: Dictionary of extracted entities by type\n",
        "        nlp_model: The loaded SpaCy NLP model (must be SpaCy, not BioBERT)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of relationship types and their instances\n",
        "    \"\"\"\n",
        "    if isinstance(nlp_model, str):\n",
        "        return {\"error\": [\"Relationship extraction requires a SpaCy model\"]}\n",
        "\n",
        "    relationships = {\n",
        "        \"species_habitat\": [],       # Which species live in which habitats\n",
        "        \"species_body_part\": [],     # Which body parts belong to which species\n",
        "        \"species_measurement\": [],   # Measurements associated with species\n",
        "        \"species_group\": []          # Species associated with animal groups\n",
        "    }\n",
        "\n",
        "    # Flatten entity lists into entity-type mappings for fast lookup\n",
        "    entity_map = {}\n",
        "    for entity_type, entities in extracted_entities.items():\n",
        "        for entity in entities:\n",
        "            entity_map[entity.lower()] = entity_type\n",
        "\n",
        "    # Process text in manageable chunks to avoid memory issues\n",
        "    max_chunk_size = 5000\n",
        "    chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
        "\n",
        "    logger.info(f\"Processing {len(chunks)} chunks for entity relationships\")\n",
        "\n",
        "    # Process each chunk\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        if i % 10 == 0:\n",
        "            logger.info(f\"Processing relationship chunk {i+1}/{len(chunks)}\")\n",
        "\n",
        "        if not chunk.strip():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Process with spaCy\n",
        "            doc = nlp_model(chunk)\n",
        "\n",
        "            # Extract sentences\n",
        "            sentences = list(doc.sents)\n",
        "\n",
        "            # Analyze each sentence for entity relationships\n",
        "            for sentence in sentences:\n",
        "                sentence_text = sentence.text.lower()\n",
        "\n",
        "                # Find entities in this sentence\n",
        "                sentence_entities = {}\n",
        "                for entity, entity_type in entity_map.items():\n",
        "                    if entity.lower() in sentence_text:\n",
        "                        start = sentence_text.find(entity.lower())\n",
        "                        if start >= 0:\n",
        "                            sentence_entities[(start, start + len(entity))] = (entity, entity_type)\n",
        "\n",
        "                # If we have multiple entity types in the sentence, check for relationships\n",
        "                entity_types_in_sent = set(et for _, et in sentence_entities.values())\n",
        "\n",
        "                # Species and habitat relationship\n",
        "                if \"SPECIES\" in entity_types_in_sent and \"HABITAT\" in entity_types_in_sent:\n",
        "                    # Look for habitat relationship patterns\n",
        "                    habitat_patterns = [\n",
        "                        \"found in\", \"lives in\", \"inhabits\", \"native to\", \"occurs in\",\n",
        "                        \"present in\", \"located in\", \"resides in\", \"dwells in\", \"occupies\"\n",
        "                    ]\n",
        "\n",
        "                    for pattern in habitat_patterns:\n",
        "                        if pattern in sentence_text:\n",
        "                            # Find closest species and habitat entities\n",
        "                            pattern_pos = sentence_text.find(pattern)\n",
        "                            species_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                               in sentence_entities.items()\n",
        "                                               if entity_type == \"SPECIES\"]\n",
        "                            habitat_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                              in sentence_entities.items()\n",
        "                                              if entity_type == \"HABITAT\"]\n",
        "\n",
        "                            if species_entities and habitat_entities:\n",
        "                                # Find closest species before the pattern\n",
        "                                species_before = [(abs(pattern_pos - (pos + len(entity[0]))), entity[0])\n",
        "                                               for pos, entity in species_entities\n",
        "                                               if pos < pattern_pos]\n",
        "\n",
        "                                # Find closest habitat after the pattern\n",
        "                                habitat_after = [(abs(pos - (pattern_pos + len(pattern))), entity[0])\n",
        "                                              for pos, entity in habitat_entities\n",
        "                                              if pos > pattern_pos]\n",
        "\n",
        "                                if species_before and habitat_after:\n",
        "                                    species = min(species_before, key=lambda x: x[0])[1]\n",
        "                                    habitat = min(habitat_after, key=lambda x: x[0])[1]\n",
        "                                    relationship = (species, habitat, sentence.text)\n",
        "                                    if relationship not in relationships[\"species_habitat\"]:\n",
        "                                        relationships[\"species_habitat\"].append(relationship)\n",
        "\n",
        "                # Species and body part relationship\n",
        "                if \"SPECIES\" in entity_types_in_sent and \"ANIMAL_BODY_PART\" in entity_types_in_sent:\n",
        "                    # Find possession patterns\n",
        "                    possession_patterns = [\n",
        "                        \"has\", \"with\", \"possesses\", \"featuring\", \"characterized by\",\n",
        "                        \"'s\", \"of the\", \"of its\", \"their\", \"its\"\n",
        "                    ]\n",
        "\n",
        "                    for pattern in possession_patterns:\n",
        "                        if pattern in sentence_text:\n",
        "                            pattern_pos = sentence_text.find(pattern)\n",
        "                            species_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                               in sentence_entities.items()\n",
        "                                               if entity_type == \"SPECIES\"]\n",
        "                            body_part_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                                in sentence_entities.items()\n",
        "                                                if entity_type == \"ANIMAL_BODY_PART\"]\n",
        "\n",
        "                            if species_entities and body_part_entities:\n",
        "                                species_before = [(abs(pattern_pos - (pos + len(entity[0]))), entity[0])\n",
        "                                               for pos, entity in species_entities\n",
        "                                               if pos < pattern_pos]\n",
        "\n",
        "                                body_part_after = [(abs(pos - (pattern_pos + len(pattern))), entity[0])\n",
        "                                              for pos, entity in body_part_entities\n",
        "                                              if pos > pattern_pos]\n",
        "\n",
        "                                if species_before and body_part_after:\n",
        "                                    species = min(species_before, key=lambda x: x[0])[1]\n",
        "                                    body_part = min(body_part_after, key=lambda x: x[0])[1]\n",
        "                                    relationship = (species, body_part, sentence.text)\n",
        "                                    if relationship not in relationships[\"species_body_part\"]:\n",
        "                                        relationships[\"species_body_part\"].append(relationship)\n",
        "\n",
        "                # Species and measurements\n",
        "                if \"SPECIES\" in entity_types_in_sent and (\"MEASUREMENT\" in entity_types_in_sent or \"LENGTH\" in entity_types_in_sent):\n",
        "                    # Look for measurement patterns\n",
        "                    measurement_patterns = [\n",
        "                        \"measures\", \"weighs\", \"length of\", \"size of\", \"weight of\",\n",
        "                        \"is about\", \"approximately\", \"typically\", \"averages\", \"ranging from\"\n",
        "                    ]\n",
        "\n",
        "                    measurement_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                           in sentence_entities.items()\n",
        "                                           if entity_type in [\"MEASUREMENT\", \"LENGTH\"]]\n",
        "\n",
        "                    species_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                       in sentence_entities.items()\n",
        "                                       if entity_type == \"SPECIES\"]\n",
        "\n",
        "                    if species_entities and measurement_entities:\n",
        "                        for species_pos, species_entity in species_entities:\n",
        "                            for measurement_pos, measurement_entity in measurement_entities:\n",
        "                                # Check if they're close to each other (within 50 characters)\n",
        "                                if abs(species_pos - measurement_pos) < 50:\n",
        "                                    relationship = (species_entity[0], measurement_entity[0], sentence.text)\n",
        "                                    if relationship not in relationships[\"species_measurement\"]:\n",
        "                                        relationships[\"species_measurement\"].append(relationship)\n",
        "\n",
        "                # Species and group relationships\n",
        "                if \"SPECIES\" in entity_types_in_sent and \"ANIMAL_GROUP\" in entity_types_in_sent:\n",
        "                    group_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                     in sentence_entities.items()\n",
        "                                     if entity_type == \"ANIMAL_GROUP\"]\n",
        "\n",
        "                    species_entities = [(pos, entity) for pos, (entity, entity_type)\n",
        "                                       in sentence_entities.items()\n",
        "                                       if entity_type == \"SPECIES\"]\n",
        "\n",
        "                    if species_entities and group_entities:\n",
        "                        for group_pos, group_entity in group_entities:\n",
        "                            # Group terms usually have \"of X\" structure, check for the species after \"of\"\n",
        "                            if \"of \" in group_entity[0]:\n",
        "                                group_text = group_entity[0]\n",
        "                                after_of = group_text.split(\"of \")[1].strip().lower()\n",
        "\n",
        "                                # Check if any species appears in or matches the group description\n",
        "                                for _, species_entity in species_entities:\n",
        "                                    species_text = species_entity[0].lower()\n",
        "                                    if species_text in after_of or after_of in species_text:\n",
        "                                        relationship = (species_entity[0], group_entity[0], sentence.text)\n",
        "                                        if relationship not in relationships[\"species_group\"]:\n",
        "                                            relationships[\"species_group\"].append(relationship)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error processing chunk {i} for relationships: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Log results\n",
        "    for relation_type, relations in relationships.items():\n",
        "        logger.info(f\"Found {len(relations)} {relation_type} relationships\")\n",
        "\n",
        "    return relationships\n",
        "\n",
        "# Function to generate a co-occurrence matrix of entities\n",
        "def generate_cooccurrence_matrix(extracted_entities, entity_types):\n",
        "    \"\"\"\n",
        "    Generate a co-occurrence matrix of entities based on entity types.\n",
        "\n",
        "    Args:\n",
        "        extracted_entities: Dictionary of extracted entities by type\n",
        "        entity_types: List of entity types to include\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with co-occurrence counts\n",
        "    \"\"\"\n",
        "    # Create a flat list of all entities with their types\n",
        "    all_entities = []\n",
        "    for entity_type in entity_types:\n",
        "        entities = extracted_entities.get(entity_type, [])\n",
        "        all_entities.extend([(entity, entity_type) for entity in entities])\n",
        "\n",
        "    # Create a dictionary to store co-occurrence counts\n",
        "    cooccurrence = defaultdict(int)\n",
        "\n",
        "    # Count co-occurrences by looking at all pairs\n",
        "    for (entity1, type1), (entity2, type2) in combinations(all_entities, 2):\n",
        "        if type1 != type2:  # Only interested in relationships between different types\n",
        "            key = (f\"{type1}: {entity1}\", f\"{type2}: {entity2}\")\n",
        "            cooccurrence[key] += 1\n",
        "\n",
        "    # Convert to a matrix format\n",
        "    unique_labeled_entities = sorted(set([f\"{type}: {entity}\" for entity, type in all_entities]))\n",
        "    matrix_size = len(unique_labeled_entities)\n",
        "\n",
        "    # Map entity labels to indices\n",
        "    entity_to_idx = {entity: i for i, entity in enumerate(unique_labeled_entities)}\n",
        "\n",
        "    # Create a sparse matrix\n",
        "    row, col, data = [], [], []\n",
        "\n",
        "    for (entity1, entity2), count in cooccurrence.items():\n",
        "        if entity1 in entity_to_idx and entity2 in entity_to_idx:\n",
        "            i, j = entity_to_idx[entity1], entity_to_idx[entity2]\n",
        "            row.append(i)\n",
        "            col.append(j)\n",
        "            data.append(count)\n",
        "            # Make it symmetric\n",
        "            row.append(j)\n",
        "            col.append(i)\n",
        "            data.append(count)\n",
        "\n",
        "    # Create the sparse matrix\n",
        "    matrix = csr_matrix((data, (row, col)), shape=(matrix_size, matrix_size))\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    cooccurrence_df = pd.DataFrame(matrix.toarray(), index=unique_labeled_entities, columns=unique_labeled_entities)\n",
        "\n",
        "    return cooccurrence_df\n",
        "\n",
        "# Function to create a relationship graph\n",
        "def create_relationship_graph(relationships, entity_types, min_weight=1):\n",
        "    \"\"\"\n",
        "    Create a NetworkX graph representing entity relationships.\n",
        "\n",
        "    Args:\n",
        "        relationships: Dictionary of relationship types and their instances\n",
        "        entity_types: List of entity types to include\n",
        "        min_weight: Minimum weight to include an edge\n",
        "\n",
        "    Returns:\n",
        "        NetworkX graph\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes with entity type information\n",
        "    entity_nodes = set()\n",
        "\n",
        "    # Process each relationship type\n",
        "    for relation_type, relations in relationships.items():\n",
        "        for relation in relations:\n",
        "            entity1, entity2, _ = relation\n",
        "\n",
        "            # Determine entity types\n",
        "            entity1_type = None\n",
        "            entity2_type = None\n",
        "\n",
        "            if relation_type == \"species_habitat\":\n",
        "                entity1_type = \"SPECIES\"\n",
        "                entity2_type = \"HABITAT\"\n",
        "            elif relation_type == \"species_body_part\":\n",
        "                entity1_type = \"SPECIES\"\n",
        "                entity2_type = \"ANIMAL_BODY_PART\"\n",
        "            elif relation_type == \"species_measurement\":\n",
        "                entity1_type = \"SPECIES\"\n",
        "                entity2_type = \"MEASUREMENT\"\n",
        "            elif relation_type == \"species_group\":\n",
        "                entity1_type = \"SPECIES\"\n",
        "                entity2_type = \"ANIMAL_GROUP\"\n",
        "\n",
        "            # Add nodes if they don't exist\n",
        "            if entity1 not in entity_nodes:\n",
        "                G.add_node(entity1, type=entity1_type)\n",
        "                entity_nodes.add(entity1)\n",
        "\n",
        "            if entity2 not in entity_nodes:\n",
        "                G.add_node(entity2, type=entity2_type)\n",
        "                entity_nodes.add(entity2)\n",
        "\n",
        "            # Add or update edge\n",
        "            if G.has_edge(entity1, entity2):\n",
        "                G[entity1][entity2]['weight'] += 1\n",
        "                G[entity1][entity2]['examples'].append(relation[2])\n",
        "                G[entity1][entity2]['types'].add(relation_type)\n",
        "            else:\n",
        "                G.add_edge(\n",
        "                    entity1,\n",
        "                    entity2,\n",
        "                    weight=1,\n",
        "                    examples=[relation[2]],\n",
        "                    types={relation_type}\n",
        "                )\n",
        "\n",
        "    # Remove edges below the minimum weight\n",
        "    edges_to_remove = [(u, v) for u, v, d in G.edges(data=True) if d['weight'] < min_weight]\n",
        "    G.remove_edges_from(edges_to_remove)\n",
        "\n",
        "    # Remove isolated nodes\n",
        "    G.remove_nodes_from(list(nx.isolates(G)))\n",
        "\n",
        "    return G\n",
        "\n",
        "# Function to visualize entity relationships\n",
        "def visualize_entity_relationships(G, output_path=None, max_nodes=50):\n",
        "    \"\"\"\n",
        "    Create a visualization of entity relationships using NetworkX.\n",
        "\n",
        "    Args:\n",
        "        G: NetworkX graph of entity relationships\n",
        "        output_path: Path to save the visualization\n",
        "        max_nodes: Maximum number of nodes to include in the visualization\n",
        "\n",
        "    Returns:\n",
        "        Path to the saved visualization\n",
        "    \"\"\"\n",
        "    if output_path is None:\n",
        "        temp_dir = tempfile.mkdtemp()\n",
        "        output_file = os.path.join(temp_dir, \"entity_relationships.png\")\n",
        "    else:\n",
        "        output_file = output_path\n",
        "\n",
        "    # If graph is too large, take a subgraph\n",
        "    if len(G.nodes()) > max_nodes:\n",
        "        # Sort edges by weight\n",
        "        edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)\n",
        "\n",
        "        # Take top N edges\n",
        "        top_edges = edges[:max_nodes]\n",
        "\n",
        "        # Create subgraph\n",
        "        nodes = set()\n",
        "        for u, v, _ in top_edges:\n",
        "            nodes.add(u)\n",
        "            nodes.add(v)\n",
        "\n",
        "        G = G.subgraph(nodes)\n",
        "\n",
        "    # Set up plot\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Define node colors by type\n",
        "    color_map = {\n",
        "        'SPECIES': '#1f77b4',       # Blue\n",
        "        'HABITAT': '#2ca02c',       # Green\n",
        "        'ANIMAL_BODY_PART': '#d62728',  # Red\n",
        "        'MEASUREMENT': '#9467bd',   # Purple\n",
        "        'LENGTH': '#8c564b',        # Brown\n",
        "        'ANIMAL_GROUP': '#e377c2'   # Pink\n",
        "    }\n",
        "\n",
        "    # Get node colors\n",
        "    node_colors = [color_map.get(G.nodes[node]['type'], '#7f7f7f') for node in G.nodes()]\n",
        "\n",
        "    # Get edge weights for line thickness\n",
        "    edge_weights = [G[u][v]['weight'] * 0.5 for u, v in G.edges()]\n",
        "\n",
        "    # Create position layout\n",
        "    pos = nx.spring_layout(G, k=0.3, iterations=50, seed=42)\n",
        "\n",
        "    # Draw the graph\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=800, node_color=node_colors, alpha=0.8)\n",
        "\n",
        "    nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.6, edge_color='gray')\n",
        "\n",
        "    # Draw labels with smaller font size for better readability\n",
        "    nx.draw_networkx_labels(G, pos, font_size=8, font_family='sans-serif')\n",
        "\n",
        "    # Create legend for node types\n",
        "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w',\n",
        "                                 label=entity_type,\n",
        "                                 markerfacecolor=color, markersize=10)\n",
        "                      for entity_type, color in color_map.items()]\n",
        "\n",
        "    plt.legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "    plt.title(\"Entity Relationship Network\", fontsize=16)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save figure\n",
        "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return output_file\n",
        "\n",
        "# Function to create a heatmap visualization of entity co-occurrences\n",
        "def visualize_cooccurrence_matrix(cooccurrence_df, output_path=None, max_items=50):\n",
        "    \"\"\"\n",
        "    Create a heatmap visualization of entity co-occurrences.\n",
        "\n",
        "    Args:\n",
        "        cooccurrence_df: DataFrame with co-occurrence counts\n",
        "        output_path: Path to save the visualization\n",
        "        max_items: Maximum number of items to include in the visualization\n",
        "\n",
        "    Returns:\n",
        "        Path to the saved visualization\n",
        "    \"\"\"\n",
        "    if output_path is None:\n",
        "        temp_dir = tempfile.mkdtemp()\n",
        "        output_file = os.path.join(temp_dir, \"entity_cooccurrence.png\")\n",
        "    else:\n",
        "        output_file = output_path\n",
        "\n",
        "    # If matrix is too large, take a subset with highest values\n",
        "    if len(cooccurrence_df) > max_items:\n",
        "        # Get the sum of co-occurrences for each entity\n",
        "        row_sums = cooccurrence_df.sum(axis=1)\n",
        "\n",
        "        # Get the top entities\n",
        "        top_entities = row_sums.nlargest(max_items).index\n",
        "\n",
        "        # Filter the matrix\n",
        "        cooccurrence_df = cooccurrence_df.loc[top_entities, top_entities]\n",
        "\n",
        "    # Set up figure\n",
        "    plt.figure(figsize=(16, 14))\n",
        "\n",
        "    # Create a custom colormap from white to blue\n",
        "    colors = [(1, 1, 1), (0.12, 0.47, 0.71)]\n",
        "    cmap = LinearSegmentedColormap.from_list(\"custom_blue\", colors, N=100)\n",
        "\n",
        "    # Create heatmap\n",
        "    ax = sns.heatmap(cooccurrence_df, cmap=cmap, linewidths=0.5,\n",
        "                     linecolor='gray', square=True, cbar_kws={\"shrink\": 0.8})\n",
        "\n",
        "    # Rotate x-axis labels for better readability\n",
        "    plt.xticks(rotation=90, fontsize=8)\n",
        "    plt.yticks(fontsize=8)\n",
        "\n",
        "    plt.title(\"Entity Co-occurrence Matrix\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save figure\n",
        "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return output_file\n",
        "\n",
        "# Modify the main process_pdf_multi_model function to include relationships\n",
        "def process_pdf_multi_model_with_relationships(pdf_path, entity_types, selected_models, output_dir=None, extract_relationships=True):\n",
        "    \"\"\"\n",
        "    Enhanced version of process_pdf_multi_model that also extracts entity relationships.\n",
        "\n",
        "    Args:\n",
        "        pdf_path: Path to the PDF file\n",
        "        entity_types: List of entity types to extract\n",
        "        selected_models: List of NLP models to use\n",
        "        output_dir: Directory to save outputs\n",
        "        extract_relationships: Whether to extract and visualize relationships\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (result_text, text_sample, chart_file, relationship_files)\n",
        "    \"\"\"\n",
        "    # First, run the original processing\n",
        "    result_text, text_sample, chart_file = process_pdf_multi_model(\n",
        "        pdf_path, entity_types, selected_models, output_dir\n",
        "    )\n",
        "\n",
        "    if isinstance(result_text, str) and result_text.startswith(\"Error\"):\n",
        "        return result_text, text_sample, chart_file, None\n",
        "\n",
        "    # Skip relationship extraction if not requested or if there was an error\n",
        "    if not extract_relationships:\n",
        "        return result_text, text_sample, chart_file, None\n",
        "\n",
        "    # Extract text from PDF again (we don't have it from the original function)\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    if isinstance(text, str) and text.startswith(\"Error\"):\n",
        "        result_text += f\"\\n\\n## Entity Relationships\\n\\nError extracting text for relationship analysis: {text}\\n\"\n",
        "        return result_text, text_sample, chart_file, None\n",
        "\n",
        "    # Track relationship results and visualizations\n",
        "    relationship_files = {}\n",
        "    extracted_entities_by_model = {}\n",
        "\n",
        "    # For each model, try to extract relationships\n",
        "    for model_name in selected_models:\n",
        "        relationship_section = f\"\\n\\n## Entity Relationships: {model_name}\\n\\n\"\n",
        "\n",
        "        # Skip if there was an error with this model\n",
        "        if model_name not in nlp_models or isinstance(nlp_models[model_name], str):\n",
        "            relationship_section += f\"Cannot extract relationships: Model unavailable or error.\\n\\n\"\n",
        "            result_text += relationship_section\n",
        "            continue\n",
        "\n",
        "        # Only extract relationships with SpaCy models (not BioBERT)\n",
        "        if model_name == \"BioBERT\" or model_name not in nlp_models:\n",
        "            relationship_section += f\"Relationship extraction is only available with SpaCy models.\\n\\n\"\n",
        "            result_text += relationship_section\n",
        "            continue\n",
        "\n",
        "        # Extract entities again (we're not saving them from the original function)\n",
        "        extracted_entities = extract_entities(text, entity_types, model_name)\n",
        "        extracted_entities_by_model[model_name] = extracted_entities\n",
        "\n",
        "        if isinstance(extracted_entities, str) and extracted_entities.startswith(\"Error\"):\n",
        "            relationship_section += f\"Error extracting entities: {extracted_entities}\\n\\n\"\n",
        "            result_text += relationship_section\n",
        "            continue\n",
        "\n",
        "        # Now extract relationships\n",
        "        logger.info(f\"Extracting entity relationships with {model_name}\")\n",
        "        relationships = extract_entity_relationships(text, extracted_entities, nlp_models[model_name])\n",
        "\n",
        "        if \"error\" in relationships:\n",
        "            relationship_section += f\"Error extracting relationships: {relationships['error'][0]}\\n\\n\"\n",
        "            result_text += relationship_section\n",
        "            continue\n",
        "\n",
        "        # Get total count of relationships\n",
        "        total_relationships = sum(len(relations) for relations in relationships.values())\n",
        "\n",
        "        relationship_section += f\"Found {total_relationships} relationships between entities.\\n\\n\"\n",
        "\n",
        "        # Report on each relationship type\n",
        "        for relation_type, relations in relationships.items():\n",
        "            if relations:\n",
        "                relationship_section += f\"### {relation_type.replace('_', ' ').title()} ({len(relations)} found)\\n\\n\"\n",
        "\n",
        "                # Show the top examples (up to 10)\n",
        "                for idx, (entity1, entity2, example) in enumerate(relations[:10]):\n",
        "                    relationship_section += f\"{idx+1}. **{entity1}** → **{entity2}**\\n\"\n",
        "                    relationship_section += f\"   *Example: \\\"{example.strip()}\\\"*\\n\\n\"\n",
        "\n",
        "                if len(relations) > 10:\n",
        "                    relationship_section += f\"*...and {len(relations) - 10} more...*\\n\\n\"\n",
        "            else:\n",
        "                relationship_section += f\"### {relation_type.replace('_', ' ').title()}\\n\\n\"\n",
        "                relationship_section += \"No relationships of this type found.\\n\\n\"\n",
        "\n",
        "        # Create graph visualization\n",
        "        if total_relationships > 0:\n",
        "            # Create relationship graph\n",
        "            G = create_relationship_graph(relationships, entity_types)\n",
        "\n",
        "            if G.number_of_nodes() > 0:\n",
        "                # Visualize graph\n",
        "                if output_dir:\n",
        "                    graph_file = os.path.join(output_dir, f\"{model_name}_relationship_graph.png\")\n",
        "                else:\n",
        "                    graph_file = None\n",
        "\n",
        "                graph_viz = visualize_entity_relationships(G, graph_file)\n",
        "\n",
        "                if graph_viz:\n",
        "                    relationship_section += f\"\\n### Relationship Graph\\n\\n\"\n",
        "                    relationship_section += f\"Entity relationship visualization saved to: {graph_viz}\\n\\n\"\n",
        "                    relationship_files[f\"{model_name}_graph\"] = graph_viz\n",
        "\n",
        "            # Create co-occurrence matrix\n",
        "            try:\n",
        "                cooccurrence_df = generate_cooccurrence_matrix(extracted_entities, entity_types)\n",
        "\n",
        "                if not cooccurrence_df.empty:\n",
        "                    # Visualize co-occurrence matrix\n",
        "                    if output_dir:\n",
        "                        matrix_file = os.path.join(output_dir, f\"{model_name}_cooccurrence_matrix.png\")\n",
        "                    else:\n",
        "                        matrix_file = None\n",
        "\n",
        "                    matrix_viz = visualize_cooccurrence_matrix(cooccurrence_df, matrix_file)\n",
        "\n",
        "                    if matrix_viz:\n",
        "                        relationship_section += f\"\\n### Entity Co-occurrence Matrix\\n\\n\"\n",
        "                        relationship_section += f\"Co-occurrence matrix visualization saved to: {matrix_viz}\\n\\n\"\n",
        "                        relationship_files[f\"{model_name}_matrix\"] = matrix_viz\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error generating co-occurrence matrix: {str(e)}\")\n",
        "                relationship_section += f\"\\nError generating co-occurrence matrix: {str(e)}\\n\\n\"\n",
        "\n",
        "        # Add relationship section to results\n",
        "        result_text += relationship_section\n",
        "\n",
        "    # Return enhanced results\n",
        "    return result_text, text_sample, chart_file, relationship_files"
      ],
      "metadata": {
        "id": "iqMPCXyP6sjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0Zue5WCyVtP"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "# Dictionary to store loaded transformer models\n",
        "transformer_models = {}\n",
        "\n",
        "# Available transformer models\n",
        "AVAILABLE_TRANSFORMER_MODELS = {\n",
        "    \"BERT NER\": \"dslim/bert-base-NER\",\n",
        "    \"RoBERTa NER\": \"Jean-Baptiste/roberta-large-ner-english\",\n",
        "    \"BioBERT\": \"dmis-lab/biobert-base-cased-v1.1-squad\",\n",
        "    \"DistilBERT NER\": \"elastic/distilbert-base-cased-finetuned-conll03-english\",\n",
        "    \"XLM-RoBERTa\": \"xlm-roberta-large-finetuned-conll03-english\"\n",
        "}\n",
        "\n",
        "def load_transformer_model(model_name):\n",
        "    if model_name in transformer_models:\n",
        "        return transformer_models[model_name]\n",
        "\n",
        "    model_path = AVAILABLE_TRANSFORMER_MODELS.get(model_name)\n",
        "    if not model_path:\n",
        "        logger.error(f\"Unknown transformer model: {model_name}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"Loading transformer model: {model_name} ({model_path})\")\n",
        "        ner_pipeline = pipeline(\"ner\", model=model_path, aggregation_strategy=\"simple\")\n",
        "        transformer_models[model_name] = ner_pipeline\n",
        "        logger.info(f\"Transformer model {model_name} loaded successfully\")\n",
        "        return ner_pipeline\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading transformer model {model_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def extract_entities_with_transformer(text, entity_types, model_name):\n",
        "    if isinstance(text, str) and not text.startswith(\"Error\"):\n",
        "        try:\n",
        "            # Get the specified transformer model\n",
        "            ner_model = transformer_models.get(model_name)\n",
        "            if ner_model is None:\n",
        "                ner_model = load_transformer_model(model_name)\n",
        "                if ner_model is None:\n",
        "                    return f\"Could not load transformer model: {model_name}\"\n",
        "\n",
        "            # Process text in chunks to avoid memory issues\n",
        "            chunk_size = 512  # Adjust based on model requirements\n",
        "            chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "            # Create a dictionary to store extracted entities\n",
        "            extracted = {entity_type: [] for entity_type in entity_types}\n",
        "\n",
        "            # Process each chunk\n",
        "            for chunk in chunks:\n",
        "                entities = ner_model(chunk)\n",
        "\n",
        "                # Map transformer entity types to your entity types\n",
        "                for entity in entities:\n",
        "                    entity_type = map_transformer_entity_type(entity[\"entity_group\"])\n",
        "                    if entity_type in entity_types:\n",
        "                        extracted[entity_type].append(entity[\"word\"])\n",
        "\n",
        "            # Remove duplicates and sort\n",
        "            for entity_type in entity_types:\n",
        "                extracted[entity_type] = sorted(list(set(extracted[entity_type])))\n",
        "\n",
        "            return extracted\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting entities with transformer {model_name}: {str(e)}\")\n",
        "            return f\"Error extracting entities with transformer {model_name}: {str(e)}\"\n",
        "    else:\n",
        "        return f\"Cannot extract entities: {text}\"\n",
        "\n",
        "def map_transformer_entity_type(transformer_type):\n",
        "    # Map transformer entity types to your custom types\n",
        "    mapping = {\n",
        "        \"B-MISC\": \"SPECIES\",\n",
        "        \"I-MISC\": \"SPECIES\",\n",
        "        \"B-ORG\": \"ANIMAL_GROUP\",\n",
        "        \"I-ORG\": \"ANIMAL_GROUP\",\n",
        "        # Add more mappings as needed\n",
        "    }\n",
        "    return mapping.get(transformer_type, \"OTHER\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdgusvuMyxIy",
        "outputId": "29350e0a-eef3-4f60-cded-f9f7e108c4ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flair in /usr/local/lib/python3.11/dist-packages (0.15.1)\n",
            "Requirement already satisfied: boto3>=1.20.27 in /usr/local/lib/python3.11/dist-packages (from flair) (1.38.8)\n",
            "Requirement already satisfied: conllu<5.0.0,>=4.0 in /usr/local/lib/python3.11/dist-packages (from flair) (4.5.3)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.11/dist-packages (from flair) (1.2.18)\n",
            "Requirement already satisfied: ftfy>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from flair) (6.3.1)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from flair) (5.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from flair) (0.30.2)\n",
            "Requirement already satisfied: langdetect>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from flair) (1.0.9)\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from flair) (5.4.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from flair) (3.10.0)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.11/dist-packages (from flair) (10.7.0)\n",
            "Requirement already satisfied: mpld3>=0.3 in /usr/local/lib/python3.11/dist-packages (from flair) (0.5.10)\n",
            "Requirement already satisfied: pptree>=3.1 in /usr/local/lib/python3.11/dist-packages (from flair) (3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from flair) (2.9.0.post0)\n",
            "Requirement already satisfied: pytorch-revgrad>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from flair) (0.2.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from flair) (2024.11.6)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from flair) (1.6.1)\n",
            "Requirement already satisfied: segtok>=1.5.11 in /usr/local/lib/python3.11/dist-packages (from flair) (1.5.11)\n",
            "Requirement already satisfied: sqlitedict>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from flair) (2.1.0)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.11/dist-packages (from flair) (0.9.0)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from flair) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.11/dist-packages (from flair) (4.67.1)\n",
            "Requirement already satisfied: transformer-smaller-training-vocab>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from flair) (0.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.25.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (4.51.3)\n",
            "Requirement already satisfied: wikipedia-api>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from flair) (0.8.1)\n",
            "Requirement already satisfied: bioc<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from flair) (2.1)\n",
            "Requirement already satisfied: jsonlines>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from bioc<3.0.0,>=2.0.0->flair) (4.0.0)\n",
            "Requirement already satisfied: intervaltree in /usr/local/lib/python3.11/dist-packages (from bioc<3.0.0,>=2.0.0->flair) (3.1.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.11/dist-packages (from bioc<3.0.0,>=2.0.0->flair) (0.6.2)\n",
            "Requirement already satisfied: botocore<1.39.0,>=1.38.8 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.20.27->flair) (1.38.8)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.20.27->flair) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.20.27->flair) (0.12.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.13->flair) (1.17.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (2.32.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (4.13.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect>=1.0.9->flair) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (3.2.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mpld3>=0.3->flair) (3.1.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->flair) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (5.29.4)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.8->boto3>=1.20.27->flair) (2.4.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair) (25.3.0)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (1.6.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.7)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from intervaltree->bioc<3.0.0,>=2.0.0->flair) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mpld3>=0.3->flair) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (5.9.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install flair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj08dWKAyr4p"
      },
      "outputs": [],
      "source": [
        "import flair\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "# Dictionary to store loaded flair models\n",
        "flair_models = {}\n",
        "\n",
        "# Available flair models\n",
        "AVAILABLE_FLAIR_MODELS = {\n",
        "    \"Flair English\": \"flair/ner-english\",\n",
        "    \"Flair English Fast\": \"flair/ner-english-fast\",\n",
        "    \"Flair Multi\": \"flair/ner-multi\",\n",
        "    \"Flair English Ontonotes\": \"flair/ner-english-ontonotes-large\"\n",
        "}\n",
        "\n",
        "def load_flair_model(model_name):\n",
        "    if model_name in flair_models:\n",
        "        return flair_models[model_name]\n",
        "\n",
        "    model_path = AVAILABLE_FLAIR_MODELS.get(model_name)\n",
        "    if not model_path:\n",
        "        logger.error(f\"Unknown flair model: {model_name}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"Loading flair model: {model_name} ({model_path})\")\n",
        "        tagger = SequenceTagger.load(model_path)\n",
        "        flair_models[model_name] = tagger\n",
        "        logger.info(f\"Flair model {model_name} loaded successfully\")\n",
        "        return tagger\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading flair model {model_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def extract_entities_with_flair(text, entity_types, model_name):\n",
        "    if isinstance(text, str) and not text.startswith(\"Error\"):\n",
        "        try:\n",
        "            # Get the specified flair model\n",
        "            tagger = flair_models.get(model_name)\n",
        "            if tagger is None:\n",
        "                tagger = load_flair_model(model_name)\n",
        "                if tagger is None:\n",
        "                    return f\"Could not load flair model: {model_name}\"\n",
        "\n",
        "            # Process text in chunks to avoid memory issues\n",
        "            max_chunk_size = 1000  # Adjust based on model requirements\n",
        "            chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
        "\n",
        "            # Create a dictionary to store extracted entities\n",
        "            extracted = {entity_type: [] for entity_type in entity_types}\n",
        "\n",
        "            # Process each chunk\n",
        "            for chunk in chunks:\n",
        "                sentence = Sentence(chunk)\n",
        "                tagger.predict(sentence)\n",
        "\n",
        "                # Extract entities from the tagged sentence\n",
        "                for entity in sentence.get_spans('ner'):\n",
        "                    entity_type = map_flair_entity_type(entity.tag)\n",
        "                    if entity_type in entity_types:\n",
        "                        extracted[entity_type].append(entity.text)\n",
        "\n",
        "            # Remove duplicates and sort\n",
        "            for entity_type in entity_types:\n",
        "                extracted[entity_type] = sorted(list(set(extracted[entity_type])))\n",
        "\n",
        "            return extracted\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting entities with flair {model_name}: {str(e)}\")\n",
        "            return f\"Error extracting entities with flair {model_name}: {str(e)}\"\n",
        "    else:\n",
        "        return f\"Cannot extract entities: {text}\"\n",
        "\n",
        "def map_flair_entity_type(flair_type):\n",
        "    # Map flair entity types to your custom types\n",
        "    mapping = {\n",
        "        \"PER\": \"SPECIES\",\n",
        "        \"LOC\": \"HABITAT\",\n",
        "        \"ORG\": \"ANIMAL_GROUP\",\n",
        "        # Add more mappings as needed\n",
        "    }\n",
        "    return mapping.get(flair_type, \"OTHER\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSPOqeI4zd5J",
        "outputId": "954109c1-414e-45ba-86d0-c1d8f16629ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting allennlp\n",
            "  Using cached allennlp-2.10.1-py3-none-any.whl.metadata (21 kB)\n",
            "INFO: pip is looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached allennlp-2.10.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Using cached allennlp-2.9.3-py3-none-any.whl.metadata (19 kB)\n",
            "  Using cached allennlp-2.9.2-py3-none-any.whl.metadata (19 kB)\n",
            "  Using cached allennlp-2.9.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Using cached allennlp-2.9.0-py3-none-any.whl.metadata (18 kB)\n",
            "  Using cached allennlp-2.8.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Using cached allennlp-2.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "INFO: pip is still looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached allennlp-2.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Using cached allennlp-2.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Using cached allennlp-2.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Using cached allennlp-2.3.1-py3-none-any.whl.metadata (17 kB)\n",
            "  Using cached allennlp-2.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached allennlp-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Using cached allennlp-2.1.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Using cached allennlp-2.0.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Using cached allennlp-2.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Using cached allennlp-1.5.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Using cached allennlp-1.4.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Using cached allennlp-1.4.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Using cached allennlp-1.3.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Using cached allennlp-1.2.2-py3-none-any.whl.metadata (15 kB)\n",
            "  Using cached allennlp-1.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "  Using cached allennlp-1.2.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Using cached allennlp-1.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Using cached allennlp-1.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached allennlp-0.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from allennlp) (2.6.0+cu124)\n",
            "Collecting overrides (from allennlp)\n",
            "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from allennlp) (3.9.1)\n",
            "Collecting spacy<2.2,>=2.1.0 (from allennlp)\n",
            "  Using cached spacy-2.1.9.tar.gz (30.7 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Collecting allennlp-models\n",
            "  Using cached allennlp_models-2.10.1-py3-none-any.whl.metadata (23 kB)\n",
            "INFO: pip is looking at multiple versions of allennlp-models to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached allennlp_models-2.10.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Using cached allennlp_models-2.9.3-py3-none-any.whl.metadata (23 kB)\n",
            "  Using cached allennlp_models-2.9.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Using cached allennlp_models-2.8.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Using cached allennlp_models-2.7.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Using cached allennlp_models-2.6.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Using cached allennlp_models-2.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "INFO: pip is still looking at multiple versions of allennlp-models to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached allennlp_models-2.4.0-py3-none-any.whl.metadata (22 kB)\n",
            "  Using cached allennlp_models-2.3.0-py3-none-any.whl.metadata (21 kB)\n",
            "  Using cached allennlp_models-2.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "  Using cached allennlp_models-2.1.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "  Using cached allennlp_models-2.0.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached allennlp_models-2.0.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "  Using cached allennlp_models-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "  Using cached allennlp_models-1.4.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "  Using cached allennlp_models-1.4.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "  Using cached allennlp_models-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "  Using cached allennlp_models-1.2.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "  Using cached allennlp_models-1.2.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "  Using cached allennlp_models-1.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting conllu==4.2.1 (from allennlp-models)\n",
            "  Using cached conllu-4.2.1-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting word2number>=1.1 (from allennlp-models)\n",
            "  Using cached word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py-rouge==1.1 (from allennlp-models)\n",
            "  Using cached py_rouge-1.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from allennlp-models) (3.9.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from allennlp-models) (6.3.1)\n",
            "Collecting allennlp==1.2.0 (from allennlp-models)\n",
            "  Using cached allennlp-1.2.0-py3-none-any.whl.metadata (13 kB)\n",
            "INFO: pip is looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting allennlp-models\n",
            "  Using cached allennlp_models-1.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting conllu==4.1 (from allennlp-models)\n",
            "  Using cached conllu-4.1-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting allennlp==1.1.0 (from allennlp-models)\n",
            "  Using cached allennlp-1.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting allennlp-models\n",
            "  Using cached allennlp_models-1.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting conllu==3.0 (from allennlp-models)\n",
            "  Using cached conllu-3.0-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting allennlp==1.0.0 (from allennlp-models)\n",
            "  Using cached allennlp-1.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "\u001b[31mERROR: Cannot install allennlp-models==1.2.1, allennlp-models==1.2.2, allennlp-models==1.3.0, allennlp-models==1.4.0, allennlp-models==1.4.1, allennlp-models==1.5.0, allennlp-models==2.0.0, allennlp-models==2.0.1, allennlp-models==2.1.0, allennlp-models==2.10.0, allennlp-models==2.10.1, allennlp-models==2.2.0, allennlp-models==2.3.0, allennlp-models==2.4.0, allennlp-models==2.5.0, allennlp-models==2.6.0, allennlp-models==2.7.0, allennlp-models==2.8.0, allennlp-models==2.9.0 and allennlp-models==2.9.3 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    allennlp-models 2.10.1 depends on torch<1.13.0 and >=1.7.0\n",
            "    allennlp-models 2.10.0 depends on torch<1.12.0 and >=1.7.0\n",
            "    allennlp-models 2.9.3 depends on torch<1.12.0 and >=1.7.0\n",
            "    allennlp-models 2.9.0 depends on torch<1.11.0 and >=1.7.0\n",
            "    allennlp-models 2.8.0 depends on torch<1.11.0 and >=1.7.0\n",
            "    allennlp-models 2.7.0 depends on torch<1.10.0 and >=1.7.0\n",
            "    allennlp-models 2.6.0 depends on torch<1.10.0 and >=1.7.0\n",
            "    allennlp-models 2.5.0 depends on torch<1.9.0 and >=1.7.0\n",
            "    allennlp-models 2.4.0 depends on torch<1.9.0 and >=1.7.0\n",
            "    allennlp-models 2.3.0 depends on torch<1.9.0 and >=1.7.0\n",
            "    allennlp-models 2.2.0 depends on torch<1.9.0 and >=1.7.0\n",
            "    allennlp-models 2.1.0 depends on torch<1.8.0 and >=1.7.0\n",
            "    allennlp-models 2.0.1 depends on torch<1.8.0 and >=1.7.0\n",
            "    allennlp-models 2.0.0 depends on torch<1.8.0 and >=1.7.0\n",
            "    allennlp-models 1.5.0 depends on torch<1.8.0 and >=1.7.0\n",
            "    allennlp-models 1.4.1 depends on torch<1.8.0 and >=1.7.0\n",
            "    allennlp-models 1.4.0 depends on torch<1.8.0 and >=1.7.0\n",
            "    allennlp-models 1.3.0 depends on torch<1.8.0 and >=1.7.0\n",
            "    allennlp-models 1.2.2 depends on torch<1.8.0 and >=1.7.0\n",
            "    allennlp-models 1.2.1 depends on torch<1.8.0 and >=1.7.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install allennlp\n",
        "!pip install allennlp-models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roRJg-ll1Ul_"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Dictionary to store loaded bio-transformer models\n",
        "bio_transformer_models = {}\n",
        "\n",
        "# Available bio-transformer models\n",
        "AVAILABLE_BIO_TRANSFORMER_MODELS = {\n",
        "    \"BioBERT NER\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
        "    \"BlueBERT\": \"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\",\n",
        "    \"SciBERT\": \"allenai/scibert_scivocab_uncased\",\n",
        "    \"PubMedBERT\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
        "}\n",
        "\n",
        "def load_bio_transformer_model(model_name):\n",
        "    if model_name in bio_transformer_models:\n",
        "        return bio_transformer_models[model_name]\n",
        "\n",
        "    model_path = AVAILABLE_BIO_TRANSFORMER_MODELS.get(model_name)\n",
        "    if not model_path:\n",
        "        logger.error(f\"Unknown bio-transformer model: {model_name}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"Loading bio-transformer model: {model_name} ({model_path})\")\n",
        "\n",
        "        # For bio-specific NER, we may need to use token classification instead of pipeline\n",
        "        # This is a simplified example - actual implementation might need custom fine-tuned models\n",
        "        ner_pipeline = pipeline(\"ner\", model=model_path)\n",
        "\n",
        "        bio_transformer_models[model_name] = ner_pipeline\n",
        "        logger.info(f\"Bio-transformer model {model_name} loaded successfully\")\n",
        "        return ner_pipeline\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading bio-transformer model {model_path}: {str(e)}\")\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8bCOFrfy64B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "18d42c4d-8cc1-4440-842f-5038e6689aeb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AVAILABLE_TRANSFORMER_MODELS' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ae4a0bd9260a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m ALL_AVAILABLE_MODELS = {\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mAVAILABLE_MODELS\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Your original SpaCy models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m**\u001b[0m\u001b[0mAVAILABLE_TRANSFORMER_MODELS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mAVAILABLE_FLAIR_MODELS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mAVAILABLE_BIO_TRANSFORMER_MODELS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AVAILABLE_TRANSFORMER_MODELS' is not defined"
          ]
        }
      ],
      "source": [
        "# Update available models dictionary to include all model types\n",
        "ALL_AVAILABLE_MODELS = {\n",
        "    **AVAILABLE_MODELS,  # Your original SpaCy models\n",
        "    **AVAILABLE_TRANSFORMER_MODELS,\n",
        "    **AVAILABLE_FLAIR_MODELS,\n",
        "    **AVAILABLE_BIO_TRANSFORMER_MODELS\n",
        "}\n",
        "\n",
        "def get_available_models():\n",
        "    # Update to check for all model types\n",
        "    installed_models = []\n",
        "\n",
        "    # Check SpaCy models\n",
        "    for name, path in AVAILABLE_MODELS.items():\n",
        "        try:\n",
        "            spacy.load(path)\n",
        "            installed_models.append(name)\n",
        "        except OSError:\n",
        "            pass\n",
        "\n",
        "    # Check for transformers library\n",
        "    try:\n",
        "        import transformers\n",
        "        # If installed, add transformer models\n",
        "        for name in AVAILABLE_TRANSFORMER_MODELS:\n",
        "            installed_models.append(name)\n",
        "    except ImportError:\n",
        "        logger.warning(\"Transformers library not installed. Use pip install transformers to add transformer models.\")\n",
        "\n",
        "    # Check for flair library\n",
        "    try:\n",
        "        import flair\n",
        "        # If installed, add flair models\n",
        "        for name in AVAILABLE_FLAIR_MODELS:\n",
        "            installed_models.append(name)\n",
        "    except ImportError:\n",
        "        logger.warning(\"Flair library not installed. Use pip install flair to add Flair models.\")\n",
        "\n",
        "    # Check for allennlp library\n",
        "    try:\n",
        "        import allennlp\n",
        "        # If installed, add allennlp models\n",
        "        for name in AVAILABLE_ALLENNLP_MODELS:\n",
        "            installed_models.append(name)\n",
        "    except ImportError:\n",
        "        logger.warning(\"AllenNLP library not installed. Use pip install allennlp allennlp_models to add AllenNLP models.\")\n",
        "\n",
        "    # Ensure at least one model is available\n",
        "    if not installed_models:\n",
        "        installed_models.append(\"SpaCy Blank Model\")\n",
        "        logger.warning(\"No models installed. Using blank model as fallback.\")\n",
        "\n",
        "    return installed_models\n",
        "\n",
        "def extract_entities_with_any_model(text, entity_types, model_name):\n",
        "    \"\"\"Dispatch to appropriate extraction function based on model type\"\"\"\n",
        "\n",
        "    # Determine which type of model this is\n",
        "    if model_name in AVAILABLE_MODELS:\n",
        "        return extract_entities(text, entity_types, model_name)  # Original SpaCy function\n",
        "    elif model_name in AVAILABLE_TRANSFORMER_MODELS:\n",
        "        return extract_entities_with_transformer(text, entity_types, model_name)\n",
        "    elif model_name in AVAILABLE_FLAIR_MODELS:\n",
        "        return extract_entities_with_flair(text, entity_types, model_name)\n",
        "    elif model_name in AVAILABLE_ALLENNLP_MODELS:\n",
        "        return extract_entities_with_allennlp(text, entity_types, model_name)\n",
        "    elif model_name in AVAILABLE_BIO_TRANSFORMER_MODELS:\n",
        "        return extract_entities_with_bio_transformer(text, entity_types, model_name)\n",
        "    else:\n",
        "        return f\"Unknown model type: {model_name}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PULCzeSI1q47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47673280-633f-4b7e-9d57-298dba0b38c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Flair library not installed. Use pip install flair to add Flair models.\n",
            "WARNING:__main__:AllenNLP library not installed. Use pip install allennlp allennlp_models to add AllenNLP models.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Available model categories:\n",
            "1. SpaCy Models\n",
            "2. Transformer Models\n",
            "3. Bio-Transformer Models\n",
            "\n",
            "Select a model category (number): 1\n",
            "\n",
            "Available models in SpaCy Models:\n",
            "1. SpaCy Small\n",
            "\n",
            "Select models (comma-separated numbers, e.g., 1,2): 1\n",
            "\n",
            "Enter the path to your PDF file: /content/Animal_welfare_Review_of_the_scientific_concept_an.pdf\n",
            "\n",
            "Available animal-related entity types:\n",
            "1. SPECIES - Animal species scientific or common names\n",
            "2. ANIMAL_GROUP - Groups or collections of animals (herd, flock, etc.)\n",
            "3. ANIMAL_BODY_PART - Animal body parts (paw, wing, etc.)\n",
            "4. MEASUREMENT - Measurements related to animals\n",
            "5. LENGTH - Length measurements\n",
            "6. HABITAT - Animal habitats and environments\n",
            "\n",
            "Select entity types to extract (comma-separated numbers or 'all'): \n",
            "Invalid selection. Using all entity types.\n",
            "\n",
            "Enter output directory (leave blank for current directory): \n",
            "\n",
            "Checking and loading selected models. This may take some time for the first run...\n",
            "Loading SpaCy model: SpaCy Small\n",
            "\n",
            "Processing /content/Animal_welfare_Review_of_the_scientific_concept_an.pdf with models: SpaCy Small\n",
            "Extracting entity types: SPECIES, ANIMAL_GROUP, ANIMAL_BODY_PART, MEASUREMENT, LENGTH, HABITAT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting entities with SpaCy Small: invalid group reference 11 at position 1\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-12-5c658afbe9e1>\", line 485, in extract_entities\n",
            "    text = preprocess_text(text)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-12-5c658afbe9e1>\", line 475, in preprocess_text\n",
            "    text = re.sub(r'(\\d+)l', r'\\11', text)  # Replace \"l\" with \"1\" when in numbers\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/re/__init__.py\", line 185, in sub\n",
            "    return _compile(pattern, flags).sub(repl, string, count)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/re/__init__.py\", line 317, in _subx\n",
            "    template = _compile_repl(template, pattern)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/re/__init__.py\", line 308, in _compile_repl\n",
            "    return _parser.parse_template(repl, pattern)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/re/_parser.py\", line 1081, in parse_template\n",
            "    addgroup(int(this[1:]), len(this) - 1)\n",
            "  File \"/usr/lib/python3.11/re/_parser.py\", line 1017, in addgroup\n",
            "    raise s.error(\"invalid group reference %d\" % index, pos)\n",
            "re.error: invalid group reference 11 at position 1\n",
            "ERROR:__main__:Error creating Excel file: cannot access local variable 'shared_format' where it is not associated with a value\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-12-5c658afbe9e1>\", line 1029, in create_comparison_excel\n",
            "    legend_sheet.write(row, 0, 'Shared Entities', shared_format)\n",
            "                               ^^^^^^^^^^^^^^^^^\n",
            "UnboundLocalError: cannot access local variable 'shared_format' where it is not associated with a value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## Animal Entity Extraction Comparison\n",
            "\n",
            "### Summary\n",
            "| Entity Type | SpaCy Small |\n",
            "|---|---|\n",
            "| SPECIES | 0 |\n",
            "| ANIMAL_GROUP | 0 |\n",
            "| ANIMAL_BODY_PART | 0 |\n",
            "| MEASUREMENT | 0 |\n",
            "| LENGTH | 0 |\n",
            "| HABITAT | 0 |\n",
            "\n",
            "### Processing Times\n",
            "| Model | Time (seconds) |\n",
            "|---|---|\n",
            "| SpaCy Small | 0.00 |\n",
            "\n",
            "## Results from SpaCy Small\n",
            "\n",
            "Error: Error extracting entities with SpaCy Small: invalid group reference 11 at position 1\n",
            "\n",
            "\n",
            "Processing completed in 2.91 seconds.\n",
            "Failed to create Excel file.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ba09784a6e0e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-ba09784a6e0e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;31m# Ask if user wants to see a sample of the extracted text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mshow_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDo you want to see a sample of the extracted text? (y/n): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshow_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nExtracted text sample:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Set up the available model categories\n",
        "    model_categories = {\n",
        "        \"SpaCy Models\": {\n",
        "            \"SpaCy Small\": \"en_core_web_sm\",\n",
        "            \"SpaCy Medium\": \"en_core_web_md\",\n",
        "            \"SpaCy Large\": \"en_core_web_lg\",\n",
        "            \"SpaCy Trf\": \"en_core_web_trf\"\n",
        "        },\n",
        "        \"Transformer Models\": {\n",
        "            \"BERT NER\": \"dslim/bert-base-NER\",\n",
        "            \"RoBERTa NER\": \"Jean-Baptiste/roberta-large-ner-english\",\n",
        "            \"DistilBERT NER\": \"elastic/distilbert-base-cased-finetuned-conll03-english\"\n",
        "        },\n",
        "        \"Bio-Transformer Models\": {\n",
        "            \"BioBERT NER\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
        "            \"SciBERT\": \"allenai/scibert_scivocab_uncased\",\n",
        "            \"PubMedBERT\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
        "        },\n",
        "        \"Flair Models\": {\n",
        "            \"Flair English\": \"flair/ner-english\",\n",
        "            \"Flair English Fast\": \"flair/ner-english-fast\"\n",
        "        },\n",
        "    }\n",
        "\n",
        "    # Check available libraries and models\n",
        "    available_model_categories = {}\n",
        "\n",
        "    # Check SpaCy\n",
        "    try:\n",
        "        import spacy\n",
        "        available_models = []\n",
        "        for name, path in model_categories[\"SpaCy Models\"].items():\n",
        "            try:\n",
        "                spacy.load(path)\n",
        "                available_models.append(name)\n",
        "            except OSError:\n",
        "                pass\n",
        "        if available_models:\n",
        "            available_model_categories[\"SpaCy Models\"] = available_models\n",
        "    except ImportError:\n",
        "        logger.warning(\"SpaCy not installed. Use pip install spacy to use SpaCy models.\")\n",
        "\n",
        "    # Check transformers\n",
        "    try:\n",
        "        import transformers\n",
        "        available_model_categories[\"Transformer Models\"] = list(model_categories[\"Transformer Models\"].keys())\n",
        "        available_model_categories[\"Bio-Transformer Models\"] = list(model_categories[\"Bio-Transformer Models\"].keys())\n",
        "    except ImportError:\n",
        "        logger.warning(\"Transformers library not installed. Use pip install transformers to add transformer models.\")\n",
        "\n",
        "    # Check flair\n",
        "    try:\n",
        "        import flair\n",
        "        available_model_categories[\"Flair Models\"] = list(model_categories[\"Flair Models\"].keys())\n",
        "    except ImportError:\n",
        "        logger.warning(\"Flair library not installed. Use pip install flair to add Flair models.\")\n",
        "\n",
        "    # Check allennlp\n",
        "    try:\n",
        "        import allennlp\n",
        "        import allennlp_models\n",
        "        available_model_categories[\"AllenNLP Models\"] = list(model_categories[\"AllenNLP Models\"].keys())\n",
        "    except ImportError:\n",
        "        logger.warning(\"AllenNLP library not installed. Use pip install allennlp allennlp_models to add AllenNLP models.\")\n",
        "\n",
        "    # Print available model categories\n",
        "    print(\"\\nAvailable model categories:\")\n",
        "    for i, category in enumerate(available_model_categories.keys()):\n",
        "        print(f\"{i+1}. {category}\")\n",
        "\n",
        "    # Let user select a category\n",
        "    category_choice = input(\"\\nSelect a model category (number): \")\n",
        "    try:\n",
        "        category_index = int(category_choice) - 1\n",
        "        categories = list(available_model_categories.keys())\n",
        "        selected_category = categories[category_index]\n",
        "\n",
        "        # Print available models in selected category\n",
        "        print(f\"\\nAvailable models in {selected_category}:\")\n",
        "        category_models = available_model_categories[selected_category]\n",
        "        for i, model in enumerate(category_models):\n",
        "            print(f\"{i+1}. {model}\")\n",
        "\n",
        "        # Let user select models from the category\n",
        "        model_indices = input(\"\\nSelect models (comma-separated numbers, e.g., 1,2): \")\n",
        "        selected_models = []\n",
        "        try:\n",
        "            indices = [int(idx.strip()) - 1 for idx in model_indices.split(\",\")]\n",
        "            for idx in indices:\n",
        "                if 0 <= idx < len(category_models):\n",
        "                    selected_models.append(category_models[idx])\n",
        "        except:\n",
        "            print(\"Invalid selection. Using the first available model.\")\n",
        "            selected_models = [category_models[0]]\n",
        "    except:\n",
        "        # Default to SpaCy if selection is invalid\n",
        "        print(\"Invalid category selection. Using available SpaCy models.\")\n",
        "        if \"SpaCy Models\" in available_model_categories:\n",
        "            selected_models = [available_model_categories[\"SpaCy Models\"][0]]\n",
        "        else:\n",
        "            print(\"No SpaCy models available. Please install at least one model.\")\n",
        "            install_model_message()\n",
        "            return\n",
        "\n",
        "    # Get PDF path from user\n",
        "    pdf_path = input(\"\\nEnter the path to your PDF file: \")\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(\"File not found!\")\n",
        "        return\n",
        "\n",
        "    # Define entity types with explanation\n",
        "    print(\"\\nAvailable animal-related entity types:\")\n",
        "    entity_types_list = [\n",
        "        (\"SPECIES\", \"Animal species scientific or common names\"),\n",
        "        (\"ANIMAL_GROUP\", \"Groups or collections of animals (herd, flock, etc.)\"),\n",
        "        (\"ANIMAL_BODY_PART\", \"Animal body parts (paw, wing, etc.)\"),\n",
        "        (\"MEASUREMENT\", \"Measurements related to animals\"),\n",
        "        (\"LENGTH\", \"Length measurements\"),\n",
        "        (\"HABITAT\", \"Animal habitats and environments\")\n",
        "    ]\n",
        "\n",
        "    for i, (entity_type, description) in enumerate(entity_types_list):\n",
        "        print(f\"{i+1}. {entity_type} - {description}\")\n",
        "\n",
        "    # Let user select entity types\n",
        "    entity_choice = input(\"\\nSelect entity types to extract (comma-separated numbers or 'all'): \")\n",
        "    selected_entity_types = []\n",
        "\n",
        "    if entity_choice.lower().strip() == 'all':\n",
        "        selected_entity_types = [et for et, _ in entity_types_list]\n",
        "    else:\n",
        "        try:\n",
        "            indices = [int(idx.strip()) - 1 for idx in entity_choice.split(\",\")]\n",
        "            for idx in indices:\n",
        "                if 0 <= idx < len(entity_types_list):\n",
        "                    selected_entity_types.append(entity_types_list[idx][0])\n",
        "        except:\n",
        "            print(\"Invalid selection. Using all entity types.\")\n",
        "            selected_entity_types = [et for et, _ in entity_types_list]\n",
        "\n",
        "    # Define output directory\n",
        "    output_dir = input(\"\\nEnter output directory (leave blank for current directory): \")\n",
        "    if not output_dir:\n",
        "        output_dir = \".\"\n",
        "\n",
        "    # Load the selected models (this will actually trigger the model downloading if needed)\n",
        "    print(\"\\nChecking and loading selected models. This may take some time for the first run...\")\n",
        "    models_to_load = []\n",
        "\n",
        "    for model_name in selected_models:\n",
        "        # Determine which category this model belongs to\n",
        "        model_type = None\n",
        "        model_path = None\n",
        "\n",
        "        for category, models in model_categories.items():\n",
        "            if model_name in models:\n",
        "                model_type = category\n",
        "                model_path = models[model_name]\n",
        "                break\n",
        "\n",
        "        if model_type == \"SpaCy Models\":\n",
        "            try:\n",
        "                import spacy\n",
        "                print(f\"Loading SpaCy model: {model_name}\")\n",
        "                nlp = spacy.load(model_path)\n",
        "                models_to_load.append(model_name)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {model_name}: {str(e)}\")\n",
        "                print(f\"To install, run: python -m spacy download {model_path}\")\n",
        "        elif model_type in [\"Transformer Models\", \"Bio-Transformer Models\"]:\n",
        "            try:\n",
        "                from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "                print(f\"Checking transformer model: {model_name}\")\n",
        "                # Just check if we can instantiate the tokenizer (will trigger download if needed)\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "                models_to_load.append(model_name)\n",
        "            except Exception as e:\n",
        "                print(f\"Error checking {model_name}: {str(e)}\")\n",
        "        elif model_type == \"Flair Models\":\n",
        "            try:\n",
        "                from flair.models import SequenceTagger\n",
        "                print(f\"Checking Flair model: {model_name}\")\n",
        "                # This will trigger download if needed\n",
        "                tagger = SequenceTagger.load(model_path)\n",
        "                models_to_load.append(model_name)\n",
        "            except Exception as e:\n",
        "                print(f\"Error checking {model_name}: {str(e)}\")\n",
        "        elif model_type == \"AllenNLP Models\":\n",
        "            try:\n",
        "                from allennlp.predictors.predictor import Predictor\n",
        "                print(f\"Checking AllenNLP model: {model_name}\")\n",
        "                # This might trigger download\n",
        "                predictor = Predictor.from_path(model_path)\n",
        "                models_to_load.append(model_name)\n",
        "            except Exception as e:\n",
        "                print(f\"Error checking {model_name}: {str(e)}\")\n",
        "\n",
        "    if not models_to_load:\n",
        "        print(\"No models could be loaded. Please install at least one model.\")\n",
        "        install_model_message()\n",
        "        return\n",
        "\n",
        "    # Process the PDF\n",
        "    print(f\"\\nProcessing {pdf_path} with models: {', '.join(models_to_load)}\")\n",
        "    print(f\"Extracting entity types: {', '.join(selected_entity_types)}\")\n",
        "\n",
        "    results, excel_file, text_sample, chart_file = process_pdf_multi_model(\n",
        "        pdf_path, selected_entity_types, models_to_load, output_dir\n",
        "    )\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + results)\n",
        "\n",
        "    if excel_file:\n",
        "        print(f\"\\nResults saved to Excel file: {excel_file}\")\n",
        "\n",
        "    if chart_file:\n",
        "        print(f\"Comparison chart saved to: {chart_file}\")\n",
        "\n",
        "    # Ask if user wants to see a sample of the extracted text\n",
        "    show_sample = input(\"\\nDo you want to see a sample of the extracted text? (y/n): \")\n",
        "    if show_sample.lower().startswith('y'):\n",
        "        print(\"\\nExtracted text sample:\")\n",
        "        print(\"-\" * 80)\n",
        "        print(text_sample)\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    # Provide additional analysis options\n",
        "    print(\"\\nAdditional analysis options:\")\n",
        "    print(\"1. Generate word frequency analysis\")\n",
        "    print(\"2. Create entity visualization\")\n",
        "    print(\"3. Export results to JSON\")\n",
        "    print(\"4. Exit\")\n",
        "\n",
        "    analysis_choice = input(\"\\nSelect an option (1-4): \")\n",
        "\n",
        "    if analysis_choice == \"1\":\n",
        "        print(\"Word frequency analysis would be generated here.\")\n",
        "        # Add your word frequency analysis code here\n",
        "    elif analysis_choice == \"2\":\n",
        "        print(\"Entity visualization would be created here.\")\n",
        "        # Add your entity visualization code here\n",
        "    elif analysis_choice == \"3\":\n",
        "        print(\"Results would be exported to JSON here.\")\n",
        "        # Add your JSON export code here\n",
        "    else:\n",
        "        print(\"\\nThank you for using the Animal Entity Extraction tool!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIMoOylysr_2",
        "outputId": "2df0ccdb-87f2-42d1-fc9e-97a5da454b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Using cached fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}